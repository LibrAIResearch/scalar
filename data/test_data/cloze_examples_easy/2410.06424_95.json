{
  "paper_id": "http://arxiv.org/abs/2410.06424v1",
  "question": "You are given a paper with a placeholder \"**[MASKED_CITATION]**\" in its content. Your task is to select the most appropriate reference from the provided reference list to replace the mask.\n\n- The paper content is enclosed within <Paper> and </Paper>.\n- The reference list is enclosed within <References> and </References>, with each reference wrapped in <Candidate> and </Candidate>.\n- After selecting the best-suited reference, output the index of that reference in the following format:\n<answer>index</answer>.\n\n<Paper>\n\nRestructuring Vector Quantization with the Rotation Trick\n\nAbstract: Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress\na continuous input to a discrete latent space and reconstruct it with minimal\ndistortion. They operate by maintaining a set of vectors -- often referred to\nas the codebook -- and quantizing each encoder output to the nearest vector in\nthe codebook. However, as vector quantization is non-differentiable, the\ngradient to the encoder flows around the vector quantization layer rather than\nthrough it in a straight-through approximation. This approximation may be\nundesirable as all information from the vector quantization operation is lost.\nIn this work, we propose a way to propagate gradients through the vector\nquantization layer of VQ-VAEs. We smoothly transform each encoder output into\nits corresponding codebook vector via a rotation and rescaling linear\ntransformation that is treated as a constant during backpropagation. As a\nresult, the relative magnitude and angle between encoder output and codebook\nvector becomes encoded into the gradient as it propagates through the vector\nquantization layer and back to the encoder. Across 11 different VQ-VAE training\nparadigms, we find this restructuring improves reconstruction metrics, codebook\nutilization, and quantization error. Our code is available at\nhttps://github.com/cfifty/rotation_trick.\n\n\n## 1 Introduction\nVector quantization (Gray, [1984]) is an approach to discretize a continuous vector space. It defines a finite set of vectors\u2014referred to as the codebook\u2014and maps any vector in the continuous vector space to the closest vector in the codebook.\nHowever, deep learning paradigms that use vector quantization are often difficult to train because replacing a vector with its closest codebook counterpart is a non-differentiable operation (Huh et al., [2023]).\nThis characteristic was not an issue at its creation during the Renaissance of Information Theory for applications like noisy channel communication (Cover, [1999]); however in the era deep learning, it presents a challenge as gradients cannot directly flow through layers that use vector quantization during backpropagation.\n\nIn deep learning, vector quantization is largely used in the eponymous Vector Quantized-Variational AutoEncoder (VQ-VAE) (Van Den Oord et al., [2017]). A VQ-VAE is an AutoEncoder with a vector quantization layer between the encoder\u2019s output and decoder\u2019s input, thereby quantizing the learned representation at the bottleneck. While VQ-VAEs are ubiquitous in state-of-the-art generative modeling (Rombach et al., [2022]; Dhariwal et al., [2020]; Brooks et al., [2024]), their gradients cannot flow from the decoder to the encoder uninterrupted as they must pass through a non-differentiable vector quantization layer.\n\nA solution to the non-differentiability problem is to approximate gradients via a \u201cstraight-through estimator\u201d (STE) (Bengio et al., [2013]). During backpropagation, the STE copies and pastes the gradients from the decoder\u2019s input to the encoder\u2019s output, thereby skipping the quantization operation altogether.\nHowever, this approximation can lead to poor-performing models and codebook collapse: a phenomena where a large percentage of the codebook converge to zero norm and are unused by the model (Mentzer et al., [2023]). Even if codebook collapse does not occur, the codebook is often under-utilized, thereby limiting the information capacity of the VQ-VAEs\u2019s bottleneck (Dhariwal et al., [2020]).\n\n![Refer to caption]Figure 1: Illustration of the rotation trick. In the forward pass, encoder output e\ud835\udc52e is rotated and rescaled to q1subscript\ud835\udc5e1q\\_{1}.\nNot shown is the rotation of the other encoder outputs to their corresponding codebook vectors. In the backward pass, the gradient at q1subscript\ud835\udc5e1q\\_{1} moves to e\ud835\udc52e so that the angle between\n\u2207q1\u2112subscript\u2207subscript\ud835\udc5e1\u2112\\\\nabla\\_{q\\_{1}}\\\\mathcal{L} and q1subscript\ud835\udc5e1q\\_{1} is preserved. Now, points within the same codebook region receive different gradients depending on their relative angle and magnitude to the codebook vector. For example, points with high angular distance can be pushed into new codebook regions, thereby increasing codebook utilization.Algorithm 1 VQ-VAE with the Rotation Trick\n\ninput example x\ud835\udc65x\n\ne\u2190Encoder\u200b(x)\u2190\ud835\udc52Encoder\ud835\udc65e\\\\leftarrow\\\\text{Encoder}(x)\n\nq\u2190\u2190\ud835\udc5eabsentq\\\\leftarrow nearest codebook vector to e\ud835\udc52e\n\nR\u2190\u2190\ud835\udc45absentR\\\\leftarrow rotation matrix that aligns e\ud835\udc52e to q\ud835\udc5eq\n\nq~\u2190stop-gradient\u200b\\[\u2225q\u2225\u2225e\u2225\u200bR\\]\u200be\u2190~\ud835\udc5estop-gradientdelimited-\\[\\]delimited-\u2225\u2225\ud835\udc5edelimited-\u2225\u2225\ud835\udc52\ud835\udc45\ud835\udc52\\\\tilde{q}\\\\leftarrow\\\\text{stop-gradient}\\\\left\\[\\\\frac{{\\\\lVert q\\\\rVert}}{{\\\\lVert e\\\\rVert}}R\\\\right\\]e\n\nx~\u2190Decoder(\\\\tilde{x}\\\\leftarrow\\\\text{Decoder}(q~~\ud835\udc5e\\\\tilde{q})))\n\nloss \u2190\u2112\u200b(x,x~)\u2190absent\u2112\ud835\udc65~\ud835\udc65\\\\leftarrow\\\\mathcal{L}(x,\\\\tilde{x})\n\nreturn loss\n\nIn this work, we propose an alternate way to propagate gradients through the vector quantization layer in VQ-VAEs.\nFor a given encoder output e\ud835\udc52e and nearest codebook vector q\ud835\udc5eq, we smoothly transform e\ud835\udc52e to q\ud835\udc5eq via a rotation and rescaling linear transformation and then send this output\u2014rather than the direct result of the codebook lookup\u2014to the decoder.\nAs the input to the decoder, q~~\ud835\udc5e\\\\tilde{q}, is now treated as a smooth linear transformation of e\ud835\udc52e, gradients flow back from the decoder to the encoder unimpeded.\nTo avoid differentiating through the rotation and rescaling, we treat both as constants with respect to e\ud835\udc52e and q\ud835\udc5eq. We explain why this choice is necessary in [SectionA.2]A.2 Treating \ud835\udc45 and \u2016\ud835\udc5e\u2016/\u2016\ud835\udc52\u2016 as Constants \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\").\nFollowing the convention of Kingma & Welling ( [2013])), we call this restructuring \u201cthe rotation trick.\u201d It is illustrated in [Figure3]In 4.1 The Rotation Trick Preserves Angles \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") and described in [Algorithm1]In 1 Introduction \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\nThe rotation trick does not change the output of the VQ-VAE in the forward pass.\nHowever, during the backward pass, it transports the gradient \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} at q\ud835\udc5eq to become the gradient \u2207e\u2112subscript\u2207\ud835\udc52\u2112\\\\nabla\\_{e}\\\\mathcal{L} at e\ud835\udc52e so that the angle between q\ud835\udc5eq and \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L}after the vector quantization layer equals the angle between e\ud835\udc52e and \u2207e\u2112subscript\u2207\ud835\udc52\u2112\\\\nabla\\_{e}\\\\mathcal{L}before the vector quantization layer. Preserving this angle encodes relative angular distances and magnitudes into the gradient and changes how points within the same codebook region are updated.\n\nThe STE applies the same update to all points within the same codebook region, maintaining their relative distances. However as we will show in [Section4.3]Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\"), the rotation trick can push points within the same codebook region farther apart\u2014or pull them closer together\u2014depending on the direction of the gradient vector.\nThe former capability can correspond to increased codebook usage while the latter to lower quantization error. In the context of lossy compression, both capabilities are desirable for reducing the distortion and increasing the information capacity of the vector quantization layer.\n\nWhen applied to several open-source VQ-VAE repositories, we find the rotation trick substantively improves reconstruction performance, increases codebook usage, and decreases the distance between encoder outputs and their corresponding codebook vectors. For instance, training the VQGAN from Rombach et al. ( [2022]) on ImageNet (Deng et al., [2009]) with the rotation trick improves reconstruction FID from 5.0 to 1.6, reconstruction IS from 141.5 to 190.3, increases codebook usage from 2% to 9%, and decreases quantization error by over an order of magnitude.\n\n## 2 Related Work\nMany researchers have built upon the seminal work of Van Den Oord et al. ( [2017]) to improve VQ-VAE performance. While non-exhaustive, our review focuses on methods that address training instabilities caused by the vector quantization layer. We partition these efforts into two categories: (1) methods that sidestep the STE and (2) methods that improve codebook-model interactions.\n\nSidestepping the STE. Several prior works have sought to fix the problems caused by the STE by avoiding deterministic vector quantization. Baevski et al. ( [2019]) employ the Gumbel-Softmax trick (Jang et al., [2016]) to fit a categorical distribution over codebook vectors that converges to a one-hot distribution towards the end of training, Gautam et al. ( [2023]) quantize using a a convex combination of codebook vectors, and Takida et al. ( [2022]) employ stochastic quantization. Unlike the above that cast vector quantization as a distribution over codebook vectors, Huh et al. ( [2023]) propose an alternating optimization where the encoder is optimized to output representations close to the codebook vectors while the decoder minimizes reconstruction loss from a fixed set of codebook vector inputs. While these approaches sidestep the training instabilities caused by the STE, they can introduce their own set of problems and complexities such as low codebook utilization at inference and the tuning of a temperature schedule (Zhang et al., [2023]). As a result, many applications and research papers continue to employ VQ-VAEs that are trained using the STE (Rombach et al., [2022]; Chang et al., [2022]; Huang et al., [2023]; Zhu et al., [2023]; Dong et al., [2023]).\n\nCodebook-Model Improvements. Another way to attack codebook collapse or under-utilization is to change the codebook lookup. Rather than use Euclidean distance, Yu et al. ( [2021]) employ a cosine similarity measure, Goswami et al. ( [2024]) a hyperbolic metric, and Lee et al. ( [2022]) stochastically sample codes as a function of the distance between the encoder output and codebook vectors. Another perspective examines the learning of the codebook. Kolesnikov et al. ( [2022]) split high-usage codebook vectors, Dhariwal et al. ( [2020]); \u0141a\u0144cucki et al. ( [2020]); Zheng & Vedaldi ( [2023])) resurrect low-usage codebook vectors throughout training, Chen et al. ( [2024]) dynamically selects one of m\ud835\udc5am codebooks for each datapoint, and Mentzer et al. ( [2023]); Zhao et al. ( [2024]); Yu et al. ( [2023]); Chiu et al. ( [2022]) fix the codebook vectors to an a priori geometry and train the model without learning the codebook at all. Other works propose loss penalties to encourage codebook utilization. Zhang et al. ( [2023]) add a KL-divergence penalty between codebook utilization and a uniform distribution while Yu et al. ( [2023]) add an entropy loss term to penalize low codebook utilization. While effective at targeting specific training difficulties, as each of these methods continue to use the STE, the training instability caused by this estimator persist. Most of our experiments in [Section5]Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") implement a subset of these approaches, and we find that replacing the STE with the rotation trick further improves performance.\n\n## 3 Straight Through Estimator (STE)\nIn this section, we review the Straight-Through Estimator (STE) and visualize its effect on the gradients. We then explore two STE alternatives that\u2014at first glance\u2014appear to correct the approximation made by the STE.\n\nFor notation, we define a sample space \ud835\udcb3\ud835\udcb3\\\\mathcal{X} over the input data with probability distribution p\ud835\udc5dp. For input x\u2208\ud835\udcb3\ud835\udc65\ud835\udcb3x\\\\in\\\\mathcal{X}, we define the encoder as a deterministic mapping that parameterizes a posterior distribution p\u2130\u200b(e\\|x)subscript\ud835\udc5d\u2130conditional\ud835\udc52\ud835\udc65p\\_{\\\\mathcal{E}}(e\\|x). The vector quantization layer, \ud835\udcac\u200b(\u22c5)\ud835\udcac\u22c5\\\\mathcal{Q}(\\\\cdot), is a function that selects the codebook vector q\u2208\ud835\udc9e\ud835\udc5e\ud835\udc9eq\\\\in\\\\mathcal{C} nearest to the encoder output e\ud835\udc52e. Under Euclidean distance, it has the form:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \ud835\udcac\u200b(q=i\\|e)\ud835\udcac\ud835\udc5econditional\ud835\udc56\ud835\udc52\\\\displaystyle\\\\mathcal{Q}(q=i\\|e) | ={1 if i=arg\u200bmin1\u2264j\u2264\\|\ud835\udc9e\\|\u2225e\u2212qj\u222520\u200b otherwise\\\\displaystyle=\\\\begin{cases}1\\\\text{ if }i=\\\\operatorname\\*{arg\\\\,min}\\_{1\\\\leq j\\\\leq\\|\\\\mathcal{C}\\|}{\\\\lVert e-q\\_{j}\\\\rVert}\\_{2}\\\\\\<br>0\\\\text{ otherwise}\\\\end{cases} |  |\n\nThe decoder is similarly defined as a deterministic mapping that parameterizes the conditional distribution over reconstructions p\ud835\udc9f\u200b(x~\\|q)subscript\ud835\udc5d\ud835\udc9fconditional~\ud835\udc65\ud835\udc5ep\\_{\\\\mathcal{D}}(\\\\tilde{x}\\|q). As in the VAE (Kingma & Welling, [2013]), the loss function follows from the ELBO with the KL-divergence term zeroing out as p\u2130\u200b(e\\|x)subscript\ud835\udc5d\u2130conditional\ud835\udc52\ud835\udc65p\\_{\\\\mathcal{E}}(e\\|x) is deterministic and the utilization over codebook vectors is assumed to be uniform. Van Den Oord et al. ( [2017]) additionally add a \u201ccodebook loss\u201d term \u2225s\u200bg\u200b(e)\u2212q\u222522superscriptsubscriptdelimited-\u2225\u2225\ud835\udc60\ud835\udc54\ud835\udc52\ud835\udc5e22{\\\\lVert sg(e)-q\\\\rVert}\\_{2}^{2} to learn the codebook vectors and a \u201ccommitment loss\u201d term \u03b2\u200b\u2225e\u2212s\u200bg\u200b(q)\u222522\ud835\udefdsuperscriptsubscriptdelimited-\u2225\u2225\ud835\udc52\ud835\udc60\ud835\udc54\ud835\udc5e22\\\\beta{\\\\lVert e-sg(q)\\\\rVert}\\_{2}^{2} to pull the encoder\u2019s output towards the codebook vectors. s\u200bg\ud835\udc60\ud835\udc54sg stands for stop-gradient and \u03b2\ud835\udefd\\\\beta is a hyperparameter, typically set to a value in \\[0.25,2\\]0.252\\[0.25,2\\]. For predicted reconstruction x~~\ud835\udc65\\\\tilde{x}, the optimization objective becomes:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2112\u200b(x~)\u2112~\ud835\udc65\\\\displaystyle\\\\mathcal{L}(\\\\tilde{x}) | =\u2225x\u2212x~\u222522+\u2225s\u200bg\u200b(e)\u2212q\u222522+\u03b2\u200b\u2225e\u2212s\u200bg\u200b(q)\u222522absentsuperscriptsubscriptdelimited-\u2225\u2225\ud835\udc65~\ud835\udc6522superscriptsubscriptdelimited-\u2225\u2225\ud835\udc60\ud835\udc54\ud835\udc52\ud835\udc5e22\ud835\udefdsuperscriptsubscriptdelimited-\u2225\u2225\ud835\udc52\ud835\udc60\ud835\udc54\ud835\udc5e22\\\\displaystyle={\\\\lVert x-\\\\tilde{x}\\\\rVert}\\_{2}^{2}+{\\\\lVert sg(e)-q\\\\rVert}\\_{2}^{2}+\\\\beta{\\\\lVert e-sg(q)\\\\rVert}\\_{2}^{2} |  |\n\nIn the subsequent analysis, we focus only on the \u2225x\u2212x~\u222522superscriptsubscriptdelimited-\u2225\u2225\ud835\udc65~\ud835\udc6522{\\\\lVert x-\\\\tilde{x}\\\\rVert}\\_{2}^{2} term as the other two are not functions of the decoder. During backpropagation, the model must differentiate through the vector quantization function \ud835\udcac\u200b(\u22c5)\ud835\udcac\u22c5\\\\mathcal{Q}(\\\\cdot). We can break down the backward pass into three terms:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2202\u2112\u2202x\u2112\ud835\udc65\\\\displaystyle\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial x} | =\u2202\u2112\u2202q\u200b\u2202q\u2202e\u200b\u2202e\u2202xabsent\u2112\ud835\udc5e\ud835\udc5e\ud835\udc52\ud835\udc52\ud835\udc65\\\\displaystyle=\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial q}\\\\frac{\\\\partial q}{\\\\partial e}\\\\frac{\\\\partial e}{\\\\partial x} |  |\n\nwhere \u2202\u2112\u2202q\u2112\ud835\udc5e\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial q} represents backpropagation through the decoder, \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} represents backpropagation through the vector quantization layer, and \u2202e\u2202x\ud835\udc52\ud835\udc65\\\\frac{\\\\partial e}{\\\\partial x} represents backpropagation through the encoder. As vector quantization is not a smooth transformation, \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} cannot be computed and gradients cannot flow through this term to update the encoder in backpropagation.\n\n![Refer to caption]Figure 2: Visualization of how the straight-through estimator (STE) transforms the gradient field for 161616 codebook vectors for (top) f\u200b(x,y)=x2+y2\ud835\udc53\ud835\udc65\ud835\udc66superscript\ud835\udc652superscript\ud835\udc662f(x,y)=x^{2}+y^{2} and (bottom) f\u200b(x,y)=log\u2061(\\|12\u200bx+tanh\u2061(y)\\|)\ud835\udc53\ud835\udc65\ud835\udc6612\ud835\udc65\ud835\udc66f(x,y)=\\\\log(\\|\\\\frac{1}{2}x+\\\\tanh(y)\\|). The STE takes the gradient at the codebook vector (qx,qy)subscript\ud835\udc5e\ud835\udc65subscript\ud835\udc5e\ud835\udc66(q\\_{x},q\\_{y}) and \u201ccopies-and-pastes\u201d it to all other locations within the same codebook region, forming a \u201cchecker-board\u201d pattern in the gradient field.\n\nTo solve the issue of non-differentiability, the STE copies the gradients from q\ud835\udc5eq to e\ud835\udc52e, bypassing vector quantization entirely. Simply, the STE sets \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} to the identity matrix I\ud835\udc3cI in the backward pass:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2202\u2112\u2202x\u2112\ud835\udc65\\\\displaystyle\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial x} | =\u2202\u2112\u2202q\u200bI\u200b\u2202e\u2202xabsent\u2112\ud835\udc5e\ud835\udc3c\ud835\udc52\ud835\udc65\\\\displaystyle=\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial q}I\\\\frac{\\\\partial e}{\\\\partial x} |  |\n\nThe first two terms \u2202\u2112\u2202q\u200b\u2202q\u2202e\u2112\ud835\udc5e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial q}\\\\frac{\\\\partial q}{\\\\partial e} combine to \u2202\u2112\u2202e\u2112\ud835\udc52\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial e} which, somewhat misleadingly, does not actually depend on e\ud835\udc52e. As a consequence, the location of e\ud835\udc52e within the Voronoi partition generated by codebook vector q\ud835\udc5eq\u2014be it close to q\ud835\udc5eq or at the boundary of the region\u2014has no impact on the gradient update to the encoder.\n\nAn example of this effect is visualized in [Figure2]In 3 Straight Through Estimator (STE) \u2023 Restructuring Vector Quantization with the Rotation Trick\") for two example functions. In the STE approximation, the \u201cexact\u201d gradient at the encoder output is replaced by the gradient at the corresponding codebook vector for each Voronoi partition, irrespective of where in that region the encoder output e\ud835\udc52e lies. As a result, the exact gradient field becomes \u201cpartitioned\u201d into 161616 different regions\u2014all with the same gradient update to the encoder\u2014for the 161616 vectors in the codebook.\n\nReturning to our question, is there a better way to propagate gradients through the vector quantization layer? At first glance, one may be tempted to estimate the curvature at q\ud835\udc5eq and use this information to transform \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} as q\ud835\udc5eq moves to e\ud835\udc52e. This is accomplished by taking a second order expansion around q\ud835\udc5eq to approximate the value of the loss at e\ud835\udc52e:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2112esubscript\u2112\ud835\udc52\\\\displaystyle\\\\mathcal{L}\\_{e} | \u2248\u2112q+(\u2207q\u2112)T\u200b(e\u2212q)+12\u200b(e\u2212q)T\u200b(\u2207q2\u2112)\u200b(e\u2212q)absentsubscript\u2112\ud835\udc5esuperscriptsubscript\u2207\ud835\udc5e\u2112\ud835\udc47\ud835\udc52\ud835\udc5e12superscript\ud835\udc52\ud835\udc5e\ud835\udc47superscriptsubscript\u2207\ud835\udc5e2\u2112\ud835\udc52\ud835\udc5e\\\\displaystyle\\\\approx\\\\mathcal{L}\\_{q}+(\\\\nabla\\_{q}\\\\mathcal{L})^{T}(e-q)+\\\\frac{1}{2}(e-q)^{T}(\\\\nabla\\_{q}^{2}\\\\mathcal{L})(e-q) |  |\n\nThen we can compute the gradient at the point e\ud835\udc52e instead of q\ud835\udc5eq up to second order approximation with:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2202\u2112\u2202e\u2112\ud835\udc52\\\\displaystyle\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial e} | \u2248\u2202\u2202e\u200b\\[\u2112q+(\u2207q\u2112)T\u200b(e\u2212q)+12\u200b(e\u2212q)T\u200b(\u2207q2\u2112)\u200b(e\u2212q)\\]absent\ud835\udc52delimited-\\[\\]subscript\u2112\ud835\udc5esuperscriptsubscript\u2207\ud835\udc5e\u2112\ud835\udc47\ud835\udc52\ud835\udc5e12superscript\ud835\udc52\ud835\udc5e\ud835\udc47superscriptsubscript\u2207\ud835\udc5e2\u2112\ud835\udc52\ud835\udc5e\\\\displaystyle\\\\approx\\\\frac{\\\\partial}{\\\\partial e}\\\\left\\[\\\\mathcal{L}\\_{q}+(\\\\nabla\\_{q}\\\\mathcal{L})^{T}(e-q)+\\\\frac{1}{2}(e-q)^{T}(\\\\nabla\\_{q}^{2}\\\\mathcal{L})(e-q)\\\\right\\] |  |\n|  |  | =\u2207q\u2112+(\u2207q2\u2112)\u200b(e\u2212q)absentsubscript\u2207\ud835\udc5e\u2112superscriptsubscript\u2207\ud835\udc5e2\u2112\ud835\udc52\ud835\udc5e\\\\displaystyle=\\\\nabla\\_{q}\\\\mathcal{L}+(\\\\nabla\\_{q}^{2}\\\\mathcal{L})(e-q) |  |\n\nWhile computing Hessians with respect to model parameters are typically prohibitive in modern deep learning architectures, computing them with respect to only the codebook is feasible. Moreover as we must only compute (\u2207q2\u2112)\u200b(e\u2212q)superscriptsubscript\u2207\ud835\udc5e2\u2112\ud835\udc52\ud835\udc5e(\\\\nabla\\_{q}^{2}\\\\mathcal{L})(e-q), one may take advantage of efficient Hessian-Vector products implementations in deep learning frameworks (Dagr\u00e9ou et al., [2024]) and avoid computing the full Hessian matrix.\n\nExtending this idea a step further, we can compute the exact gradient \u2202\u2112\u2202e\u2112\ud835\udc52\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial e} at e\ud835\udc52e by making two passes through the network. Let \u2112qsubscript\u2112\ud835\udc5e\\\\mathcal{L}\\_{q} be the loss with the vector quantization layer and \u2112esubscript\u2112\ud835\udc52\\\\mathcal{L}\\_{e} be the loss without vector quantization, i.e. q=e\ud835\udc5e\ud835\udc52q=e rather than q=\ud835\udcac\u200b(e)\ud835\udc5e\ud835\udcac\ud835\udc52q=\\\\mathcal{Q}(e).\nThen one may form the total loss \u2112=\u2112q+\u03bb\u200b\u2112e\u2112subscript\u2112\ud835\udc5e\ud835\udf06subscript\u2112\ud835\udc52\\\\mathcal{L}=\\\\mathcal{L}\\_{q}+\\\\lambda\\\\mathcal{L}\\_{e}, where \u03bb\ud835\udf06\\\\lambda is a small constant like 10\u22126superscript10610^{-6}, to scale down the effect of \u2112esubscript\u2112\ud835\udc52\\\\mathcal{L}\\_{e} on the decoder\u2019s parameters and use a gradient scaling multiplier of \u03bb\u22121superscript\ud835\udf061\\\\lambda^{-1} to reweigh the effect of \u2112esubscript\u2112\ud835\udc52\\\\mathcal{L}\\_{e} on the encoder\u2019s parameters to 111. As \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} is non-differentiable, gradients from \u2112qsubscript\u2112\ud835\udc5e\\\\mathcal{L}\\_{q} will not flow to the encoder.\n\nWhile seeming to correct the encoder\u2019s gradients, replacing the STE with either approach will likely result in worse performance. This is because computing the exact gradient with respect to e\ud835\udc52e is actually the AutoEncoder (Hinton & Zemel, [1993]) gradient, the model that VAEs (Kingma & Welling, [2013]) and VQ-VAEs (Van Den Oord et al., [2017]) were designed to replace given the AutoEncoder\u2019s propensity to overfit and difficultly generalizing. Accordingly using either Hessian approximation or exact gradients via a double forward pass will cause the encoder to be trained like an AutoEncoder and the decoder to be trained like a VQ-VAE. This mis-match in optimization objectives is likely another contributing factor to the poor performance we observe for both methods in [Table1]In 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\n## 4 The Rotation Trick\nAs discussed in [Section3]Straight Through Estimator (STE) \u2023 Restructuring Vector Quantization with the Rotation Trick\"), updating the encoder\u2019s parameters by approximating, or exactly, computing the gradient at the encoder\u2019s output is undesirable.\nSimilarly, the STE appears to lose information: the location of e\ud835\udc52e within the quantized region\u2014be it close to q\ud835\udc5eq or far away at the boundary\u2014has no impact on the gradient update to the encoder.\nCapturing this information, i.e. using the location of e\ud835\udc52e in relation to q\ud835\udc5eq to transform the gradients through \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e}, could be beneficial to the encoder\u2019s gradient updates and an improvement over the STE.\n\nViewed geometrically, we ask how to move the gradient \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} from q\ud835\udc5eq to e\ud835\udc52e, and what characteristics of \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} and q\ud835\udc5eq should be preserved during this movement. The STE offers one possible answer: move the gradient from q\ud835\udc5eq to e\ud835\udc52e so that its direction and magnitude are preserved. However, this paper supplies a different answer: move the gradient so that the angle between \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} and q\ud835\udc5eq is preserved as \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} moves to e\ud835\udc52e. We term this approach \u201cthe rotation trick\u201d, and in [Section4.3]Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") we show that preserving the angle between q\ud835\udc5eq and \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} conveys desirable properties to how points move within the same quantized region.\n\n### 4.1 The Rotation Trick Preserves Angles\n\nIn this section, we formally define the rotation trick. For encoder output e\ud835\udc52e, let q=\ud835\udcac\u200b(e)\ud835\udc5e\ud835\udcac\ud835\udc52q=\\\\mathcal{Q}(e) represent the corresponding codebook vector. \ud835\udcac\u200b(\u22c5)\ud835\udcac\u22c5\\\\mathcal{Q}(\\\\cdot) is non-differentiable so gradients cannot flow through this layer during the backward pass.\nThe STE solves this problem\u2014maintaining the direction and magnitude of the gradient \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L}\u2014as \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} moves from q\ud835\udc5eq to e\ud835\udc52e with some clever hacking of the backpropagation function in deep learning frameworks:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | q~~\ud835\udc5e\\\\displaystyle\\\\tilde{q} | =e\u2212(q\u2212e)\u23dfconstantabsent\ud835\udc52subscript\u23df\ud835\udc5e\ud835\udc52constant\\\\displaystyle=e-\\\\underbrace{(q-e)}\\_{\\\\text{constant}} |  |\n\n![Refer to caption]Figure 3: Illustration of how the gradient at q\ud835\udc5eq moves to e\ud835\udc52e via the STE (middle) and rotation trick (right). The STE \u201ccopies-and-pastes\u201d the gradient to preserve its direction while the rotation trick moves the gradient so the angle between q\ud835\udc5eq and \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} is preserved.\n\nwhich is a parameterization of vector quantization that sets the gradient at the encoder output to the gradient at the decoder\u2019s input. The rotation trick offers a different parameterization: casting the forward pass as a rotation and rescaling that aligns e\ud835\udc52e with q\ud835\udc5eq:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | q~~\ud835\udc5e\\\\displaystyle\\\\tilde{q} | =\\[\u2225q\u2225\u2225e\u2225\u200bR\\]\u23dfconstant\u200beabsentsubscript\u23dfdelimited-\\[\\]delimited-\u2225\u2225\ud835\udc5edelimited-\u2225\u2225\ud835\udc52\ud835\udc45constant\ud835\udc52\\\\displaystyle=\\\\underbrace{\\\\left\\[\\\\frac{{\\\\lVert q\\\\rVert}}{{\\\\lVert e\\\\rVert}}R\\\\right\\]}\\_{\\\\text{constant}}e |  |\n\nR\ud835\udc45R is the rotation111A rotation is defined as a linear transformation so that R\u200bRT=I\ud835\udc45superscript\ud835\udc45\ud835\udc47\ud835\udc3cRR^{T}=I, R\u22121=RTsuperscript\ud835\udc451superscript\ud835\udc45\ud835\udc47R^{-1}=R^{T}, and det(R\ud835\udc45R) = 1. transformation that aligns e\ud835\udc52e with q\ud835\udc5eq and \u2225q\u2225\u2225e\u2225delimited-\u2225\u2225\ud835\udc5edelimited-\u2225\u2225\ud835\udc52\\\\frac{{\\\\lVert q\\\\rVert}}{{\\\\lVert e\\\\rVert}} rescales e\ud835\udc52e to have the same magnitude as q\ud835\udc5eq. Note that both R\ud835\udc45R and \u2225q\u2225\u2225e\u2225delimited-\u2225\u2225\ud835\udc5edelimited-\u2225\u2225\ud835\udc52\\\\frac{{\\\\lVert q\\\\rVert}}{{\\\\lVert e\\\\rVert}} are functions of e\ud835\udc52e. To avoid differentiating through this dependency, we treat them as fixed constants\u2014or detached from the computational graph in deep learning frameworks\u2014when differentiating. This choice is explained in [SectionA.2]A.2 Treating \ud835\udc45 and \u2016\ud835\udc5e\u2016/\u2016\ud835\udc52\u2016 as Constants \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\nWhile the rotation trick does not change the output of the forward pass, the backward pass changes. Rather than set \u2202q\u2202e=I\ud835\udc5e\ud835\udc52\ud835\udc3c\\\\frac{\\\\partial q}{\\\\partial e}=I as in the STE, the rotation trick sets \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} to be a rotation and rescaling transformation:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2202q~\u2202e~\ud835\udc5e\ud835\udc52\\\\displaystyle\\\\frac{\\\\partial\\\\tilde{q}}{\\\\partial e} | =\u2225q\u2225\u2225e\u2225\u200bRabsentdelimited-\u2225\u2225\ud835\udc5edelimited-\u2225\u2225\ud835\udc52\ud835\udc45\\\\displaystyle=\\\\frac{{\\\\lVert q\\\\rVert}}{{{\\\\lVert e\\\\rVert}}}R |  |\n\nAs a result, \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} changes based on the position of e\ud835\udc52e in the codebook partition of q\ud835\udc5eq, and notably, the angle between \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} and q\ud835\udc5eq is preserved as \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} moves to e\ud835\udc52e. This effect is visualized in [Figure3]In 4.1 The Rotation Trick Preserves Angles \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\"). While the STE translates the gradient from q\ud835\udc5eq to e\ud835\udc52e, the rotation trick rotates it so that the angle between \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} and q\ud835\udc5eq is preserved. In a sense, the rotation trick and the STE are sibilings. They choose different characteristics of the gradient as desiderata and then preserve those characteristics as the gradient flows around the non-differentiable vector quantization operation to the encoder.\n\n### 4.2 Efficient Rotation Computation\n\nThe rotation transformation R\ud835\udc45R that rotates e\ud835\udc52e to q\ud835\udc5eq can be efficiently computed with Householder matrix reflections. We define e^=e\u2225e\u2225^\ud835\udc52\ud835\udc52delimited-\u2225\u2225\ud835\udc52\\\\hat{e}=\\\\frac{e}{{\\\\lVert e\\\\rVert}}, q^=q\u2225q\u2225^\ud835\udc5e\ud835\udc5edelimited-\u2225\u2225\ud835\udc5e\\\\hat{q}=\\\\frac{q}{{\\\\lVert q\\\\rVert}}, \u03bb=\u2225q\u2225\u2225e\u2225\ud835\udf06delimited-\u2225\u2225\ud835\udc5edelimited-\u2225\u2225\ud835\udc52\\\\lambda=\\\\frac{{\\\\lVert q\\\\rVert}}{{\\\\lVert e\\\\rVert}}, and r=e^+q^\u2225e^+q^\u2225\ud835\udc5f^\ud835\udc52^\ud835\udc5edelimited-\u2225\u2225^\ud835\udc52^\ud835\udc5er=\\\\frac{\\\\hat{e}+\\\\hat{q}}{{\\\\lVert\\\\hat{e}+\\\\hat{q}\\\\rVert}}. Then the rotation and rescaling that aligns e\ud835\udc52e to q\ud835\udc5eq is simply:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | q~~\ud835\udc5e\\\\displaystyle\\\\tilde{q} | =\u03bb\u200bR\u200beabsent\ud835\udf06\ud835\udc45\ud835\udc52\\\\displaystyle=\\\\lambda Re |  |\n|  |  | =\u03bb\u200b(I\u22122\u200br\u200brT+2\u200bq^\u200be^T)\u200beabsent\ud835\udf06\ud835\udc3c2\ud835\udc5fsuperscript\ud835\udc5f\ud835\udc472^\ud835\udc5esuperscript^\ud835\udc52\ud835\udc47\ud835\udc52\\\\displaystyle=\\\\lambda(I-2rr^{T}+2\\\\hat{q}\\\\hat{e}^{T})e |  |\n|  |  | =\u03bb\u200b\\[e\u22122\u200br\u200brT\u200be+2\u200bq^\u200be^T\u200be\\]absent\ud835\udf06delimited-\\[\\]\ud835\udc522\ud835\udc5fsuperscript\ud835\udc5f\ud835\udc47\ud835\udc522^\ud835\udc5esuperscript^\ud835\udc52\ud835\udc47\ud835\udc52\\\\displaystyle=\\\\lambda\\[e-2rr^{T}e+2\\\\hat{q}\\\\hat{e}^{T}e\\] |  |\n\nDue to space constraints, we leave the derivation of this formula to [SectionA.4]A.4 HouseHolder Reflection Transformation \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\"). Parameterizing the rotation in this fashion avoids computing outer products and therefore consumes minimal GPU VRAM. Further, we did not detect a difference in wall-clock time between VQ-VAEs trained with the STE and VQ-VAEs trained with the rotation trick for our experiments in [Section5]Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\n### 4.3 Voronoi Partition Analysis\n\n![Refer to caption]Figure 4: Depiction of how points within the same codebook region change after a gradient update (red arrow) at the codebook vector (orange circle). The STE applies the same update to each point in the same region. The rotation trick modifies the update based on the location of each point with respect to the codebook vector.![Refer to caption]Figure 5: With the STE, the distances among points within the same region do not change. However with the rotation trick, the distances among points do change. When \u03b8~<\u03c0/2~\ud835\udf03\ud835\udf0b2\\\\tilde{\\\\theta}<\\\\pi/2, points with large angular distance are pushed away (blue: increasing distance). When \u03b8~>\u03c0/2~\ud835\udf03\ud835\udf0b2\\\\tilde{\\\\theta}>\\\\pi/2, points are pulled towards the codebook vector (green: decreasing distance).\n\nIn the context of lossy compression, vector quantization works well when the distortion, or equivalently quantization error \u2225e\u2212q\u222522superscriptsubscriptdelimited-\u2225\u2225\ud835\udc52\ud835\udc5e22{\\\\lVert e-q\\\\rVert}\\_{2}^{2}, is low and the information capacity\u2014equivalently codebook utilization\u2014is high (Cover, [1999]).\nLater in [Section5]Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\"), we will see that VQ-VAEs trained with the rotation trick have this desiderata\u2014often reducing quantization error by an order of magnitude and substantially increasing codebook usage\u2014when compared to VQ-VAEs trained with the STE. However, the underlying reason why this occurs is less clear.\n\nIn this section, we analyze the effect of the rotation trick by looking at how encoder outputs that are mapped to the same Voronoi region are updated. While the STE applies the same update to all points within the same partition, the rotation trick changes the update based on the location of points within the Voronoi region. It can push points within the same region farther apart or pull them closer together depending on the direction of the gradient vector. The former capability can correspond to increased codebook usage while the latter to lower quantization error.\n\nLet \u03b8\ud835\udf03\\\\theta be the angle between e\ud835\udc52e and q\ud835\udc5eq and \u03b8~~\ud835\udf03\\\\tilde{\\\\theta} be the angle between q\ud835\udc5eq and \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L}. Intuitively, when \u03b8\ud835\udf03\\\\theta is large, e\ud835\udc52e maps to q\ud835\udc5eq with high distortion. In this case, we\u2019d like e\ud835\udc52e to commit one way or another. Either (1) make e\ud835\udc52e commit to its current codebook vector by pullinge\ud835\udc52e closer to q\ud835\udc5eq or (2) pushe\ud835\udc52e to a different codebook vector entirely. This behavior is preferable to maintaining states of high distortion where the distance between e\ud835\udc52e and q\ud835\udc5eq remains constant. We will show the rotation trick enables both (1) and (2); however, the STE perpetuates the latter as visualized in [Figure5]In 4.3 Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\nWhen \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} and q\ud835\udc5eq point in the same direction, i.e. \u2212\u03c0/2<\u03b8~<\u03c0/2\ud835\udf0b2~\ud835\udf03\ud835\udf0b2-\\\\pi/2<\\\\tilde{\\\\theta}<\\\\pi/2, encoder outputs with large angular distance to q\ud835\udc5eq are pushed farther away than they would otherwise be moved by the STE update.\n[Figure5]In 4.3 Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") illustrates this effect. The points with large angular distance (blue regions) move further away from q\ud835\udc5eq than the points with low angular distance (ivory regions).\n\nThe top right partitions of [Figure4]In 4.3 Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") present an example of this effect. The two clusters of points at the boundary\u2014with relatively large angle to the codebook vector\u2014are pushed away while the cluster of points with small angle to the codebook vector move with it. The ability to push points at the boundary out of a quantized region and into another is desirable for increasing codebook utilization. Specifically, codebook utilization improves when points are pushed into the Voronoi regions of previously unused codebook vectors. This capability is not shared by the STE, which moves all points in the same region by the same amount.\n\nWhen \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} and q\ud835\udc5eq point in opposite directions, i.e. \u03c0/2<\u03b8~<3\u200b\u03c0/2\ud835\udf0b2~\ud835\udf033\ud835\udf0b2\\\\pi/2<\\\\tilde{\\\\theta}<3\\\\pi/2, the distance among points within the same Voronoi region decreases as they are pulled towards the location of the updated codebook vector. This effect is visualized in [Figure5]In 4.3 Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") (green regions) and the bottom partitions of [Figure4]In 4.3 Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") show an example. Unlike the STE update\u2014that maintains the distances among points\u2014the rotation trick pulls points with high angular distances closer towards the post-update codebook vector. This capability is desirable for reducing the quantization error and enabling the encoder to lock on(Van Den Oord et al., [2017]) to a target codebook vector.\n\nTaken together, both capabilities can form a push-pull effect that achieves two desiderata of vector quantization: increasing information capacity and reducing distortion. Encoder outputs that have large angular distance to the chosen codebook vector are \u201cpushed\u201d to other, possibly unused, codebook regions by outwards-pointing gradients, thereby increasing codebook utilization. Concurrent with this effect, center-pointing gradients will \u201cpull\u201d points loosely clustered around the codebook vector closer together, locking on to the chosen codebook vector and reducing quantization error.\n\nA third and final difference with the STE occurs when \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L} is exactly orthogonal to q\ud835\udc5eq, i.e. \u03b8~=\u03c0/2~\ud835\udf03\ud835\udf0b2\\\\tilde{\\\\theta}=\\\\pi/2. For this event, each point within the Voronoi region generated by q\ud835\udc5eq is rotated by the same amount in the direction of \u2207q\u2112subscript\u2207\ud835\udc5e\u2112\\\\nabla\\_{q}\\\\mathcal{L}. The top left Voroni regions of [Figure4]In 4.3 Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\") visualize this case. While it is unlikely that the gradient is exactly orthogonal to the codebook vector, the gradient\u2019s orthogonal component to q\ud835\udc5eq\nenables points to move in orthogonal directions via a rotation and expansion (if \u03b8~<\u03c0/2~\ud835\udf03\ud835\udf0b2\\\\tilde{\\\\theta}<\\\\pi/2) or a rotation and contraction (if \u03b8~>\u03c0/2~\ud835\udf03\ud835\udf0b2\\\\tilde{\\\\theta}>\\\\pi/2).\n\n## 5 Experiments\nIn [Section4.3]Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\"), we showed the rotation trick enables behavior that would increase codebook utilization and reduce quantization error by changing how points within the same Voronoi region are updated. However, the extent to which these changes will affect applications is unclear. In this section, we evaluate the effect of the rotation trick across many different VQ-VAE paradigms.\n\nWe begin with image reconstruction: training a VQ-VAE with the reconstruction objective of Van Den Oord et al. ( [2017]) and later extend our evaluation to the more complex VQGANs (Esser et al., [2021]), the VQGANs designed for latent diffusion (Rombach et al., [2022]), and then the ViT-VQGAN (Yu et al., [2021]). Finally, we evaluate VQ-VAE reconstructions on videos using a TimeSformer (Bertasius et al., [2021]) encoder and decoder.\nIn total, our empirical analysis spans 11 different VQ-VAE configurations.\nFor all experiments, aside from handling \u2202\ud835\udc92\u2202\ud835\udc86\ud835\udc92\ud835\udc86\\\\bm{\\\\frac{\\\\partial q}{\\\\partial e}} differently, the models, hyperparameters, and training settings are identical.\n\nTable 1: Comparison of VQ-VAEs trained on ImageNet following Van Den Oord et al. ( [2017]). We use the Vector Quantization layer from [https://github.com/lucidrains/vector-quantize-pytorch].\n\n| Approach | Training Metrics | Validation Metrics |\n| Codebook Usage (\u2191\u2191\\\\uparrow) | Rec. Loss (\u2193\u2193\\\\downarrow) | Quantization Error (\u2193\u2193\\\\downarrow) | Rec. Loss (\u2193\u2193\\\\downarrow) | r-FID (\u2193\u2193\\\\downarrow) | r-IS (\u2191\u2191\\\\uparrow) |\n| Codebook Lookup: Euclidean & Latent Shape: 32\u00d732\u00d73232323232\\\\times 32\\\\times 32 & Codebook Size: 102410241024 |  |\n| VQ-VAE | 100% | 0.107 | 5.9e-3 | 0.115 | 106.1 | 11.7 |\n| VQ-VAE w/ Rotation Trick | 97% | 0.116 | 5.1e-4 | 0.122 | 85.7 | 17.0 |\n| Codebook Lookup: Cosine & Latent Shape: 32\u00d732\u00d73232323232\\\\times 32\\\\times 32 & Codebook Size: 102410241024 |  |\n| VQ-VAE | 75% | 0.107 | 2.9e-3 | 0.114 | 84.3 | 17.7 |\n| VQ-VAE w/ Rotation Trick | 91% | 0.105 | 2.7e-3 | 0.111 | 82.9 | 18.1 |\n| Codebook Lookup: Euclidean & Latent Shape: 64\u00d764\u00d736464364\\\\times 64\\\\times 3 & Codebook Size: 819281928192 |  |\n| VQ-VAE | 100% | 0.028 | 1.0e-3 | 0.030 | 19.0 | 97.3 |\n| VQ-VAE w/ Hessian Approx. | 39% | 0.082 | 6.9e-5 | 0.112 | 35.6 | 65.1 |\n| VQ-VAE w/ Exact Gradients | 84% | 0.050 | 2.0e-3 | 0.053 | 25.4 | 80.4 |\n| VQ-VAE w/ Rotation Trick | 99% | 0.028 | 1.4e-4 | 0.030 | 16.5 | 106.3 |\n| Codebook Lookup: Cosine & Latent Shape: 64\u00d764\u00d736464364\\\\times 64\\\\times 3 & Codebook Size: 819281928192 |  |\n| VQ-VAE | 31% | 0.034 | 1.2e-4 | 0.038 | 26.0 | 77.8 |\n| VQ-VAE w/ Hessian Approx. | 37% | 0.035 | 3.8e-5 | 0.037 | 29.0 | 71.5 |\n| VQ-VAE w/ Exact Gradients | 38% | 0.035 | 3.6e-5 | 0.037 | 28.2 | 75.0 |\n| VQ-VAE w/ Rotation Trick | 38% | 0.033 | 9.6e-5 | 0.035 | 24.2 | 83.9 |\n\n### 5.1 VQ-VAE Evaluation\n\nWe begin with a straightforward evaluation: training a VQ-VAE to reconstruct examples from ImageNet (Deng et al., [2009]). Following Van Den Oord et al. ( [2017]), our training objective is a linear combination of the reconstruction, codebook, and commitment loss:\n\n|     |     |     |\n| --- | --- | --- |\n|  | \u2112=\u2225x\u2212x~\u222522+\u2225s\u200bg\u200b(e)\u2212q\u222522+\u03b2\u200b\u2225e\u2212s\u200bg\u200b(q)\u222522\u2112superscriptsubscriptdelimited-\u2225\u2225\ud835\udc65~\ud835\udc6522superscriptsubscriptdelimited-\u2225\u2225\ud835\udc60\ud835\udc54\ud835\udc52\ud835\udc5e22\ud835\udefdsuperscriptsubscriptdelimited-\u2225\u2225\ud835\udc52\ud835\udc60\ud835\udc54\ud835\udc5e22\\\\displaystyle\\\\mathcal{L}={\\\\lVert x-\\\\tilde{x}\\\\rVert}\\_{2}^{2}+{\\\\lVert sg(e)-q\\\\rVert}\\_{2}^{2}+\\\\beta{\\\\lVert e-sg(q)\\\\rVert}\\_{2}^{2} |  |\n\nwhere \u03b2\ud835\udefd\\\\beta is a hyperparameter scaling constant. Following convention, we drop the codebook loss term from the objective and instead use an exponential moving average to update the codebook vectors.\n\nEvaluation Settings. For 256\u00d7256\u00d732562563256\\\\times 256\\\\times 3 input images, we evaluate two different settings: (1) compressing to a latent space of dimension 32\u00d732\u00d73232323232\\\\times 32\\\\times 32 with a codebook size of 102410241024 following Yu et al. ( [2021]) and (2) compressing to 64\u00d764\u00d736464364\\\\times 64\\\\times 3 with a codebook size of 819281928192 following Rombach et al. ( [2022]). In both settings, we compare with a Euclidean and cosine similarity codebook lookup.\n\nEvaluation Metrics. We log both training and validation set reconstruction metrics. Of note, we compute reconstruction FID (Heusel et al., [2017]) and reconstruction IS (Salimans et al., [2016]) on reconstructions from the full ImageNet validation set as a measure of reconstruction quality. We also compute codebook usage, or the percentage of codebook vectors that are used in each batch of data, as a measure of the information capacity of the vector quantization layer and quantization error \u2225e\u2212q\u222522superscriptsubscriptdelimited-\u2225\u2225\ud835\udc52\ud835\udc5e22{\\\\lVert e-q\\\\rVert}\\_{2}^{2} as a measure of distortion.\n\nBaselines. Our comparison spans the STE estimator (VQ-VAE), the Hessian approximation described in [Section3]Straight Through Estimator (STE) \u2023 Restructuring Vector Quantization with the Rotation Trick\") (VQ-VAE w/ Hessian Approx), the exact gradient backward pass described in [Section3]Straight Through Estimator (STE) \u2023 Restructuring Vector Quantization with the Rotation Trick\") (VQ-VAE w/ Exact Gradients), and the rotation trick (VQ-VAE w/ Rotation Trick). All methods share the same architecture, hyperparameters, and training settings, and these settings are summarized in [Table 6]Table 6 \u2023 A.5.1 VQ-VAE Evaluation. \u2023 A.5 Training Settings \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\") of the Appendix. There is no functional difference among methods in the forward pass; the only differences relates to how gradients are propagated through \u2202q\u2202e\ud835\udc5e\ud835\udc52\\\\frac{\\\\partial q}{\\\\partial e} during backpropagation.\n\nResults. [Table1]In 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") displays our findings. We find that using the rotation trick reduces the quantization error\u2014sometimes by an order of magnitude\u2014and improves low codebook utilization. Both results are expected given the Voronoi partition analysis in [Section4.3]Voronoi Partition Analysis \u2023 4 The Rotation Trick \u2023 Restructuring Vector Quantization with the Rotation Trick\"): points at the boundary of quantized regions are likely pushed to under-utilized codebook vectors while points loosely grouped around the codebook vector are condensed towards it. These two features appear to have a meaningful effect on reconstruction metrics: training a VQ-VAE with the rotation trick substantially improves r-FID and r-IS.\n\nWe also see that the Hessian Approximation or using Exact Gradients results in poor reconstruction performance. While the gradients to the encoder are, in a sense, \u201cmore accurate\u201d, training the encoder like an AutoEncoder (Hinton & Zemel, [1993]) likely introduces overfitting and poor generalization. Moreover, the mismatch in training objectives between the encoder and decoder is likely an aggravating factor and partly responsible for both models\u2019 poor performance.\n\nTable 2: Results for VQGAN designed for autoregressive generation as implemented in [https://github.com/CompVis/taming-transformers]. Experiments on ImageNet and the combined dataset FFHQ (Karras et al., [2019]) and CelebA-HQ (Karras, [2017]) use a latent bottleneck of dimension 16\u00d716\u00d7256161625616\\\\times 16\\\\times 256 with 102410241024 codebook vectors.\n\n| Approach | Dataset | Codebook Usage | Quantization Error (\u2193\u2193\\\\downarrow) | Valid Loss (\u2193\u2193\\\\downarrow) | r-FID (\u2193\u2193\\\\downarrow) | r-IS (\u2191\u2191\\\\uparrow) |\n| --- | --- | --- | --- | --- | --- | --- |\n| VQGAN (reported) | ImageNet | \u2014 | \u2014 | \u2014 | 7.9 | 114.4 |\n| VQGAN (our run) | ImageNet | 95% | 0.134 | 0.594 | 7.3 | 118.2 |\n| VQGAN w/ Rotation Trick | ImageNet | 98% | 0.002 | 0.422 | 4.6 | 146.5 |\n| VQGAN | FFHQ & CelebA-HQ | 27% | 0.233 | 0.565 | 4.7 | 5.0 |\n| VQGAN w/ Rotation Trick | FFHQ & CelebA-HQ | 99% | 0.002 | 0.313 | 3.7 | 5.2 |\n\nTable 3: Results for VQGAN designed for latent diffusion as implemented in [https://github.com/CompVis/latent-diffusion]. Both settings train on ImageNet.\n\n| Approach | Latent Shape | Codebook Size | Codebook Usage | Quantization Error (\u2193\u2193\\\\downarrow) | Valid Loss (\u2193\u2193\\\\downarrow) | r-FID (\u2193\u2193\\\\downarrow) | r-IS (\u2191\u2191\\\\uparrow) |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| VQGAN | 64\u00d764\u00d736464364\\\\times 64\\\\times 3 | 819281928192 | 15% | 2.5e-3 | 0.183 | 0.53 | 220.6 |\n| VQGAN w/ Rotation Trick | 64\u00d764\u00d736464364\\\\times 64\\\\times 3 | 819281928192 | 10% | 2.9e-4 | 0.173 | 0.38 | 222.4 |\n| VQGAN | 32\u00d732\u00d743232432\\\\times 32\\\\times 4 | 163841638416384 | 2% | 1.2e-2 | 0.385 | 5.0 | 141.5 |\n| VQGAN w/ Rotation Trick | 32\u00d732\u00d743232432\\\\times 32\\\\times 4 | 163841638416384 | 9% | 7.2e-4 | 0.287 | 1.6 | 190.3 |\n\n### 5.2 VQGAN Evaluation\n\nMoving to the next level of complexity, we evaluate the effect of the rotation trick on VQGANs (Esser et al., [2021]). The VQGAN training objective is:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2112VQGANsubscript\u2112VQGAN\\\\displaystyle\\\\mathcal{L}\\_{\\\\text{VQGAN}} | =\u2112Per+\u2225s\u200bg\u200b(e)\u2212q\u222522+\u03b2\u200b\u2225e\u2212s\u200bg\u200b(q)\u222522+\u03bb\u200b\u2112Advabsentsubscript\u2112Persuperscriptsubscriptdelimited-\u2225\u2225\ud835\udc60\ud835\udc54\ud835\udc52\ud835\udc5e22\ud835\udefdsuperscriptsubscriptdelimited-\u2225\u2225\ud835\udc52\ud835\udc60\ud835\udc54\ud835\udc5e22\ud835\udf06subscript\u2112Adv\\\\displaystyle=\\\\mathcal{L}\\_{\\\\text{Per}}+{\\\\lVert sg(e)-q\\\\rVert}\\_{2}^{2}+\\\\beta{\\\\lVert e-sg(q)\\\\rVert}\\_{2}^{2}+\\\\lambda\\\\mathcal{L}\\_{\\\\text{Adv}} |  |\n\nwhere \u2112Persubscript\u2112Per\\\\mathcal{L}\\_{\\\\text{Per}} is the perceptual loss from Johnson et al. ( [2016]) and replaces the L2subscript\ud835\udc3f2L\\_{2} loss used to train VQ-VAEs. LA\u200bd\u200bvsubscript\ud835\udc3f\ud835\udc34\ud835\udc51\ud835\udc63L\\_{Adv} is a patch-based adversarial loss similar to the adversarial loss in Conditional GAN (Isola et al., [2017]). \u03b2\ud835\udefd\\\\beta is a constant that weights the commitment loss while \u03bb\ud835\udf06\\\\lambda is an adaptive weight based on the ratio of \u2207\u2112Per\u2207subscript\u2112Per\\\\nabla\\\\mathcal{L}\\_{\\\\text{Per}} to \u2207\u2112Adv\u2207subscript\u2112Adv\\\\nabla\\\\mathcal{L}\\_{\\\\text{Adv}} with respect to the last layer of the decoder.\n\nExperimental Settings.\nWe evaluate VQGANs under two settings: (1) the paradigm amenable to autoregressive modeling with Transformers as described in Esser et al. ( [2021]) and (2) the paradigm suitable to latent diffusion models as described in Rombach et al. ( [2022]). The first setting follows the convolutional neural network and default hyperparameters described in Esser et al. ( [2021]) while the second follows those from Rombach et al. ( [2022]). A full description of both training settings is provided in [Table 7]Table 7 \u2023 A.5.2 VQGAN Evaluation \u2023 A.5 Training Settings \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\") of the Appendix.\n\nResults. Our results are listed in [Table2]In 5.1 VQ-VAE Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") for the first setting and [Table3]In 5.1 VQ-VAE Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") for the second. Similar to our findings in [Section5.1]VQ-VAE Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\"), we find that training a VQ-VAE with the rotation trick substantially decreases quantization error and often improves codebook usage. Moreover, reconstruction performance as measured on the validation set by the total loss, r-FID, and r-IS are significantly improved across both modeling paradigms.\n\nTable 4: Results for ViT-VQGAN (Yu et al., [2021]) trained on ImageNet. The latent shape is 8\u00d78\u00d73288328\\\\times 8\\\\times 32 with 819281928192 codebook vectors. r-FID and r-IS are reported on the validation set.\n\n| Approach | Codebook Usage (\u2191\u2191\\\\uparrow) | Train Loss (\u2193\u2193\\\\downarrow) | Quantization Error (\u2193\u2193\\\\downarrow) | Valid Loss (\u2193\u2193\\\\downarrow) | r-FID (\u2193\u2193\\\\downarrow) | r-IS (\u2191\u2191\\\\uparrow) |\n| --- | --- | --- | --- | --- | --- | --- |\n| ViT-VQGAN \\[reported\\] | \u2014 | \u2014 | \u2014 | \u2014 | 22.8 | 72.9 |\n| ViT-VQGAN \\[ours\\] | 0.3% | 0.124 | 6.7e-3 | 0.127 | 29.2 | 43.0 |\n| ViT-VQGAN w/ Rotation Trick | 2.2% | 0.113 | 8.3e-3 | 0.113 | 11.2 | 93.1 |\n\nTable 5: Results for TimeSformer-VQGAN trained on BAIR and UCF-101 with 1024 codebook vectors. \u2020\u2020\\\\dagger: model suffers from codebook collapse and diverges. r-FVD is computed on the validation set.\n\n| Approach | Dataset | Codebook Usage | Train Loss (\u2193\u2193\\\\downarrow) | Quantization Error (\u2193\u2193\\\\downarrow) | Valid Loss (\u2193\u2193\\\\downarrow) | r-FVD (\u2193\u2193\\\\downarrow) |\n| --- | --- | --- | --- | --- | --- | --- |\n| TimeSformer\u2020 | BAIR | 0.4% | 0.221 | 0.03 | 0.28 | 1661.1 |\n| TimeSformer w/ Rotation Trick | BAIR | 43% | 0.074 | 3.0e-3 | 0.074 | 21.4 |\n| TimeSformer\u2020 | UCF-101 | 0.1% | 0.190 | 0.006 | 0.169 | 2878.1 |\n| TimeSformer w/ Rotation Trick | UCF-101 | 30% | 0.111 | 0.020 | 0.109 | 229.1 |\n\n### 5.3 ViT-VQGAN Evaluation\n\nImproving upon the VQGAN model, Yu et al. ( [2021]) propose using a ViT (**[MASKED_CITATION]**) rather than CNN to parameterize the encoder and decoder. The ViT-VQGAN uses factorized codes and L2subscript\ud835\udc3f2L\\_{2} normalization on the output and input to the vector quantization layer to improve performance and training stability. Additionally, the authors change the training objective, adding a logit-laplace loss and restoring the L2subscript\ud835\udc3f2L\\_{2} reconstruction error to \u2112VQGANsubscript\u2112VQGAN\\\\mathcal{L}\\_{\\\\text{VQGAN}}.\n\nExperimental Settings. We follow the open source implementation of [https://github.com/thuanz123/enhancing-transformers] and use the default model and hyperparameter settings for the small ViT-VQGAN. A complete description of the training settings can be found in [Table8]In Figure 7 \u2023 A.5.3 ViT-VQGAN Evaluation \u2023 A.5 Training Settings \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\") of the Appendix.\n\nResults. [Table4]In 5.2 VQGAN Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") summarizes our findings. Similar to our previous results for VQ-VAEs in [Section5.1]VQ-VAE Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") and VQGANS in [Section5.2]VQGAN Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\"), codebook utilization and reconstruction metrics are significantly improved; however in this case, the quantization error is roughly the same.\n\n### 5.4 Video Evaluation\n\nExpanding our analysis beyond the image modality, we evaluate the effect of the rotation trick on video reconstructions from the BAIR Robot dataset (Ebert et al., [2017]) and from the UCF101 action recognition dataset (Soomro, [2012]). We follow the quantization paradigm used by ViT-VQGAN, but replace the ViT with a TimeSformer (Bertasius et al., [2021]) video model. Due to compute limitations, both encoder and decoder follow a relatively small TimeSformer model: 8 layers, 256 hidden dimensions, 4 attention heads, and 768 MLP hidden dimensions. A complete description of the architecture, training settings, and hyperparameters are provided in [SectionA.5.4]A.5.4 TimeSformer Video Evaluation \u2023 A.5 Training Settings \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\nResults. [Table5]In 5.2 VQGAN Evaluation \u2023 5 Experiments \u2023 Restructuring Vector Quantization with the Rotation Trick\") shows our results. For both datasets, training a TimeSformer-VQGAN model with the STE results in codebook collapse. We explored several different hyperparameter settings; however in all cases, codebook utilization drops to almost 0% within the first several epochs. On the other hand, models trained with the rotation trick do not exhibit any training instability and produce high quality reconstructions as indicated by r-FVD (Unterthiner et al., [2018]). Several non-cherry picked video reconstructions are displayed in [SectionA.5.4]A.5.4 TimeSformer Video Evaluation \u2023 A.5 Training Settings \u2023 Appendix A Appendix \u2023 Restructuring Vector Quantization with the Rotation Trick\").\n\n## 6 Conclusion\nIn this work, we explore different ways to propagate gradients through the vector quantization layer of VQ-VAEs and find that preserving the angle\u2014rather than the direction\u2014between the codebook vector and gradient induces desirable effects for how points within the same codebook region are updated. Specifically, the gradient to the encoder changes as it propagates through the vector quantization layer to push points with high distortion to new codebook regions or to pull them closer to the chosen codebook vector.\nThe former capability corresponds to increased codebook usage and higher information capacity and the latter to decreased quantization error and lower distortion.\n\nThese effects cause a substantial improvement in model performance. Across 11 different settings, we find that training VQ-VAEs with the rotation trick improves their reconstructions. For example, training one of the VQGANs used in latent diffusion with the rotation trick improves r-FID from 5.0 to 1.6 and r-IS from 141.5 to 190.3, reduces quantization error by two orders of magnitude, and increases codebook usage by 4.5x.</Paper>\n\n<References>\n<Candidate>Candidate [0]:\nAn image is worth 16x16 words: Transformers for image recognition at scale.\n\n<Abstract>Abstract: While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.</Abstract><Content>\n## 1 Introduction\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., [2017])), have become the model of choice in natural language processing (NLP).\nThe dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset (Devlin et al., [2019])).\nThanks to Transformers\u2019 computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., [2020]); Lepikhin et al., [2020])).\nWith the models and datasets growing, there is still no sign of saturating performance.\n\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., [1989]); Krizhevsky et al., [2012]); He et al., [2016])).\nInspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., [2018]); Carion et al., [2020])), some replacing the convolutions entirely (Ramachandran et al., [2019]); Wang et al., [2020a])).\nThe latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns.\nTherefore, in large-scale image recognition, classic ResNet-like architectures are still state of the art (Mahajan et al., [2018]); Xie et al., [2020]); Kolesnikov et al., [2020])).\n\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications.\nTo do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.\nImage patches are treated the same way as tokens (words) in an NLP application.\nWe train the model on image classification in supervised fashion.\n\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size.\nThis seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.\n\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images).\nWe find that large scale training trumps inductive bias.\nOur Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.\nWhen pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks.\nIn particular, the best model reaches the accuracy of 88.55%percent88.5588.55\\\\% on ImageNet, 90.72%percent90.7290.72\\\\% on ImageNet-ReaL, 94.55%percent94.5594.55\\\\% on CIFAR-100, and 77.63%percent77.6377.63\\\\% on the VTAB suite of 19 tasks.\n\n## 2 Related Work\nTransformers were proposed by Vaswani et al. ( [2017])) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., [2019])) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., [2018]); [2019]); Brown et al., [2020])).\n\nNaive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes.\nThus, to apply Transformers in the context of image processing, several approximations have been tried in the past.\nParmar et al. ( [2018])) applied the self-attention only in local neighborhoods for each query pixel instead of globally.\nSuch local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., [2019]); Ramachandran et al., [2019]); Zhao et al., [2020])).\nIn a different line of work, Sparse Transformers (Child et al., [2019])) employ scalable approximations to global self-attention in order to be applicable to images.\nAn alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., [2019])), in the extreme case only along individual axes (Ho et al., [2019]); Wang et al., [2020a])).\nMany of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators.\n\nMost related to ours is the model of Cordonnier et al. ( [2020])), which extracts patches of size 2\u00d72222\\\\times 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. ( [2020])) use a small patch size of 2\u00d72222\\\\times 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.\n\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification (Bello et al., [2019])) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., [2018]); Carion et al., [2020])), video processing (Wang et al., [2018]); Sun et al., [2019])), image classification (Wu et al., [2020])), unsupervised object discovery (Locatello et al., [2020])), or unified text-vision tasks (Chen et al., [2020c]); Lu et al., [2019]); Li et al., [2019])).\n\nAnother recent related model is image GPT (iGPT) (Chen et al., [2020a])), which applies Transformers to image pixels after reducing image resolution and color space. The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet.\n\nOur work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset.\nThe use of additional data sources allows to achieve state-of-the-art results on standard benchmarks (Mahajan et al., [2018]); Touvron et al., [2019]); Xie et al., [2020])).\nMoreover, Sun et al. ( [2017])) study how CNN performance scales with dataset size, and Kolesnikov et al. ( [2020])); Djolonga et al. ( [2020])) perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M.\nWe focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works.\n\n## 3 Method\n|     |\n| --- |\n| ![Refer to caption] |\n\nFigure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable \u201cclassification token\u201d to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. ( [2017])).\n\nIn model design we follow the original Transformer (Vaswani et al., [2017])) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and their efficient implementations \u2013 can be used almost out of the box.\n\n### 3.1 Vision Transformer (ViT)\n\nAn overview of the model is depicted in Figure [1]Figure 1 \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\").\nThe standard Transformer receives as input a 1D sequence of token embeddings.\nTo handle 2D images, we reshape the image \ud835\udc31\u2208\u211dH\u00d7W\u00d7C\ud835\udc31superscript\u211d\ud835\udc3b\ud835\udc4a\ud835\udc36\\\\mathbf{x}\\\\in\\\\mathbb{R}^{H\\\\times W\\\\times C} into a sequence of flattened 2D patches \ud835\udc31p\u2208\u211dN\u00d7(P2\u22c5C)subscript\ud835\udc31\ud835\udc5dsuperscript\u211d\ud835\udc41\u22c5superscript\ud835\udc432\ud835\udc36\\\\mathbf{x}\\_{p}\\\\in\\\\mathbb{R}^{N\\\\times(P^{2}\\\\cdot C)}, where (H,W)\ud835\udc3b\ud835\udc4a(H,W) is the resolution of the original image, C\ud835\udc36C is the number of channels, (P,P)\ud835\udc43\ud835\udc43(P,P) is the resolution of each image patch, and N=H\u200bW/P2\ud835\udc41\ud835\udc3b\ud835\udc4asuperscript\ud835\udc432N=HW/P^{2} is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.\nThe Transformer uses constant latent vector size D\ud835\udc37D through all of its layers, so we flatten the patches and map to D\ud835\udc37D dimensions with a trainable linear projection (Eq. [1]In 3.1 Vision Transformer (ViT) \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\nWe refer to the output of this projection as the patch embeddings.\n\nSimilar to BERT\u2019s `[class]` token, we prepend a learnable embedding to the sequence of embedded patches (\ud835\udc3300=\ud835\udc31classsuperscriptsubscript\ud835\udc3300subscript\ud835\udc31class\\\\mathbf{z}\\_{0}^{0}=\\\\mathbf{x}\\_{\\\\text{class}}), whose state at the output of the Transformer encoder (\ud835\udc33L0subscriptsuperscript\ud835\udc330\ud835\udc3f\\\\mathbf{z}^{0}\\_{L}) serves as the image representation \ud835\udc32\ud835\udc32\\\\mathbf{y} (Eq. [4]In 3.1 Vision Transformer (ViT) \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\nBoth during pre-training and fine-tuning, a classification head is attached to \ud835\udc33L0subscriptsuperscript\ud835\udc330\ud835\udc3f\\\\mathbf{z}^{0}\\_{L}.\nThe classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\n\nPosition embeddings are added to the patch embeddings to retain positional information.\nWe use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix [D.4]D.4 Positional Embedding \u2023 Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\nThe resulting sequence of embedding vectors serves as input to the encoder.\n\nThe Transformer encoder (Vaswani et al., [2017])) consists of alternating layers of multiheaded self-attention (MSA, see Appendix [A]Appendix A Multihead Self-attention \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")) and MLP blocks (Eq. [2]In 3.1 Vision Transformer (ViT) \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"), [3]In 3.1 Vision Transformer (ViT) \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\nLayernorm (LN) is applied before every block, and residual connections after every block (Wang et al., [2019]); Baevski & Auli, [2019])).\nThe MLP contains two layers with a GELU non-linearity.\n\n|     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- |\n|  | \ud835\udc330subscript\ud835\udc330\\\\displaystyle\\\\mathbf{z}\\_{0} | =\\[\ud835\udc31class;\ud835\udc31p1\u200b\ud835\udc04;\ud835\udc31p2\u200b\ud835\udc04;\u22ef;\ud835\udc31pN\u200b\ud835\udc04\\]+\ud835\udc04p\u200bo\u200bs,absentsubscript\ud835\udc31classsubscriptsuperscript\ud835\udc311\ud835\udc5d\ud835\udc04subscriptsuperscript\ud835\udc312\ud835\udc5d\ud835\udc04\u22efsubscriptsuperscript\ud835\udc31\ud835\udc41\ud835\udc5d\ud835\udc04subscript\ud835\udc04\ud835\udc5d\ud835\udc5c\ud835\udc60\\\\displaystyle=\\[\\\\mathbf{x}\\_{\\\\text{class}};\\\\,\\\\mathbf{x}^{1}\\_{p}\\\\mathbf{E};\\\\,\\\\mathbf{x}^{2}\\_{p}\\\\mathbf{E};\\\\cdots;\\\\,\\\\mathbf{x}^{N}\\_{p}\\\\mathbf{E}\\]+\\\\mathbf{E}\\_{pos}, | \ud835\udc04\u2208\u211d(P2\u22c5C)\u00d7D,\ud835\udc04p\u200bo\u200bs\u2208\u211d(N+1)\u00d7Dformulae-sequence\ud835\udc04superscript\u211d\u22c5superscript\ud835\udc432\ud835\udc36\ud835\udc37subscript\ud835\udc04\ud835\udc5d\ud835\udc5c\ud835\udc60superscript\u211d\ud835\udc411\ud835\udc37\\\\displaystyle\\\\mathbf{E}\\\\in\\\\mathbb{R}^{(P^{2}\\\\cdot C)\\\\times D},\\\\,\\\\mathbf{E}\\_{pos}\\\\in\\\\mathbb{R}^{(N+1)\\\\times D} |  | (1) |\n|  | \ud835\udc33\u2032\u2113subscriptsuperscript\ud835\udc33\u2032\u2113\\\\displaystyle\\\\mathbf{z^{\\\\prime}}\\_{\\\\ell} | =MSA\u2061(LN\u2061(\ud835\udc33\u2113\u22121))+\ud835\udc33\u2113\u22121,absentMSALNsubscript\ud835\udc33\u21131subscript\ud835\udc33\u21131\\\\displaystyle=\\\\operatorname{MSA}(\\\\operatorname{LN}(\\\\mathbf{z}\\_{\\\\ell-1}))+\\\\mathbf{z}\\_{\\\\ell-1}, | \u2113=1\u200b\u2026\u200bL\u21131\u2026\ud835\udc3f\\\\displaystyle\\\\ell=1\\\\ldots L |  | (2) |\n|  | \ud835\udc33\u2113subscript\ud835\udc33\u2113\\\\displaystyle\\\\mathbf{z}\\_{\\\\ell} | =MLP\u2061(LN\u2061(\ud835\udc33\u2032\u2113))+\ud835\udc33\u2032\u2113,absentMLPLNsubscriptsuperscript\ud835\udc33\u2032\u2113subscriptsuperscript\ud835\udc33\u2032\u2113\\\\displaystyle=\\\\operatorname{MLP}(\\\\operatorname{LN}(\\\\mathbf{z^{\\\\prime}}\\_{\\\\ell}))+\\\\mathbf{z^{\\\\prime}}\\_{\\\\ell}, | \u2113=1\u200b\u2026\u200bL\u21131\u2026\ud835\udc3f\\\\displaystyle\\\\ell=1\\\\ldots L |  | (3) |\n|  | \ud835\udc32\ud835\udc32\\\\displaystyle\\\\mathbf{y} | =LN\u2061(\ud835\udc33L0)absentLNsuperscriptsubscript\ud835\udc33\ud835\udc3f0\\\\displaystyle=\\\\operatorname{LN}(\\\\mathbf{z}\\_{L}^{0}) |  | (4) |\n\n##### Inductive bias.\n\nWe note that Vision Transformer has much less image-specific inductive bias than CNNs.\nIn CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model.\nIn ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.\nThe two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below).\nOther than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.\n\n##### Hybrid Architecture.\n\nAs an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., [1989])).\nIn this hybrid model, the patch embedding projection \ud835\udc04\ud835\udc04\\\\mathbf{E} (Eq. [1]In 3.1 Vision Transformer (ViT) \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")) is applied to patches extracted from a CNN feature map.\nAs a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classification input embedding and position embeddings are added as described above.\n\n### 3.2 Fine-tuning and Higher Resolution\n\nTypically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks.\nFor this, we remove the pre-trained prediction head and attach a zero-initialized D\u00d7K\ud835\udc37\ud835\udc3eD\\\\times K feedforward layer, where K\ud835\udc3eK is the number of downstream classes.\nIt is often beneficial to fine-tune at higher resolution than pre-training (Touvron et al., [2019]); Kolesnikov et al., [2020])).\nWhen feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length.\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful.\nWe therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image.\nNote that this resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n\n## 4 Experiments\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the hybrid.\nTo understand the data requirements of each model, we pre-train on datasets of varying size and evaluate many benchmark tasks.\nWhen considering the computational cost of pre-training the model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at a lower pre-training cost.\nLastly, we perform a small experiment using self-supervision, and show that self-supervised ViT holds promise for the future.\n\n### 4.1 Setup\n\nDatasets.\nTo explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows),\nits superset ImageNet-21k with 21k classes and 14M images (Deng et al., [2009])),\nand JFT (Sun et al., [2017])) with 18k classes and 303M high-resolution images.\nWe de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following Kolesnikov et al. ( [2020])).\nWe transfer the models trained on these dataset to several benchmark tasks:\nImageNet on the original validation labels and the cleaned-up ReaL labels (Beyer et al., [2020])),\nCIFAR-10/100 (Krizhevsky, [2009])),\nOxford-IIIT Pets (Parkhi et al., [2012])),\nand Oxford Flowers-102 (Nilsback & Zisserman, [2008])).\nFor these datasets, pre-processing follows Kolesnikov et al. ( [2020])).\n\nWe also evaluate on the 19-task VTAB classification suite (Zhai et al., [2019b])).\nVTAB evaluates low-data transfer to diverse tasks, using 1\u2009000 training examples per task.\nThe tasks are divided into three groups:\nNatural \u2013 tasks like the above, Pets, CIFAR, etc.\nSpecialized \u2013 medical and satellite imagery, and\nStructured \u2013 tasks that require geometric understanding like localization.\n\nModel Variants.\nWe base ViT configurations on those used for BERT (Devlin et al., [2019])), as summarized in Table [1]Table 1 \u2023 4.1 Setup \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\").\nThe \u201cBase\u201d and \u201cLarge\u201d models are directly adopted from BERT and we add the larger \u201cHuge\u201d model.\nIn what follows we use brief notation to indicate the model size and the input patch size: for instance, ViT-L/16 means the \u201cLarge\u201d variant with 16\u00d716161616\\\\times 16 input patch size.\nNote that the Transformer\u2019s sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive.\n\nFor the baseline CNNs, we use ResNet (He et al., [2016])), but replace the Batch Normalization layers (Ioffe & Szegedy, [2015])) with Group Normalization (Wu & He, [2018])), and used standardized convolutions (Qiao et al., [2019])).\nThese modifications improve transfer (Kolesnikov et al., [2020])), and we denote the modified model \u201cResNet (BiT)\u201d.\nFor the hybrids, we feed the intermediate feature maps into ViT with patch size of one \u201cpixel\u201d.\nTo experiment with different sequence lengths, we either\n(i) take the output of stage 4 of a regular ResNet50 or\n(ii) remove stage 4, place the same number of layers in stage 3 (keeping the total number of layers), and take the output of this extended stage 3.\nOption (ii) results in a 4x longer sequence length, and a more expensive ViT model.\n\n| Model | Layers | Hidden size D\ud835\udc37D | MLP size | Heads | Params |\n| --- | --- | --- | --- | --- | --- |\n| ViT-Base | 12 | 768 | 3072 | 12 | 86M |\n| ViT-Large | 24 | 1024 | 4096 | 16 | 307M |\n| ViT-Huge | 32 | 1280 | 5120 | 16 | 632M |\n\nTable 1: Details of Vision Transformer model variants.\n\nTraining & Fine-tuning.\nWe train all models, including ResNets, using Adam (Kingma & Ba, [2015])) with \u03b21=0.9subscript\ud835\udefd10.9\\\\beta\\_{1}=0.9, \u03b22=0.999subscript\ud835\udefd20.999\\\\beta\\_{2}=0.999, a batch size of 4096 and apply a high weight decay of 0.10.10.1, which we found to be useful for transfer of all models (Appendix [D.1]D.1 SGD vs. Adam for ResNets \u2023 Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting).\nWe use a linear learning rate warmup and decay, see Appendix [B.1]B.1 Training \u2023 Appendix B Experiment details \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") for details.\nFor fine-tuning we use SGD with momentum, batch size 512, for all models, see Appendix [B.1.1]B.1.1 Fine-tuning \u2023 B.1 Training \u2023 Appendix B Experiment details \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\").\nFor ImageNet results in Table [2]Table 2 \u2023 4.2 Comparison to State of the Art \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"), we fine-tuned at higher resolution: 512512512 for ViT-L/16 and 518518518 for ViT-H/14, and also used Polyak & Juditsky ( [1992])) averaging with a factor of 0.99990.99990.9999(Ramachandran et al., [2019]); Wang et al., [2020b])).\n\nMetrics.\nWe report results on downstream datasets either through few-shot or fine-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after fine-tuning it on the respective dataset.\nFew-shot accuracies are obtained by solving a regularized least-squares regression problem that maps the (frozen) representation of a subset of training images to {\u22121,1}Ksuperscript11\ud835\udc3e\\\\{-1,1\\\\}^{K} target vectors.\nThis formulation allows us to recover the exact solution in closed form.\nThough we mainly focus on fine-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-fly evaluation where fine-tuning would be too costly.\n\n### 4.2 Comparison to State of the Art\n\nWe first compare our largest models \u2013 ViT-H/14 and ViT-L/16  \u2013 to state-of-the-art CNNs from the literature.\nThe first comparison point is Big Transfer (BiT) (Kolesnikov et al., [2020])), which performs supervised transfer learning with large ResNets.\nThe second is Noisy Student (Xie et al., [2020])),\nwhich is a large EfficientNet trained using semi-supervised learning on ImageNet and JFT-300M with the labels removed.\nCurrently, Noisy Student is the state of the art on ImageNet and BiT-L on the other datasets reported here.\nAll models were trained on TPUv3 hardware, and we report the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU v3 cores (2 per chip) used for training multiplied by the training time in days.\n\n|  | Ours-JFT | Ours-JFT | Ours-I21k | BiT-L | Noisy Student |\n| --- | --- | --- | --- | --- | --- |\n|  | (ViT-H/14) | (ViT-L/16) | (ViT-L/16) | (ResNet152x4) | (EfficientNet-L2) |\n| --- | --- | --- | --- | --- | --- |\n| ImageNet | 88.55\u00b1\u20090.04plus-or-minus88.550.04\\\\mathbf{88.55}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.04} | 87.76\u00b1\u20090.03plus-or-minus87.760.0387.76{\\\\scriptstyle\\\\,\\\\pm\\\\,0.03} | 85.30\u00b1\u20090.02plus-or-minus85.300.0285.30{\\\\scriptstyle\\\\,\\\\pm\\\\,0.02} | 87.54\u00b1\u20090.02plus-or-minus87.540.0287.54{\\\\scriptstyle\\\\,\\\\pm\\\\,0.02} | 88.4/88.5\u221788.4superscript88.588.4/88.5^{\\*} |\n| ImageNet ReaL | 90.72\u00b1\u20090.05plus-or-minus90.720.05\\\\mathbf{90.72}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.05} | 90.54\u00b1\u20090.03plus-or-minus90.540.0390.54{\\\\scriptstyle\\\\,\\\\pm\\\\,0.03} | 88.62\u00b1\u20090.05plus-or-minus88.620.0588.62{\\\\scriptstyle\\\\,\\\\pm\\\\,0.05} | 90.5490.5490.54 | 90.5590.5590.55 |\n| CIFAR-10 | 99.50\u00b1\u20090.06plus-or-minus99.500.06\\\\mathbf{99.50}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.06} | 99.42\u00b1\u20090.03plus-or-minus99.420.0399.42{\\\\scriptstyle\\\\,\\\\pm\\\\,0.03} | 99.15\u00b1\u20090.03plus-or-minus99.150.0399.15{\\\\scriptstyle\\\\,\\\\pm\\\\,0.03} | 99.37\u00b1\u20090.06plus-or-minus99.370.0699.37{\\\\scriptstyle\\\\,\\\\pm\\\\,0.06} | \u2212- |\n| CIFAR-100 | 94.55\u00b1\u20090.04plus-or-minus94.550.04\\\\mathbf{94.55}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.04} | 93.90\u00b1\u20090.05plus-or-minus93.900.0593.90{\\\\scriptstyle\\\\,\\\\pm\\\\,0.05} | 93.25\u00b1\u20090.05plus-or-minus93.250.0593.25{\\\\scriptstyle\\\\,\\\\pm\\\\,0.05} | 93.51\u00b1\u20090.08plus-or-minus93.510.0893.51{\\\\scriptstyle\\\\,\\\\pm\\\\,0.08} | \u2212- |\n| Oxford-IIIT Pets | 97.56\u00b1\u20090.03plus-or-minus97.560.03\\\\mathbf{97.56}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.03} | 97.32\u00b1\u20090.11plus-or-minus97.320.1197.32{\\\\scriptstyle\\\\,\\\\pm\\\\,0.11} | 94.67\u00b1\u20090.15plus-or-minus94.670.1594.67{\\\\scriptstyle\\\\,\\\\pm\\\\,0.15} | 96.62\u00b1\u20090.23plus-or-minus96.620.2396.62{\\\\scriptstyle\\\\,\\\\pm\\\\,0.23} | \u2212- |\n| Oxford Flowers-102 | 99.68\u00b1\u20090.02plus-or-minus99.680.0299.68{\\\\scriptstyle\\\\,\\\\pm\\\\,0.02} | 99.74\u00b1\u20090.00plus-or-minus99.740.00\\\\mathbf{99.74}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.00} | 99.61\u00b1\u20090.02plus-or-minus99.610.0299.61{\\\\scriptstyle\\\\,\\\\pm\\\\,0.02} | 99.63\u00b1\u20090.03plus-or-minus99.630.0399.63{\\\\scriptstyle\\\\,\\\\pm\\\\,0.03} | \u2212- |\n| VTAB (19 tasks) | 77.63\u00b1\u20090.23plus-or-minus77.630.23\\\\mathbf{77.63}{\\\\scriptstyle\\\\,\\\\pm\\\\,0.23} | 76.28\u00b1\u20090.46plus-or-minus76.280.4676.28{\\\\scriptstyle\\\\,\\\\pm\\\\,0.46} | 72.72\u00b1\u20090.21plus-or-minus72.720.2172.72{\\\\scriptstyle\\\\,\\\\pm\\\\,0.21} | 76.29\u00b1\u20091.70plus-or-minus76.291.7076.29{\\\\scriptstyle\\\\,\\\\pm\\\\,1.70} | \u2212- |\n| TPUv3-core-days | 2.52.52.5k | 0.680.680.68k | 0.230.230.23k | 9.99.99.9k | 12.312.312.3k |\n\nTable 2:\nComparison with state of the art on popular image classification benchmarks.\nWe report mean and standard deviation of the accuracies, averaged over three fine-tuning runs.\nVision Transformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all datasets, while taking substantially less computational resources to pre-train.\nViT pre-trained on the smaller public ImageNet-21k dataset performs well too.\n\u2217Slightly improved 88.5%percent88.588.5\\\\% result reported in Touvron et al. ( [2020])).![Refer to caption]Figure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\n\nTable [2]Table 2 \u2023 4.2 Comparison to State of the Art \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") shows the results.\nThe smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L (which is pre-trained on the same dataset) on all tasks, while requiring substantially less computational resources to train.\nThe larger model, ViT-H/14, further improves the performance, especially on the more challenging datasets \u2013 ImageNet, CIFAR-100, and the VTAB suite.\nInterestingly, this model still took substantially less compute to pre-train than prior state of the art. However, we note that pre-training efficiency may be affected not only by the architecture choice, but also other parameters, such as training schedule, optimizer, weight decay, etc.\nWe provide a controlled study of performance vs. compute for different architectures in Section [4.4]Scaling Study \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\").\nFinally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.\n\nFigure [2]Figure 2 \u2023 4.2 Comparison to State of the Art \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") decomposes the VTAB tasks into their respective groups, and compares to previous SOTA methods on this benchmark:\nBiT,\nVIVI \u2013 a ResNet co-trained on ImageNet and Youtube (Tschannen et al., [2020])),\nand S4L \u2013 supervised plus semi-supervised learning on ImageNet (Zhai et al., [2019a])).\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks.\nOn the Specialized the performance of the top two models is similar.\n\n### 4.3 Pre-training Data Requirements\n\n![Refer to caption]Figure 3: Transfer to ImageNet. While large ViT models perform worse than BiT ResNets (shaded area) when pre-trained on small datasets, they shine when pre-trained on larger datasets. Similarly, larger ViT variants overtake smaller ones as the dataset grows.\n\n![Refer to caption]Figure 4: Linear few-shot evaluation on ImageNet versus pre-training size.\nResNets perform better with smaller pre-training datasets but plateau sooner than ViT, which performs better with larger pre-training. ViT-b is ViT-B with all hidden dimensions halved.\n\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset.\nWith fewer inductive biases for vision than ResNets, how crucial is the dataset size?\nWe perform two series of experiments.\n\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M.\nTo boost the performance on the smaller datasets, we optimize three basic regularization parameters \u2013 weight decay, dropout, and label smoothing.\nFigure [4]Figure 4 \u2023 4.3 Pre-training Data Requirements \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") shows the results after fine-tuning to ImageNet (results on other datasets are shown in Table [5]Table 5 \u2023 Appendix C Additional Results \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"))222Note that the ImageNet pre-trained models are also fine-tuned, but again on ImageNet. This is because the resolution increase during fine-tuning improves the performance..\nWhen pre-trained on the smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite (moderate) regularization.\nWith ImageNet-21k pre-training, their performances are similar.\nOnly with JFT-300M, do we see the full benefit of larger models.\nFigure [4]Figure 4 \u2023 4.3 Pre-training Data Requirements \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") also shows the performance region spanned by BiT models of different sizes.\nThe BiT CNNs outperform ViT on ImageNet, but with the larger datasets, ViT overtakes.\n\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-300M dataset.\nWe do not perform additional regularization on the smaller subsets and use the same hyper-parameters for all settings.\nThis way, we assess the intrinsic model properties, and not the effect of regularization.\nWe do, however, use early-stopping, and report the best validation accuracy achieved during training.\nTo save compute, we report few-shot linear accuracy instead of full fine-tuning accuracy.\nFigure [4]Figure 4 \u2023 4.3 Pre-training Data Requirements \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") contains the results.\nVision Transformers overfit more than ResNets with comparable computational cost on smaller datasets.\nFor example, ViT-B/32 is slightly faster than ResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets.\nThe same is true for ResNet152x2 and ViT-L/16.\nThis result reinforces the intuition that the convolutional inductive bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from data is sufficient, even beneficial.\n\nOverall, the few-shot results on ImageNet (Figure [4]Figure 4 \u2023 4.3 Pre-training Data Requirements \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")), as well as the low-data results on VTAB (Table [2]Table 2 \u2023 4.2 Comparison to State of the Art \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT is an exciting direction of future work.\n\n![Refer to caption]Figure 5: Performance versus pre-training compute for different architectures: Vision Transformers, ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\n\n### 4.4 Scaling Study\n\nWe perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M.\nIn this setting data size does not bottleneck the models\u2019 performances, and we assess performance versus pre-training cost of each model.\nThe model set includes:\n7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs;\n6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs;\nand 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone).\n\nFigure [5]Figure 5 \u2023 4.3 Pre-training Data Requirements \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") contains the transfer performance versus total pre-training compute (see Appendix [D.5]D.5 Empirical Computational Costs \u2023 Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") for details on computational costs).\nDetailed results per model are provided in Table [6]Table 6 \u2023 Appendix C Additional Results \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") in the Appendix.\nA few patterns can be observed.\nFirst, Vision Transformers dominate ResNets on the performance/compute trade-off.\nViT uses approximately 2\u22124\u00d72-4\\\\times less compute to attain the same performance (average over 5 datasets).\nSecond, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models.\nThis result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size.\nThird, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n\n### 4.5 Inspecting Vision Transformer\n\n![Refer to caption]Figure 6: Representative examples of attention from the output token to the input space. See Appendix [D.7]D.7 Attention Distance \u2023 Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") for details.\n\nTo begin to understand how the Vision Transformer processes image data, we analyze its internal representations.\nThe first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. [1]In 3.1 Vision Transformer (ViT) \u2023 3 Method \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\nFigure [7]Figure 7 \u2023 4.5 Inspecting Vision Transformer \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") (left) shows the top principal components of the the learned embedding filters.\nThe components resemble plausible basis functions for a low-dimensional representation of the fine structure within each patch.\n\nAfter the projection, a learned position embedding is added to the patch representations.\nFigure [7]Figure 7 \u2023 4.5 Inspecting Vision Transformer \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") (center) shows that the model learns to encode distance within the image in the similarity of position embeddings, i.e. closer patches tend to have more similar position embeddings.\nFurther, the row-column structure appears; patches in the same row/column have similar embeddings.\nFinally, a sinusoidal structure is sometimes apparent for larger grids (Appendix [D]Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\nThat the position embeddings learn to represent 2D image topology explains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix [D.4]D.4 Positional Embedding \u2023 Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\n\n![Refer to caption]\n\n![Refer to caption]![Refer to caption]\n\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32.\nCenter: Similarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position embedding of the patch with the indicated row and column and the position embeddings of all other patches.\nRight: Size of attended area by head and network depth. Each dot shows the mean attention distance across images for one of 16 heads at one layer. See Appendix [D.7]D.7 Attention Distance \u2023 Appendix D Additional Analyses \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") for details.\n\nSelf-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Specifically, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure [7]Figure 7 \u2023 4.5 Inspecting Vision Transformer \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"), right). This \u201cattention distance\u201d is analogous to receptive field size in CNNs. We find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads have consistently small attention distances in the low layers. This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer (Figure [7]Figure 7 \u2023 4.5 Inspecting Vision Transformer \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"), right), suggesting that it may serve a similar function as early convolutional layers in CNNs.\nFurther, the attention distance increases with network depth.\nGlobally, we find that the model attends to image regions that are semantically relevant for classification (Figure [6]Figure 6 \u2023 4.5 Inspecting Vision Transformer \u2023 4 Experiments \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\")).\n\n### 4.6 Self-supervision\n\nTransformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin et al., [2019]); Radford et al., [2018])). We also perform a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT. With self-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a significant improvement of 2% to training from scratch, but still 4% behind supervised pre-training. Appendix [B.1.2]B.1.2 Self-supervision \u2023 B.1 Training \u2023 Appendix B Experiment details \u2023 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\") contains further details.\nWe leave exploration of contrastive pre-training (Chen et al., [2020b]); He et al., [2020]); Bachman et al., [2019]); H\u00e9naff et al., [2020])) to future work.\n\n## 5 Conclusion\nWe have explored the direct application of Transformers to image recognition.\nUnlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step.\nInstead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP.\nThis simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classification datasets, whilst being relatively cheap to pre-train.\n\nWhile these initial results are encouraging, many challenges remain.\nOne is to apply ViT to other computer vision tasks, such as detection and segmentation.\nOur results, coupled with those in Carion et al. ( [2020])), indicate the promise of this approach.\nAnother challenge is to continue exploring self-supervised pre-training methods.\nOur initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training.\nFinally, further scaling of ViT would likely lead to improved performance.\n</Content></Candidate>\n\n<Candidate>Candidate [1]:\nEstimating or propagating gradients through stochastic neurons for conditional computation.\n\n<Abstract>Abstract: Stochastic neurons and hard non-linearities can be useful for a number of\nreasons in deep learning models, but in many cases they pose a challenging\nproblem: how to estimate the gradient of a loss function with respect to the\ninput of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\"\nthrough these stochastic neurons? We examine this question, existing\napproaches, and compare four families of solutions, applicable in different\nsettings. One of them is the minimum variance unbiased gradient estimator for\nstochatic binary neurons (a special case of the REINFORCE algorithm). A second\napproach, introduced here, decomposes the operation of a binary stochastic\nneuron into a stochastic binary part and a smooth differentiable part, which\napproximates the expected effect of the pure stochatic binary neuron to first\norder. A third approach involves the injection of additive or multiplicative\nnoise in a computational graph that is otherwise differentiable. A fourth\napproach heuristically copies the gradient with respect to the stochastic\noutput directly as an estimator of the gradient with respect to the sigmoid\nargument (we call this the straight-through estimator). To explore a context\nwhere these estimators are useful, we consider a small-scale version of {\\em\nconditional computation}, where sparse stochastic units form a distributed\nrepresentation of gaters that can turn off in combinatorially many ways large\nchunks of the computation performed in the rest of the neural network. In this\ncase, it is important that the gating units produce an actual 0 most of the\ntime. The resulting sparsity can be potentially be exploited to greatly reduce\nthe computational cost of large deep networks for which conditional computation\nwould be useful.</Abstract><Content>\n## 1 Introduction and Background\nMany learning algorithms and in particular those based on neural networks\nor deep learning rely on gradient-based learning. To compute exact gradients,\nit is better if the relationship between parameters and the training objective\nis continuous and generally smooth. If it is only constant by parts, i.e., mostly flat, then\ngradient-based learning is impractical. This was what motivated the\nmove from neural networks based on so-called formal neurons, with a hard\nthreshold output, to neural networks whose units are based on a\nsigmoidal non-linearity, and the well-known back-propagation\nalgorithm to compute the gradients (Rumelhart et al., [1986])).\n\nWe call the computational graph or flow graph the graph that\nrelates inputs and parameters to outputs and training criterion. Although\nit had been taken for granted by most researchers that smoothness of this\ngraph was a necessary condition for exact gradient-based training methods\nto work well, recent successes of deep networks with rectifiers and other\n\u201cnon-smooth\u201d\nnon-linearities (Glorot et al., [2011]); Krizhevsky et al., [2012a]); Goodfellow et al., [2013]))\nclearly question that belief: see Section [2]Non-Smooth Stochastic Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\") for a deeper\ndiscussion.\n\nIn principle, even if there are hard decisions (such as the\ntreshold function typically found in formal neurons) in the computational\ngraph, it is possible to obtain estimated gradients by introducing\nperturbations in the system and observing the effects. Although\nfinite-difference approximations of the gradient\nappear hopelessly inefficient (because independently perturbing\neach of N\ud835\udc41N parameters to estimate its gradient would be N\ud835\udc41N times\nmore expensive than ordinary back-propagation), another option\nis to introduce random perturbations, and this idea has\nbeen pushed far (and experimented on neural networks for control)\nby Spall ( [1992])) with the\nSimultaneous Perturbation Stochastic Approximation (SPSA) algorithm.\n\nAs discussed here (Section [2]Non-Smooth Stochastic Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\")), non-smooth non-linearities\nand stochastic perturbations can be combined to obtain reasonably\nlow-variance estimators of the gradient, and a good example of that success\nis with the recent advances with dropout(Hinton et al., [2012]); Krizhevsky et al., [2012b]); Goodfellow et al., [2013])). The\nidea is to multiply the output of a non-linear unit by independent binomial\nnoise. This noise injection is useful as a regularizer and it does slow\ndown training a bit, but not apparently by a lot (maybe 2-fold), which is very\nencouraging. The symmetry-breaking and induced sparsity may also compensate\nfor the extra variance and possibly help to reduce ill-conditioning, as\nhypothesized by Bengio ( [2013])).\n\nHowever, it is appealing to consider noise whose amplitude can be modulated\nby the signals computed in the computational graph, such as with\nstochastic binary neurons, which output a 1 or a 0 according\nto a sigmoid probability. Short of computing an average over an\nexponential number of configurations, it would seem that computing the\nexact gradient (with respect to the average of the loss over all possible\nbinary samplings of all the stochastic neurons in the neural network)\nis impossible in such neural networks. The question is whether good\nestimators (which might have bias and variance) or similar alternatives\ncan be computed and yield effective training. We discuss and compare here\nfour reasonable solutions to this problem, present theoretical\nresults about them, and small-scale experiments to validate that training\ncan be effective, in the context where one wants to use such stochastic\nunits to gate computation. The motivation, described further below, is\nto exploit such sparse stochastic gating units for conditional\ncomputation, i.e., avoiding to visit every parameter for every example,\nthus allowing to train potentially much larger models for the same\ncomputational cost.\n\n### 1.1 More Motivations and Conditional Computation\n\nOne motivation for studying stochastic neurons is that stochastic behavior\nmay be a required ingredient in modeling biological neurons. The\napparent noise in neuronal spike trains could come from an actual noise\nsource or simply from the hard to reproduce changes in the set of input\nspikes entering a neuron\u2019s dendrites. Until this question is resolved by\nbiological observations, it is interesting to study how such noise \u2013 which\nhas motivated the Boltzmann machine (Hinton et al., [1984])) \u2013 may impact\ncomputation and learning in neural networks.\n\nStochastic neurons with binary outputs are also interesting because\nthey can easily give rise to sparse representations (that have many zeros),\na form of regularization that has been used in many representation\nlearning algorithms (Bengio et al., [2013])).\nSparsity of the representation corresponds to the prior that, for a given\ninput scene, most of the explanatory factors are irrelevant (and that\nwould be represented by many zeros in the representation).\n\nAs argued by Bengio ( [2013])), sparse representations may\nbe a useful ingredient of conditional computation, by which\nonly a small subset of the model parameters are \u201cactivated\u201d (and need\nto be visited) for any particular example, thereby greatly reducing the\nnumber of computations needed per example. Sparse gating units may\nbe trained to select which part of the model actually need to be computed\nfor a given example.\n\nBinary representations are also useful as keys for a hash table, as in the\nsemantic hashing algorithm (Salakhutdinov and Hinton, [2009])).\nTrainable stochastic neurons would also be useful inside recurrent networks\nto take hard stochastic decisions about temporal events at different\ntime scales. This would be useful to train multi-scale temporal hierarchies\n111Daan Wierstra, personal communication such that back-propagated\ngradients could quickly flow through the slower time scales. Multi-scale\ntemporal hierarchies for recurrent nets have already been proposed and\ninvolved exponential moving averages of different time\nconstants (El Hihi and Bengio, [1996])), where each unit is still updated\nafter each time step. Instead, identifying key events at a high level of\nabstraction would allow these high-level units to only be updated when\nneeded (asynchronously), creating a fast track (short path) for gradient\npropagation through time.\n\n### 1.2 Prior Work\n\nThe idea of having stochastic neuron models is of course very old, with one\nof the major family of algorithms relying on such neurons being the\nBoltzmann machine (Hinton et al., [1984])). Another biologically motivated\nproposal for synaptic strength learning was proposed by\nFiete and Seung ( [2006])). It is based on small zero-mean i.i.d.\nperturbations applied at each stochastic neuron potential (prior to a\nnon-linearity) and a Taylor expansion of the expected reward as a function\nof these variations. Fiete and Seung ( [2006])) end up proposing a gradient\nestimator that looks like a correlation between the reward and the\nperturbation, very similar to that presented in\nSection [3]Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\"), and which is a special case of the\nREINFORCE (Williams, [1992])) algorithm. However, unlike REINFORCE, their\nestimator is only unbiased in the limit of small perturbations.\n\nGradient estimators based on stochastic perturbations have long been shown\nto be much more efficient than standard finite-difference\napproximations (Spall, [1992])). Consider N\ud835\udc41N quantities uisubscript\ud835\udc62\ud835\udc56u\\_{i} to\nbe adjusted in order to minimize an expected loss L\u200b(u)\ud835\udc3f\ud835\udc62L(u). A finite difference\napproximation is based on measuring separately the effect of changing each\none of the parameters, e.g., through L\u200b(u)\u2212L\u200b(u\u2212\u03f5\u200bei)\u03f5\ud835\udc3f\ud835\udc62\ud835\udc3f\ud835\udc62italic-\u03f5subscript\ud835\udc52\ud835\udc56italic-\u03f5\\\\frac{L(u)-L(u-\\\\epsilon e\\_{i})}{\\\\epsilon},\nor even better, through L\u200b(u+\u03f5\u200bei)\u2212L\u200b(u\u2212\u03f5\u200bei)2\u200b\u03f5\ud835\udc3f\ud835\udc62italic-\u03f5subscript\ud835\udc52\ud835\udc56\ud835\udc3f\ud835\udc62italic-\u03f5subscript\ud835\udc52\ud835\udc562italic-\u03f5\\\\frac{L(u+\\\\epsilon e\\_{i})-L(u-\\\\epsilon e\\_{i})}{2\\\\epsilon},\nwhere ei=(0,0,\u22ef,1,0,0,\u22ef,0)subscript\ud835\udc52\ud835\udc5600\u22ef100\u22ef0e\\_{i}=(0,0,\\\\cdots,1,0,0,\\\\cdots,0) where the 1 is at position i\ud835\udc56i. With N\ud835\udc41N\nquantities (and typically O\u200b(N)\ud835\udc42\ud835\udc41O(N) computations to calculate L\u200b(u)\ud835\udc3f\ud835\udc62L(u)), the\ncomputational cost of the gradient estimator is O\u200b(N2)\ud835\udc42superscript\ud835\udc412O(N^{2}). Instead, a perturbation-based\nestimator such as found in\nSimultaneous Perturbation Stochastic Approximation (SPSA) (Spall, [1992]))\nchooses a random perturbation vector z\ud835\udc67z (e.g., isotropic Gaussian noise\nof variance \u03c32superscript\ud835\udf0e2\\\\sigma^{2}) and estimates the gradient of the expected loss with\nrespect to uisubscript\ud835\udc62\ud835\udc56u\\_{i} through L\u200b(u+z)\u2212L\u200b(u\u2212z)2\u200bzi\ud835\udc3f\ud835\udc62\ud835\udc67\ud835\udc3f\ud835\udc62\ud835\udc672subscript\ud835\udc67\ud835\udc56\\\\frac{L(u+z)-L(u-z)}{2z\\_{i}}.\nSo long as the perturbation does not put too much probability around 0,\nthis estimator is as efficient as the finite-difference estimator but\nrequires O\u200b(N)\ud835\udc42\ud835\udc41O(N) less computation. However, like the algorithm proposed\nby Fiete and Seung ( [2006])) this estimator becomes unbiased only\nas the perturbations go towards 0. When we want to consider all-or-none\nperturbations (like a neuron sending a spike or not), it is not clear\nif these assumptions are appropriate. An advantage of the approaches\nproposed here is that they do not require that the perturbations\nbe small to be valid.\n\n## 2 Non-Smooth Stochastic Neurons\nOne way to achieve gradient-based learning in networks of stochastic neurons\nis to build an architecture in which noise is injected so that gradients\ncan sometimes flow into the neuron and can then adjust it (and its predecessors\nin the computational graph) appropriately.\n\nIn general, we can consider the output hisubscript\u210e\ud835\udc56h\\_{i} of a stochastic neuron as the\napplication of a deterministic function that depends on some noise source zisubscript\ud835\udc67\ud835\udc56z\\_{i}\nand on some differentiable\ntransformation aisubscript\ud835\udc4e\ud835\udc56a\\_{i} of its inputs\n(typically, a vector containing the outputs of other neurons) and internal parameters\n(typically the bias and incoming weights of the neuron):\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | hi=f\u200b(ai,zi)subscript\u210e\ud835\udc56\ud835\udc53subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc67\ud835\udc56h\\_{i}=f(a\\_{i},z\\_{i}) |  | (1) |\n\nSo long as f\u200b(ai,zi)\ud835\udc53subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc67\ud835\udc56f(a\\_{i},z\\_{i}) in the above equation has a non-zero\ngradient with respect to aisubscript\ud835\udc4e\ud835\udc56a\\_{i}, gradient-based learning\n(with back-propagation to compute gradients) can proceed.\nIn this paper, we consider the usual affine transformation\nai=bi+\u2211jWi\u200bj\u200bxi\u200bjsubscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4f\ud835\udc56subscript\ud835\udc57subscript\ud835\udc4a\ud835\udc56\ud835\udc57subscript\ud835\udc65\ud835\udc56\ud835\udc57a\\_{i}=b\\_{i}+\\\\sum\\_{j}W\\_{ij}x\\_{ij}, where xisubscript\ud835\udc65\ud835\udc56x\\_{i} is the vector\nof inputs into neuron i\ud835\udc56i.\n\nFor example, if the noise zisubscript\ud835\udc67\ud835\udc56z\\_{i} is added or multiplied somewhere in the\ncomputation of hisubscript\u210e\ud835\udc56h\\_{i}, gradients can be computed as usual. Dropout\nnoise (Hinton et al., [2012])) and masking noise (in denoising\nauto-encoders (Vincent et al., [2008]))) is multiplied (just\nafter the neuron non-linearity), while in semantic\nhashing (Salakhutdinov and Hinton, [2009])) noise is added (just before the\nnon-linearity). For example, that noise can be binomial (dropout, masking\nnoise) or Gaussian (semantic hashing).\n\nHowever, if we want hisubscript\u210e\ud835\udc56h\\_{i} to be binary (like in stochastic binary neurons),\nthen f\ud835\udc53f will have derivatives that are 0 almost everywhere (and infinite\nat the threshold), so that gradients can never flow.\n\nThere is an intermediate option that we put forward here: choose f\ud835\udc53f so\nthat it has two main kinds of behavior, with zero derivatives in some\nregions, and with significantly non-zero derivatives in other regions.\nWe call these two states of the neuron respectively the insensitive state\nand the sensitive state.\n\n### 2.1 Noisy Rectifier\n\nA special case is when the insensitive state corresponds to hi=0subscript\u210e\ud835\udc560h\\_{i}=0 and we\nhave sparsely activated neurons. The prototypical example of that situation\nis the rectifier unit (Nair and Hinton, [2010]); Glorot et al., [2011])), whose\nnon-linearity is simply max\u2061(0,arg)0arg\\\\max(0,{\\\\rm arg}), i.e.,\n\n|     |     |     |\n| --- | --- | --- |\n|  | hi=max\u2061(0,zi+ai)subscript\u210e\ud835\udc560subscript\ud835\udc67\ud835\udc56subscript\ud835\udc4e\ud835\udc56h\\_{i}=\\\\max(0,z\\_{i}+a\\_{i}) |  |\n\nwhere zisubscript\ud835\udc67\ud835\udc56z\\_{i} is a zero-mean noise. Although in practice Gaussian noise is\nappropriate, interesting properties can be shown for zisubscript\ud835\udc67\ud835\udc56z\\_{i} sampled from\nthe logistic density p\u200b(z)=sigm\u200b(z)\u200b(1\u2212sigm\u200b(z))\ud835\udc5d\ud835\udc67sigm\ud835\udc671sigm\ud835\udc67p(z)={\\\\rm sigm}(z)(1-{\\\\rm sigm}(z)), where sigm\u200b(\u22c5)sigm\u22c5{\\\\rm sigm}(\\\\cdot)\nrepresents the logistic sigmoid function.\n\nIn that case, we can prove nice properties of the noisy rectifier.\n\n###### Proposition 1.\n\nThe noisy rectifier with logistic noise has the following properties: (a) P\u200b(hi>0)=sigm\u200b(ai)\ud835\udc43subscript\u210e\ud835\udc560sigmsubscript\ud835\udc4e\ud835\udc56P(h\\_{i}>0)={\\\\rm sigm}(a\\_{i}),\n(b) E\u200b\\[hi\\]=s+\u200b(ai)\ud835\udc38delimited-\\[\\]subscript\u210e\ud835\udc56subscriptssubscript\ud835\udc4e\ud835\udc56E\\[h\\_{i}\\]={\\\\rm s}\\_{+}(a\\_{i}), where s+\u200b(x)=log\u2061(1+exp\u2061(x))subscripts\ud835\udc651\ud835\udc65{\\\\rm s}\\_{+}(x)=\\\\log(1+\\\\exp(x)) is the softplus function\nand the random variable is zisubscript\ud835\udc67\ud835\udc56z\\_{i} (given a fixed aisubscript\ud835\udc4e\ud835\udc56a\\_{i}).\n\n###### Proof.\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | P\u200b(hi>0)\ud835\udc43subscript\u210e\ud835\udc560\\\\displaystyle P(h\\_{i}>0) | =\\\\displaystyle= | P\u200b(zi>\u2212ai)=1\u2212sigm\u200b(\u2212ai)\ud835\udc43subscript\ud835\udc67\ud835\udc56subscript\ud835\udc4e\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56\\\\displaystyle P(z\\_{i}>-a\\_{i})=1-{\\\\rm sigm}(-a\\_{i}) |  |\n|  |  | =\\\\displaystyle= | 1\u221211+eai=11+e\u2212ai=sigm\u200b(ai)111superscript\ud835\udc52subscript\ud835\udc4e\ud835\udc5611superscript\ud835\udc52subscript\ud835\udc4e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\\\\displaystyle 1-\\\\frac{1}{1+e^{a\\_{i}}}=\\\\frac{1}{1+e^{-a\\_{i}}}={\\\\rm sigm}(a\\_{i}) |  |\n\nwhich proves (a). For (b), we use the facts d\u200bsigm\u200b(x)d\u200bx=sigm\u200b(x)\u200b(1\u2212sigm\u200b(x))\ud835\udc51sigm\ud835\udc65\ud835\udc51\ud835\udc65sigm\ud835\udc651sigm\ud835\udc65\\\\frac{d\\\\,{\\\\rm sigm}(x)}{dx}={\\\\rm sigm}(x)(1-{\\\\rm sigm}(x)),\ns+\u200b(u)\u2192u\u2192subscripts\ud835\udc62\ud835\udc62{\\\\rm s}\\_{+}(u)\\\\rightarrow u as u\u2192\u221e\u2192\ud835\udc62u\\\\rightarrow\\\\infty,\nchange of variable x=ai+zi\ud835\udc65subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc67\ud835\udc56x=a\\_{i}+z\\_{i} and integrate by parts:\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | E\u200b\\[hi\\]\ud835\udc38delimited-\\[\\]subscript\u210e\ud835\udc56\\\\displaystyle E\\[h\\_{i}\\] | =\\\\displaystyle= | limt\u2192\u221e\u222bzi>\u2212ait(ai+zi)sigm(zi)(1\u2212sigm(zi)dzi\\\\displaystyle\\\\lim\\_{t\\\\rightarrow\\\\infty}\\\\int\\_{z\\_{i}>-a\\_{i}}^{t}(a\\_{i}+z\\_{i}){\\\\rm sigm}(z\\_{i})(1-{\\\\rm sigm}(z\\_{i})dz\\_{i} |  |\n|  |  | =\\\\displaystyle= | limt\u2192\u221e(\\[x\u200bsigm\u200b(x\u2212ai)\\]0t\u2212\u222b0tsigm\u200b(x\u2212a)\u200b\ud835\udc51x)subscript\u2192\ud835\udc61superscriptsubscriptdelimited-\\[\\]\ud835\udc65sigm\ud835\udc65subscript\ud835\udc4e\ud835\udc560\ud835\udc61superscriptsubscript0\ud835\udc61sigm\ud835\udc65\ud835\udc4edifferential-d\ud835\udc65\\\\displaystyle\\\\lim\\_{t\\\\rightarrow\\\\infty}\\\\left(\\\\left\\[x\\\\,{\\\\rm sigm}(x-a\\_{i})\\\\right\\]\\_{0}^{t}-\\\\int\\_{0}^{t}{\\\\rm sigm}(x-a)dx\\\\right) |  |\n|  |  | =\\\\displaystyle= | limt\u2192\u221e\\[x\u200bsigm\u200b(x\u2212ai)\u2212s+\u200b(x\u2212ai)\\]0tsubscript\u2192\ud835\udc61superscriptsubscriptdelimited-\\[\\]\ud835\udc65sigm\ud835\udc65subscript\ud835\udc4e\ud835\udc56subscripts\ud835\udc65subscript\ud835\udc4e\ud835\udc560\ud835\udc61\\\\displaystyle\\\\lim\\_{t\\\\rightarrow\\\\infty}\\\\left\\[x\\\\,{\\\\rm sigm}(x-a\\_{i})-{\\\\rm s}\\_{+}(x-a\\_{i})\\\\right\\]\\_{0}^{t} |  |\n|  |  | =\\\\displaystyle= | ai+s+\u200b(\u2212ai)=s+\u200b(ai)subscript\ud835\udc4e\ud835\udc56subscriptssubscript\ud835\udc4e\ud835\udc56subscriptssubscript\ud835\udc4e\ud835\udc56\\\\displaystyle a\\_{i}+{\\\\rm s}\\_{+}(-a\\_{i})={\\\\rm s}\\_{+}(a\\_{i}) |  |\n\n\u220e\n\nLet us now consider two cases:\n\n1. 1.\n\n\nIf f\u200b(ai,0)>0\ud835\udc53subscript\ud835\udc4e\ud835\udc5600f(a\\_{i},0)>0, the basic state is active,\nthe unit is generally sensitive and non-zero,\nbut sometimes it is shut off (e.g., when zisubscript\ud835\udc67\ud835\udc56z\\_{i} is sufficiently negative to push\nthe argument of the rectifier below 0). In that case gradients will flow\nin most cases (samples of zisubscript\ud835\udc67\ud835\udc56z\\_{i}). If the rest of the system sends the signal\nthat hisubscript\u210e\ud835\udc56h\\_{i} should have been smaller, then gradients will push it towards\nbeing more often in the insensitive state.\n\n2. 2.\n\n\nIf f\u200b(ai,0)=0\ud835\udc53subscript\ud835\udc4e\ud835\udc5600f(a\\_{i},0)=0, the basic state is inactive,\nthe unit is generally insensitive and zero,\nbut sometimes turned on (e.g., when zisubscript\ud835\udc67\ud835\udc56z\\_{i} is sufficiently positive to push the\nargument of the rectifier above 0). In that case gradients will not flow\nin most cases, but when they do, the signal will either push the weighted\nsum lower (if being active was not actually a good thing for that unit in\nthat context) and reduce the chances of being active again, or it will\npush the weight sum higher (if being active was actually a good thing for\nthat unit in that context) and increase the chances of being active\nagain.\n\n\nSo it appears that even though the gradient does not always flow\n(as it would with sigmoid or tanh units), it might flow sufficiently\noften to provide the appropriate training information. The important\nthing to notice is that even when the basic state (second case, above)\nis for the unit to be insensitive and zero, there will be\nan occasional gradient signal that can draw it out of there.\n\nOne concern with this approach is that one can see an asymmetry between\nthe number of times that a unit with an active state can get a chance\nto receive a signal telling it to become inactive, versus the number of\ntimes that a unit with an inactive state can get a signal telling\nit to become active.\n\nAnother potential and related concern is that some of these units will\n\u201cdie\u201d (become useless) if their basic state is inactive in the vast\nmajority of cases (for example, because their weights pushed them into that\nregime due to random fluctations). Because of the above asymmetry, dead\nunits would stay dead for very long before getting a chance of being born\nagain, while live units would sometimes get into the death zone by chance\nand then get stuck there. What we propose here is a simple mechanism to\nadjust the bias of each unit so that in average its \u201cfiring rate\u201d\n(fraction of the time spent in the active state) reaches some pre-defined\ntarget. For example, if the moving average of being non-zero falls below a\nthreshold, the bias is pushed up until that average comes back above the\nthreshold.\n\n### 2.2 STS Units: Stochastic Times Smooth\n\nWe propose here a novel form of stochastic unit that is related to\nthe noisy rectifier and to the stochastic binary unit, but that can\nbe trained by ordinary gradient descent with the gradient obtained\nby back-propagation in a computational graph. We call it the\nSTS unit (Stochastic Times Smooth).\nWith aisubscript\ud835\udc4e\ud835\udc56a\\_{i} the\nactivation before the stochastic non-linearity, the output of\nthe STS unit is\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | pi=subscript\ud835\udc5d\ud835\udc56absent\\\\displaystyle p\\_{i}= | sigm\u200b(ai)sigmsubscript\ud835\udc4e\ud835\udc56\\\\displaystyle\\\\;{\\\\rm sigm}(a\\_{i}) |  |\n|  | bi\u223csimilar-tosubscript\ud835\udc4f\ud835\udc56absent\\\\displaystyle b\\_{i}\\\\sim | Binomial\u200b(pi)Binomialsubscript\ud835\udc5d\ud835\udc56\\\\displaystyle\\\\;{\\\\rm Binomial}(\\\\sqrt{p\\_{i}}) |  |\n|  | hi=subscript\u210e\ud835\udc56absent\\\\displaystyle h\\_{i}= | bi\u200bpisubscript\ud835\udc4f\ud835\udc56subscript\ud835\udc5d\ud835\udc56\\\\displaystyle\\\\;b\\_{i}\\\\sqrt{p\\_{i}} |  | (2) |\n\n###### Proposition 2.\n\nThe STS unit benefits from the following properties: (a) E\u200b\\[hi\\]=pi\ud835\udc38delimited-\\[\\]subscript\u210e\ud835\udc56subscript\ud835\udc5d\ud835\udc56E\\[h\\_{i}\\]=p\\_{i},\n(b) P\u200b(hi>0)=sigm\u200b(ai)\ud835\udc43subscript\u210e\ud835\udc560sigmsubscript\ud835\udc4e\ud835\udc56P(h\\_{i}>0)=\\\\sqrt{{\\\\rm sigm}(a\\_{i})}, (c) for any differentiable function f\ud835\udc53f\nof hisubscript\u210e\ud835\udc56h\\_{i}, we have\n\n|     |     |     |\n| --- | --- | --- |\n|  | E\u200b\\[f\u200b(hi)\\]=f\u200b(pi)+o\u200b(pi)\ud835\udc38delimited-\\[\\]\ud835\udc53subscript\u210e\ud835\udc56\ud835\udc53subscript\ud835\udc5d\ud835\udc56\ud835\udc5csubscript\ud835\udc5d\ud835\udc56E\\[f(h\\_{i})\\]=f(p\\_{i})+o(\\\\sqrt{p\\_{i}}) |  |\n\nas pi\u21920\u2192subscript\ud835\udc5d\ud835\udc560p\\_{i}\\\\rightarrow 0, and\nwhere the expectations and probability are over the injected noise bisubscript\ud835\udc4f\ud835\udc56b\\_{i}, given aisubscript\ud835\udc4e\ud835\udc56a\\_{i}.\n\n###### Proof.\n\nStatements (a) and (b) are immediately derived from the definitions\nin Eq. [2.2]STS Units: Stochastic Times Smooth \u2023 2 Non-Smooth Stochastic Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\") by noting that E\u200b\\[bi\\]=P\u200b(bi>0)=P\u200b(hi>0)=pi\ud835\udc38delimited-\\[\\]subscript\ud835\udc4f\ud835\udc56\ud835\udc43subscript\ud835\udc4f\ud835\udc560\ud835\udc43subscript\u210e\ud835\udc560subscript\ud835\udc5d\ud835\udc56E\\[b\\_{i}\\]=P(b\\_{i}>0)=P(h\\_{i}>0)=\\\\sqrt{p\\_{i}}.\nStatement (c) is obtained by first writing out the expectation,\n\n|     |     |     |\n| --- | --- | --- |\n|  | E\u200b\\[f\u200b(hi)\\]=pi\u200bf\u200b(pi)+(1\u2212pi)\u200bf\u200b(0)\ud835\udc38delimited-\\[\\]\ud835\udc53subscript\u210e\ud835\udc56subscript\ud835\udc5d\ud835\udc56\ud835\udc53subscript\ud835\udc5d\ud835\udc561subscript\ud835\udc5d\ud835\udc56\ud835\udc530E\\[f(h\\_{i})\\]=\\\\sqrt{p\\_{i}}f(\\\\sqrt{p\\_{i}})+(1-\\\\sqrt{p\\_{i}})f(0) |  |\n\nand then performing a Taylor expansion of f\ud835\udc53f around pisubscript\ud835\udc5d\ud835\udc56p\\_{i} as pi\u21920\u2192subscript\ud835\udc5d\ud835\udc560p\\_{i}\\\\rightarrow 0):\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | f\u200b(pi)\ud835\udc53subscript\ud835\udc5d\ud835\udc56\\\\displaystyle f(\\\\sqrt{p\\_{i}}) | =\\\\displaystyle= | f\u200b(pi)+f\u2032\u200b(pi)\u200b(pi\u2212pi)+o\u200b(pi\u2212pi)\ud835\udc53subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\ud835\udc5csubscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\\\\displaystyle f(p\\_{i})+f^{\\\\prime}(p\\_{i})(\\\\sqrt{p\\_{i}}-p\\_{i})+o(\\\\sqrt{p\\_{i}}-p\\_{i}) |  |\n|  | f\u200b(0)\ud835\udc530\\\\displaystyle f(0) | =\\\\displaystyle= | f\u200b(pi)+f\u2032\u200b(pi)\u200b(\u2212pi)+o\u200b(pi)\ud835\udc53subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\ud835\udc5csubscript\ud835\udc5d\ud835\udc56\\\\displaystyle f(p\\_{i})+f^{\\\\prime}(p\\_{i})(-p\\_{i})+o(p\\_{i}) |  |\n\nwhere o\u200b(x)x\u21920\u2192\ud835\udc5c\ud835\udc65\ud835\udc650\\\\frac{o(x)}{x}\\\\rightarrow 0 as x\u21920\u2192\ud835\udc650x\\\\rightarrow 0, so we obtain\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | E\u200b\\[f\u200b(hi)\\]\ud835\udc38delimited-\\[\\]\ud835\udc53subscript\u210e\ud835\udc56\\\\displaystyle E\\[f(h\\_{i})\\] | =\\\\displaystyle= | pi\u200b(f\u200b(pi)+f\u2032\u200b(pi)\u200b(pi\u2212pi))+(1\u2212pi)\u200b(f\u200b(pi)+f\u2032\u200b(pi)\u200b(\u2212pi))+o\u200b(pi)subscript\ud835\udc5d\ud835\udc56\ud835\udc53subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc561subscript\ud835\udc5d\ud835\udc56\ud835\udc53subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\ud835\udc5csubscript\ud835\udc5d\ud835\udc56\\\\displaystyle\\\\sqrt{p\\_{i}}(f(p\\_{i})+f^{\\\\prime}(p\\_{i})(\\\\sqrt{p\\_{i}}-p\\_{i}))+(1-\\\\sqrt{p\\_{i}})(f(p\\_{i})+f^{\\\\prime}(p\\_{i})(-p\\_{i}))+o(\\\\sqrt{p\\_{i}}) |  |\n|  |  | =\\\\displaystyle= | pi\u200bf\u200b(pi)+f\u2032\u200b(pi)\u200b(pi\u2212pi\u200bpi)+f\u200b(pi)\u200b(1\u2212pi)\u2212f\u2032\u200b(pi)\u200b(pi\u2212pi\u200bpi)+o\u200b(pi)subscript\ud835\udc5d\ud835\udc56\ud835\udc53subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\ud835\udc53subscript\ud835\udc5d\ud835\udc561subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\ud835\udc5csubscript\ud835\udc5d\ud835\udc56\\\\displaystyle\\\\sqrt{p\\_{i}}f(p\\_{i})+f^{\\\\prime}(p\\_{i})(p\\_{i}-p\\_{i}\\\\sqrt{p\\_{i}})+f(p\\_{i})(1-\\\\sqrt{p\\_{i}})-f^{\\\\prime}(p\\_{i})(p\\_{i}-p\\_{i}\\\\sqrt{p\\_{i}})+o(\\\\sqrt{p\\_{i}}) |  |\n|  |  | =\\\\displaystyle= | f\u200b(pi)+o\u200b(pi)\ud835\udc53subscript\ud835\udc5d\ud835\udc56\ud835\udc5csubscript\ud835\udc5d\ud835\udc56\\\\displaystyle f(p\\_{i})+o(\\\\sqrt{p\\_{i}}) |  |\n\nusing the fact that pi>pisubscript\ud835\udc5d\ud835\udc56subscript\ud835\udc5d\ud835\udc56\\\\sqrt{p\\_{i}}>p\\_{i}, o\u200b(pi)\ud835\udc5csubscript\ud835\udc5d\ud835\udc56o(p\\_{i}) can be replaced by o\u200b(pi)\ud835\udc5csubscript\ud835\udc5d\ud835\udc56o(\\\\sqrt{p\\_{i}}).\n\u220e\n\nNote that this derivation can be generalized to an expansion around f\u200b(x)\ud835\udc53\ud835\udc65f(x) for\nany x\u2264pi\ud835\udc65subscript\ud835\udc5d\ud835\udc56x\\\\leq\\\\sqrt{p\\_{i}}, yielding\n\n|     |     |     |\n| --- | --- | --- |\n|  | E\u200b\\[f\u200b(hi)\\]=f\u200b(x)+(x\u2212pi)\u200bf\u2032\u200b(x)+o\u200b(pi)\ud835\udc38delimited-\\[\\]\ud835\udc53subscript\u210e\ud835\udc56\ud835\udc53\ud835\udc65\ud835\udc65subscript\ud835\udc5d\ud835\udc56superscript\ud835\udc53\u2032\ud835\udc65\ud835\udc5csubscript\ud835\udc5d\ud835\udc56E\\[f(h\\_{i})\\]=f(x)+(x-p\\_{i})f^{\\\\prime}(x)+o(\\\\sqrt{p\\_{i}}) |  |\n\nwhere the effect of the first derivative is canceled out when we choose x=pi\ud835\udc65subscript\ud835\udc5d\ud835\udc56x=p\\_{i}.\nIt is also a reasonable choice to expand around pisubscript\ud835\udc5d\ud835\udc56p\\_{i} because E\u200b\\[hi\\]=pi\ud835\udc38delimited-\\[\\]subscript\u210e\ud835\udc56subscript\ud835\udc5d\ud835\udc56E\\[h\\_{i}\\]=p\\_{i}.\n\n## 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons\nThe above proposals cannot deal with non-linearities like\nthe indicator function that have a derivative of 0 almost everywhere.\nLet us consider this case now, where we want some component of our model\nto take a hard binary decision but allow this decision to be stochastic,\nwith a probability that is a continuous function of some\nquantities, through parameters that we wish to learn.\nWe will also assume that many such decisions can be taken\nin parallel with independent noise sources zisubscript\ud835\udc67\ud835\udc56z\\_{i} driving the stochastic\nsamples. Without loss of generality, we consider here a\nset of binary decisions, i.e., the setup corresponds to\nhaving a set of stochastic binary neurons, whose output hisubscript\u210e\ud835\udc56h\\_{i}\ninfluences an observed future loss L\ud835\udc3fL. In the framework of\nEq. [1]In 2 Non-Smooth Stochastic Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\"), we could have for example\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | hi=f\u200b(ai,zi)=\ud835\udfcfzi>sigm\u200b(ai)subscript\u210e\ud835\udc56\ud835\udc53subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc67\ud835\udc56subscript1subscript\ud835\udc67\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56h\\_{i}=f(a\\_{i},z\\_{i})=\\\\mathbf{1}\\_{z\\_{i}>{\\\\rm sigm}(a\\_{i})} |  | (3) |\n\nwhere zi\u223cU\u200b\\[0,1\\]similar-tosubscript\ud835\udc67\ud835\udc56\ud835\udc4801z\\_{i}\\\\sim U\\[0,1\\] is uniform and sigm\u200b(u)=1/(1+exp\u2061(\u2212u))sigm\ud835\udc6211\ud835\udc62{\\\\rm sigm}(u)=1/(1+\\\\exp(-u))\nis the sigmoid function.\nWe would ideally like to estimate\nhow a change in aisubscript\ud835\udc4e\ud835\udc56a\\_{i} would impact L\ud835\udc3fL in average over the\nnoise sources, so as to be able to propagate this estimated\ngradients into parameters and inputs of the stochastic neuron.\n\n###### Theorem 1.\n\nLet hisubscript\u210e\ud835\udc56h\\_{i} be defined as in Eq. [3]In 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\"),\nwith L=L\u200b(hi,ci,c\u2212i)\ud835\udc3f\ud835\udc3fsubscript\u210e\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56L=L(h\\_{i},c\\_{i},c\\_{-i}) a loss that depends stochastically on hisubscript\u210e\ud835\udc56h\\_{i}, cisubscript\ud835\udc50\ud835\udc56c\\_{i} the\nnoise sources that influence aisubscript\ud835\udc4e\ud835\udc56a\\_{i}, and c\u2212isubscript\ud835\udc50\ud835\udc56c\\_{-i} those that do not\ninfluence aisubscript\ud835\udc4e\ud835\udc56a\\_{i},\nthen g^i=(hi\u2212sigm\u200b(ai))\u00d7Lsubscript^\ud835\udc54\ud835\udc56subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f\\\\hat{g}\\_{i}=(h\\_{i}-{\\\\rm sigm}(a\\_{i}))\\\\times L\nis an unbiased estimator of gi=\u2202Ezi,c\u2212i\u200b\\[L\\|ci\\]\u2202aisubscript\ud835\udc54\ud835\udc56subscript\ud835\udc38subscript\ud835\udc67\ud835\udc56subscript\ud835\udc50\ud835\udc56delimited-\\[\\]conditional\ud835\udc3fsubscript\ud835\udc50\ud835\udc56subscript\ud835\udc4e\ud835\udc56g\\_{i}=\\\\frac{\\\\partial E\\_{z\\_{i},c\\_{-i}}\\[L\\|c\\_{i}\\]}{\\\\partial a\\_{i}} where the expectation is over zisubscript\ud835\udc67\ud835\udc56z\\_{i}\nand c\u2212isubscript\ud835\udc50\ud835\udc56c\\_{-i}, conditioned on the set of\nnoise sources cisubscript\ud835\udc50\ud835\udc56c\\_{i} that influence aisubscript\ud835\udc4e\ud835\udc56a\\_{i}.\n\n###### Proof.\n\nWe will compute the expected value of the estimator and verify\nthat it equals the desired derivative. The set of all noise sources\nin the system is {zi}\u222aci\u222ac\u2212isubscript\ud835\udc67\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\{z\\_{i}\\\\}\\\\cup c\\_{i}\\\\cup c\\_{-i}. We can consider\nL\ud835\udc3fL to be an implicit deterministic function of all the noise sources\n(zi,ci,c\u2212i)subscript\ud835\udc67\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56(z\\_{i},c\\_{i},c\\_{-i}), Evz\u200b\\[\u22c5\\]subscript\ud835\udc38subscript\ud835\udc63\ud835\udc67delimited-\\[\\]\u22c5E\\_{v\\_{z}}\\[\\\\cdot\\] denotes the expectation over variable vzsubscript\ud835\udc63\ud835\udc67v\\_{z},\nwhile E\\[\u22c5\\|vz\\]E\\[\\\\cdot\\|v\\_{z}\\] denotes\nthe expectation over all the other random variables besides vzsubscript\ud835\udc63\ud835\udc67v\\_{z}, i.e., conditioned on vZsubscript\ud835\udc63\ud835\udc4dv\\_{Z}.\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | E\u200b\\[L\\|ci\\]\ud835\udc38delimited-\\[\\]conditional\ud835\udc3fsubscript\ud835\udc50\ud835\udc56\\\\displaystyle E\\[L\\|c\\_{i}\\] | =Ec\u2212i\u200b\\[Ezi\u200b\\[L\u200b(hi,ci,c\u2212i)\\]\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56delimited-\\[\\]subscript\ud835\udc38subscript\ud835\udc67\ud835\udc56delimited-\\[\\]\ud835\udc3fsubscript\u210e\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{-i}}\\[E\\_{z\\_{i}}\\[L(h\\_{i},c\\_{i},c\\_{-i})\\]\\] |  |\n|  |  | =Ec\u2212i\u200b\\[Ezi\u200b\\[hi\u200bL\u200b(1,ci,c\u2212i)+(1\u2212hi)\u200bL\u200b(0,ci,c\u2212i)\\]\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56delimited-\\[\\]subscript\ud835\udc38subscript\ud835\udc67\ud835\udc56delimited-\\[\\]subscript\u210e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc561subscript\u210e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{-i}}\\[E\\_{z\\_{i}}\\[h\\_{i}L(1,c\\_{i},c\\_{-i})+(1-h\\_{i})L(0,c\\_{i},c\\_{-i})\\]\\] |  |\n|  |  | =Ec\u2212i\u200b\\[P\u200b(hi=1\\|ai)\u200bL\u200b(1,ci,c\u2212i)+P\u200b(hi=0\\|ai)\u200bL\u200b(0,ci,c\u2212i)\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56delimited-\\[\\]\ud835\udc43subscript\u210e\ud835\udc56conditional1subscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\ud835\udc43subscript\u210e\ud835\udc56conditional0subscript\ud835\udc4e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{-i}}\\[P(h\\_{i}=1\\|a\\_{i})L(1,c\\_{i},c\\_{-i})+P(h\\_{i}=0\\|a\\_{i})L(0,c\\_{i},c\\_{-i})\\] |  |\n|  |  | =Ec\u2212i\u200b\\[sigm\u200b(ai)\u200bL\u200b(1,ci,c\u2212i)+(1\u2212sigm\u200b(ai))\u200bL\u200b(0,ci,c\u2212i)\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56delimited-\\[\\]sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{-i}}\\[{\\\\rm sigm}(a\\_{i})L(1,c\\_{i},c\\_{-i})+(1-{\\\\rm sigm}(a\\_{i}))L(0,c\\_{i},c\\_{-i})\\] |  | (4) |\n\nSince aisubscript\ud835\udc4e\ud835\udc56a\\_{i} does not influence P\u200b(c\u2212i)\ud835\udc43subscript\ud835\udc50\ud835\udc56P(c\\_{-i}), differentiating with respect to aisubscript\ud835\udc4e\ud835\udc56a\\_{i} gives\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | gi=def\u2202E\u200b\\[L\\|ci\\]\u2202aisuperscriptdefsubscript\ud835\udc54\ud835\udc56\ud835\udc38delimited-\\[\\]conditional\ud835\udc3fsubscript\ud835\udc50\ud835\udc56subscript\ud835\udc4e\ud835\udc56\\\\displaystyle g\\_{i}\\\\stackrel{{\\\\scriptstyle\\\\text{def}}}{{=}}\\\\frac{\\\\partial E\\[L\\|c\\_{i}\\]}{\\\\partial a\\_{i}} | =Ec\u2212i\u200b\\[\u2202sigm\u200b(ai)\u2202ai\u200bL\u200b(1,ci,c\u2212i)\u2212\u2202sigm\u200b(ai)\u2202ai\u200bL\u200b(0,ci,c\u2212i)\\|ci\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56delimited-\\[\\]sigmsubscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56conditionalsigmsubscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{-i}}\\[\\\\frac{\\\\partial{\\\\rm sigm}(a\\_{i})}{\\\\partial a\\_{i}}L(1,c\\_{i},c\\_{-i})-\\\\frac{\\\\partial{\\\\rm sigm}(a\\_{i})}{\\\\partial a\\_{i}}L(0,c\\_{i},c\\_{-i})\\|c\\_{i}\\] |  |\n|  |  | =Ec\u2212i\\[sigm(ai)(1\u2212sigm(ai))(L(1,ci,c\u2212i)\u2212L(0,ci,c\u2212i)\\|ci\\]\\\\displaystyle=E\\_{c\\_{-i}}\\[{\\\\rm sigm}(a\\_{i})(1-{\\\\rm sigm}(a\\_{i}))(L(1,c\\_{i},c\\_{-i})-L(0,c\\_{i},c\\_{-i})\\|c\\_{i}\\] |  | (5) |\n\nFirst consider that since hi\u2208{0,1}subscript\u210e\ud835\udc5601h\\_{i}\\\\in\\\\{0,1\\\\},\n\n|     |     |     |\n| --- | --- | --- |\n|  | L\u200b(hi,ci,c\u2212i)=hi\u200bL\u200b(1,ci,c\u2212i)+(1\u2212hi)\u200bL\u200b(0,ci,c\u2212i)\ud835\udc3fsubscript\u210e\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\u210e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc561subscript\u210e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56L(h\\_{i},c\\_{i},c\\_{-i})=h\\_{i}L(1,c\\_{i},c\\_{-i})+(1-h\\_{i})L(0,c\\_{i},c\\_{-i}) |  |\n\nhi2=hisuperscriptsubscript\u210e\ud835\udc562subscript\u210e\ud835\udc56h\\_{i}^{2}=h\\_{i} and hi\u200b(1\u2212hi)=0subscript\u210e\ud835\udc561subscript\u210e\ud835\udc560h\\_{i}(1-h\\_{i})=0, so\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | g^i=def(hi\u2212sigm\u200b(ai))\u200bL\u200b(hi,ci,c\u2212i)superscriptdefsubscript^\ud835\udc54\ud835\udc56subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u210e\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle\\\\hat{g}\\_{i}\\\\stackrel{{\\\\scriptstyle\\\\text{def}}}{{=}}(h\\_{i}-{\\\\rm sigm}(a\\_{i}))L(h\\_{i},c\\_{i},c\\_{-i}) | =hi\u200b(hi\u2212sigm\u200b(ai))\u200bL\u200b(1,ci,c\u2212i)absentsubscript\u210e\ud835\udc56subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=h\\_{i}(h\\_{i}-{\\\\rm sigm}(a\\_{i}))L(1,c\\_{i},c\\_{-i}) |  | (6) |\n|  |  | +(hi\u2212sigm(ai))(1\u2212hi)L(0,ci,c\u2212i))\\\\displaystyle\\\\hskip 28.45274pt+(h\\_{i}-{\\\\rm sigm}(a\\_{i}))(1-h\\_{i})L(0,c\\_{i},c\\_{-i})) |  |\n|  |  | =hi\u200b(1\u2212sigm\u200b(ai))\u200bL\u200b(1,ci,c\u2212i)absentsubscript\u210e\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=h\\_{i}(1-{\\\\rm sigm}(a\\_{i}))L(1,c\\_{i},c\\_{-i}) |  | (7) |\n|  |  | \u2212(1\u2212hi)\u200bsigm\u200b(ai)\u200bL\u200b(0,ci,c\u2212i).1subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle\\\\hskip 28.45274pt-(1-h\\_{i}){\\\\rm sigm}(a\\_{i})L(0,c\\_{i},c\\_{-i}). |  | (8) |\n\nNow let us consider the expected value of the estimator g^i=(hi\u2212sigm\u200b(ai))\u200bL\u200b(hi,ci,c\u2212i)subscript^\ud835\udc54\ud835\udc56subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u210e\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\hat{g}\\_{i}=(h\\_{i}-{\\\\rm sigm}(a\\_{i}))L(h\\_{i},c\\_{i},c\\_{-i}).\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | l\u200bl\u200bE\u200b\\[g^i\\]\ud835\udc59\ud835\udc59\ud835\udc38delimited-\\[\\]subscript^\ud835\udc54\ud835\udc56\\\\displaystyle{ll}E\\[\\\\hat{g}\\_{i}\\] | =E\u200b\\[hi\u200b(1\u2212sigm\u200b(ai))\u200bL\u200b(1,ci,c\u2212i)\u2212(1\u2212hi)\u200bsigm\u200b(ai)\u200bL\u200b(0,ci,c\u2212i)\\]absent\ud835\udc38delimited-\\[\\]subscript\u210e\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc561subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\[h\\_{i}(1-{\\\\rm sigm}(a\\_{i}))L(1,c\\_{i},c\\_{-i})-(1-h\\_{i}){\\\\rm sigm}(a\\_{i})L(0,c\\_{i},c\\_{-i})\\] |  |\n|  |  | =Eci,c\u2212i\u200b\\[sigm\u200b(ai)\u200b(1\u2212sigm\u200b(ai))\u200bL\u200b(1,ci,c\u2212i)\u2212(1\u2212sigm\u200b(ai))\u200bsigm\u200b(ai)\u200bL\u200b(0,ci,c\u2212i)\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56delimited-\\[\\]sigmsubscript\ud835\udc4e\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{i},c\\_{-i}}\\[{\\\\rm sigm}(a\\_{i})(1-{\\\\rm sigm}(a\\_{i}))L(1,c\\_{i},c\\_{-i})-(1-{\\\\rm sigm}(a\\_{i})){\\\\rm sigm}(a\\_{i})L(0,c\\_{i},c\\_{-i})\\] |  |\n|  |  | =Eci,c\u2212i\u200b\\[sigm\u200b(ai)\u200b(1\u2212sigm\u200b(ai))\u200b(L\u200b(1,ci,c\u2212i)\u2212L\u200b(0,ci,c\u2212i))\\]absentsubscript\ud835\udc38subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56delimited-\\[\\]sigmsubscript\ud835\udc4e\ud835\udc561sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f1subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\ud835\udc3f0subscript\ud835\udc50\ud835\udc56subscript\ud835\udc50\ud835\udc56\\\\displaystyle=E\\_{c\\_{i},c\\_{-i}}\\[{\\\\rm sigm}(a\\_{i})(1-{\\\\rm sigm}(a\\_{i}))(L(1,c\\_{i},c\\_{-i})-L(0,c\\_{i},c\\_{-i}))\\] |  | (9) |\n\nwhich is the same as Eq. [5]In Proof. \u2023 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\"), i.e., the expected value of the\nestimator equals the gradient of the expected loss, E\u200b\\[g^i\\]=gi\ud835\udc38delimited-\\[\\]subscript^\ud835\udc54\ud835\udc56subscript\ud835\udc54\ud835\udc56E\\[\\\\hat{g}\\_{i}\\]=g\\_{i}.\n\u220e\n\nThis estimator is a special case of the REINFORCE\nalgorithm when the stochastic unit is a Bernoulli with probability\ngiven by a sigmoid (Williams, [1992])). In the REINFORCE paper, Williams\nshows that if the stochastic action h\u210eh is sampled with probability\np\u03b8\u200b(h)subscript\ud835\udc5d\ud835\udf03\u210ep\\_{\\\\theta}(h) and yields a reward R\ud835\udc45R, then\n\n|     |     |     |\n| --- | --- | --- |\n|  | Eh\u200b\\[(R\u2212b)\u22c5\u2202log\u2061p\u03b8\u200b(h)\u2202\u03b8\\]=\u2202Eh\u200b\\[R\\]\u2202\u03b8subscript\ud835\udc38\u210edelimited-\\[\\]\u22c5\ud835\udc45\ud835\udc4fsubscript\ud835\udc5d\ud835\udf03\u210e\ud835\udf03subscript\ud835\udc38\u210edelimited-\\[\\]\ud835\udc45\ud835\udf03E\\_{h}\\[(R-b)\\\\cdot\\\\frac{\\\\partial\\\\log p\\_{\\\\theta}(h)}{\\\\partial\\\\theta}\\]=\\\\frac{\\\\partial E\\_{h}\\[R\\]}{\\\\partial\\\\theta} |  |\n\nwhere b\ud835\udc4fb is an arbitrary constant,\ni.e., the sampled value h\u210eh can be seen as a weighted maximum\nlikelihood target for the output distribution p\u03b8\u200b(\u22c5)subscript\ud835\udc5d\ud835\udf03\u22c5p\\_{\\\\theta}(\\\\cdot),\nwith the weights R\u2212b\ud835\udc45\ud835\udc4fR-b proportional to the reward. The additive\nnormalization constant b\ud835\udc4fb does not change the expected gradient,\nbut influences its variance, and an optimal choice can be computed,\nas shown below.\n\n###### Corollary 1.\n\nUnder the same conditions as Theorem [1]Theorem 1. \u2023 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\"),\nand for any (possibly unit-specific) constant L\u00afisubscript\u00af\ud835\udc3f\ud835\udc56\\\\bar{L}\\_{i}\nthe centered estimator (hi\u2212sigm\u200b(ai))\u200b(L\u2212L\u00afi)subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56(h\\_{i}-{\\\\rm sigm}(a\\_{i}))(L-\\\\bar{L}\\_{i}),\nis also an unbiased estimator of\ngi=\u2202Ezi,c\u2212i\u200b\\[L\\|ci\\]\u2202aisubscript\ud835\udc54\ud835\udc56subscript\ud835\udc38subscript\ud835\udc67\ud835\udc56subscript\ud835\udc50\ud835\udc56delimited-\\[\\]conditional\ud835\udc3fsubscript\ud835\udc50\ud835\udc56subscript\ud835\udc4e\ud835\udc56g\\_{i}=\\\\frac{\\\\partial E\\_{z\\_{i},c\\_{-i}}\\[L\\|c\\_{i}\\]}{\\\\partial a\\_{i}}.\nFurthermore, among all possible values of L\u00afisubscript\u00af\ud835\udc3f\ud835\udc56\\\\bar{L}\\_{i}, the minimum\nvariance choice is\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | L\u00afi=E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\\]E\u200b\\[(hi\u2212sigm\u200b(ai))2\\],subscript\u00af\ud835\udc3f\ud835\udc56\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\ud835\udc3f\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\\\\bar{L}\\_{i}=\\\\frac{E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L\\]}{E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}\\]}, |  | (10) |\n\nwhich we note is a weighted average of the loss values L\ud835\udc3fL, whose\nweights are specific to unit i\ud835\udc56i.\n\n###### Proof.\n\nThe centered estimator (hi\u2212sigm\u200b(ai))\u200b(L\u2212L\u00afi)subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56(h\\_{i}-{\\\\rm sigm}(a\\_{i}))(L-\\\\bar{L}\\_{i}) can be decomposed\ninto the sum of the uncentered estimator g^isubscript^\ud835\udc54\ud835\udc56\\\\hat{g}\\_{i} and the term\n(hi\u2212sigm\u200b(ai))\u200bL\u00afisubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56subscript\u00af\ud835\udc3f\ud835\udc56(h\\_{i}-{\\\\rm sigm}(a\\_{i}))\\\\bar{L}\\_{i}. Since Ezi\u200b\\[hi\\|ai\\]=sigm\u200b(ai)subscript\ud835\udc38subscript\ud835\udc67\ud835\udc56delimited-\\[\\]conditionalsubscript\u210e\ud835\udc56subscript\ud835\udc4e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56E\\_{z\\_{i}}\\[h\\_{i}\\|a\\_{i}\\]={\\\\rm sigm}(a\\_{i}),\nE\u200b\\[L\u00afi\u200b(hi\u2212sigm\u200b(ai))\\|ai\\]=0\ud835\udc38delimited-\\[\\]conditionalsubscript\u00af\ud835\udc3f\ud835\udc56subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc560E\\[\\\\bar{L}\\_{i}(h\\_{i}-{\\\\rm sigm}(a\\_{i}))\\|a\\_{i}\\]=0, so that\nthe expected value of the centered estimator equals the\nexpected value of the uncentered estimator. By\nTheorem [1]Theorem 1. \u2023 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\") (the uncentered estimator is\nunbiased), the centered estimator is therefore also unbiased,\nwhich completes the proof of the first statement.\n\nRegarding the optimal choice of L\u00afisubscript\u00af\ud835\udc3f\ud835\udc56\\\\bar{L}\\_{i},\nfirst note that the variance of the uncentered estimator is\n\n|     |     |     |\n| --- | --- | --- |\n|  | V\u200ba\u200br\u200b\\[(hi\u2212sigm\u200b(ai))\u200bL\\]=E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL2\\]\u2212E\u200b\\[g^i\\]2.\ud835\udc49\ud835\udc4e\ud835\udc5fdelimited-\\[\\]subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscript\ud835\udc3f2\ud835\udc38superscriptdelimited-\\[\\]subscript^\ud835\udc54\ud835\udc562Var\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))L\\]=E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L^{2}\\]-E\\[\\\\hat{g}\\_{i}\\]^{2}. |  |\n\nNow let us compute the variance of the centered estimator:\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | V\u200ba\u200br\u200b\\[(hi\u2212sigm\u200b(ai))\u200b(L\u2212L\u00afi)\\]\ud835\udc49\ud835\udc4e\ud835\udc5fdelimited-\\[\\]subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56\\\\displaystyle Var\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))(L-\\\\bar{L}\\_{i})\\] | =E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200b(L\u2212L\u00afi)2\\]\u2212E\u200b\\[(hi\u2212\u03c3\u200b(ai))\u200b(L\u2212L\u00afi)\\]2absent\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscript\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc562\ud835\udc38superscriptdelimited-\\[\\]subscript\u210e\ud835\udc56\ud835\udf0esubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc562\\\\displaystyle=E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}(L-\\\\bar{L}\\_{i})^{2}\\]-E\\[(h\\_{i}-\\\\sigma(a\\_{i}))(L-\\\\bar{L}\\_{i})\\]^{2} |  |\n|  |  | =E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL2\\]+E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u00afi2\\]absent\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscript\ud835\udc3f2\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscriptsubscript\u00af\ud835\udc3f\ud835\udc562\\\\displaystyle=E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L^{2}\\]+E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}\\\\bar{L}\\_{i}^{2}\\] |  |\n|  |  | \u22122\u200bE\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u200bL\u00afi\\]\u2212(E\u200b\\[g^i\\]\u22120)22\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56superscript\ud835\udc38delimited-\\[\\]subscript^\ud835\udc54\ud835\udc5602\\\\displaystyle-2E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L\\\\bar{L}\\_{i}\\]-(E\\[\\\\hat{g}\\_{i}\\]-0)^{2} |  |\n|  |  | =V\u200ba\u200br\u200b\\[(hi\u2212sigm\u200b(ai))\u200bL\\]\u2212\u0394absent\ud835\udc49\ud835\udc4e\ud835\udc5fdelimited-\\[\\]subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3f\u0394\\\\displaystyle=Var\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))L\\]-\\\\Delta |  | (11) |\n\nwhere \u0394=2\u200bE\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u200bL\u00afi\\]\u2212E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u00afi2\\]\u03942\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscriptsubscript\u00af\ud835\udc3f\ud835\udc562\\\\Delta=2E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L\\\\bar{L}\\_{i}\\]-E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}\\\\bar{L}\\_{i}^{2}\\].\nLet us rewrite \u0394\u0394\\\\Delta:\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n|  | \u0394\u0394\\\\displaystyle\\\\Delta | =2\u200bE\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u200bL\u00afi\\]\u2212E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u00afi2\\]absent2\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscriptsubscript\u00af\ud835\udc3f\ud835\udc562\\\\displaystyle=2E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L\\\\bar{L}\\_{i}\\]-E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}\\\\bar{L}\\_{i}^{2}\\] |  |\n|  |  | =E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\u00afi\u200b(2\u200bL\u2212L\u00afi)\\]absent\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562subscript\u00af\ud835\udc3f\ud835\udc562\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56\\\\displaystyle=E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}\\\\bar{L}\\_{i}(2L-\\\\bar{L}\\_{i})\\] |  |\n|  |  | =E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200b(L2\u2212(L\u2212L\u00afi)2)\\]absent\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscript\ud835\udc3f2superscript\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc562\\\\displaystyle=E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}(L^{2}-(L-\\\\bar{L}\\_{i})^{2})\\] |  | (12) |\n\n\u0394\u0394\\\\Delta is maximized (to minimize variance of the estimator)\nwhen E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200b(L\u2212L\u00afi)2\\]\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562superscript\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc562E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}(L-\\\\bar{L}\\_{i})^{2}\\] is minimized. Taking the\nderivative of that expression with respect to L\u00afisubscript\u00af\ud835\udc3f\ud835\udc56\\\\bar{L}\\_{i}, we obtain\n\n|     |     |     |\n| --- | --- | --- |\n|  | 2\u200bE\u200b\\[(hi\u2212sigm\u200b(ai))2\u200b(L\u00afi\u2212L)\\]=02\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562subscript\u00af\ud835\udc3f\ud835\udc56\ud835\udc3f0\\\\vspace\\*{-2mm}2E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}(\\\\bar{L}\\_{i}-L)\\]=0 |  |\n\nwhich, as claimed, is achieved for\n\n|     |     |     |\n| --- | --- | --- |\n|  | L\u00afi=E\u200b\\[(hi\u2212sigm\u200b(ai))2\u200bL\\]E\u200b\\[(hi\u2212sigm\u200b(ai))2\\].subscript\u00af\ud835\udc3f\ud835\udc56\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\ud835\udc3f\ud835\udc38delimited-\\[\\]superscriptsubscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc562\\\\vspace\\*{-3mm}\\\\bar{L}\\_{i}=\\\\frac{E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}L\\]}{E\\[(h\\_{i}-{\\\\rm sigm}(a\\_{i}))^{2}\\]}.\\\\vspace\\*{-2mm} |  |\n\n\u220e\n\nNote that a general formula for the lowest variance estimator\nfor REINFORCE (Williams, [1992])) had already been introduced in the reinforcement\nlearning context by Weaver and Tao ( [2001])),\nwhich includes the above result as a special case. This followed from\nprevious work (Dayan, [1990])) for the case of binary immediate reward.\n\nPractically, we could get the lowest variance estimator (among all choices of the L\u00afisubscript\u00af\ud835\udc3f\ud835\udc56\\\\bar{L}\\_{i})\nby keeping track of two numbers (running or moving averages)\nfor each stochastic neuron, one for the numerator\nand one for the denominator of the unit-specific L\u00afisubscript\u00af\ud835\udc3f\ud835\udc56\\\\bar{L}\\_{i} in Eq. [10]In Corollary 1. \u2023 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\").\nThis would lead the lowest-variance estimator (hi\u2212sigm\u200b(ai))\u200b(L\u2212L\u00afi)subscript\u210e\ud835\udc56sigmsubscript\ud835\udc4e\ud835\udc56\ud835\udc3fsubscript\u00af\ud835\udc3f\ud835\udc56(h\\_{i}-{\\\\rm sigm}(a\\_{i}))(L-\\\\bar{L}\\_{i}).\nNote how the unbiased estimator only requires broadcasting L\ud835\udc3fL throughout the\nnetwork, no back-propagation and only local computation.\nNote also how this could be applied even with an estimate\nof future rewards or losses L\ud835\udc3fL, as would be useful in the context of\nreinforcement learning (where the actual loss or reward will be measured farther\ninto the future, much after hisubscript\u210e\ud835\udc56h\\_{i} has been sampled).\n\n## 4 Straight-Through Estimator\nAnother estimator of the expected gradient through stochastic neurons\nwas proposed by Hinton ( [2012]))\nin his lecture 15b. The idea is simply to back-propagate\nthrough the hard threshold function (1 if the argument is positive, 0 otherwise)\nas if it had been the identity function. It is clearly a biased estimator,\nbut when considering a single layer of neurons, it has the right sign\n(this is not guaranteed anymore when back-propagating through more hidden layers).\nWe call it the straight-through (ST) estimator. A possible variant\ninvestigated here multiplies the gradient on hisubscript\u210e\ud835\udc56h\\_{i} by the derivative of the\nsigmoid. Better results were actually obtained without multiplying by\nthe derivative of the sigmoid. With hisubscript\u210e\ud835\udc56h\\_{i} sampled as per Eq. [3]In 3 Unbiased Estimator of Gradient for Stochastic Binary Neurons \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\"),\nthe straight-through estimator of the gradient of the loss L\ud835\udc3fL with respect to the\npre-sigmoid activation aisubscript\ud835\udc4e\ud835\udc56a\\_{i} is thus\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | gi=\u2202L\u2202hi.subscript\ud835\udc54\ud835\udc56\ud835\udc3fsubscript\u210e\ud835\udc56g\\_{i}=\\\\frac{\\\\partial L}{\\\\partial h\\_{i}}. |  | (13) |\n\nLike the other estimators, it is then back-propagated to obtain gradients on the parameters\nthat influence aisubscript\ud835\udc4e\ud835\udc56a\\_{i}.\n\n![Refer to caption]Figure 1: Conditional computation architecture: gater path on the left\nproduces sparse outputs hisubscript\u210e\ud835\udc56h\\_{i} which are multiplied elementwise by\nthe expert outputs, hidden units Hisubscript\ud835\udc3b\ud835\udc56H\\_{i}.\n\n## 5 Conditional Computation Experiments\nWe consider the potential application of stochastic neurons in the\ncontext of conditional computation, where the stochastic neurons are\nused to select which parts of some computational graph should be\nactually computed, given the current input. The particular architecture we\nexperimented with is a neural network with a large hidden layer whose\nunits Hisubscript\ud835\udc3b\ud835\udc56H\\_{i} will be selectively turned off by gating units hisubscript\u210e\ud835\udc56h\\_{i},\ni.e., the output of the i-th hidden unit is Hi\u200bhisubscript\ud835\udc3b\ud835\udc56subscript\u210e\ud835\udc56H\\_{i}h\\_{i}, as illustrated\nin Figure [1]Figure 1 \u2023 4 Straight-Through Estimator \u2023 Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\").\nFor this to make sense in the context of conditional computation, we\nwant hisubscript\u210e\ud835\udc56h\\_{i} to be non-zero only a small fraction \u03b1\ud835\udefc\\\\alpha of the time (10% in the\nexperiments) while the amount of computation required to compute hisubscript\u210e\ud835\udc56h\\_{i}\nshould be much less than the amount of computation required to compute Hisubscript\ud835\udc3b\ud835\udc56H\\_{i}.\nIn this way, we can first compute hisubscript\u210e\ud835\udc56h\\_{i} and only compute Hisubscript\ud835\udc3b\ud835\udc56H\\_{i} if hi\u22600subscript\u210e\ud835\udc560h\\_{i}\\\\neq 0,\nthereby saving much computation. To achieve this, we connect the previous\nlayer to hisubscript\u210e\ud835\udc56h\\_{i} through a bottleneck layer. Hence if the previous layer\n(or the input) has size N\ud835\udc41N and the main path layer also has size N\ud835\udc41N\n(i.e., i\u2208{1,\u2026\u200bN}\ud835\udc561\u2026\ud835\udc41i\\\\in\\\\{1,\\\\ldots N\\\\}), and the gater\u2019s bottleneck layer has size M\u226aNmuch-less-than\ud835\udc40\ud835\udc41M\\\\ll N,\nthen computing the gater output is O\u200b(M\u200bN)\ud835\udc42\ud835\udc40\ud835\udc41O(MN), which can reduce the main path\ncomputations from O\u200b(N2)\ud835\udc42superscript\ud835\udc412O(N^{2}) to O\u200b(\u03b1\u200bN2)\ud835\udc42\ud835\udefcsuperscript\ud835\udc412O(\\\\alpha N^{2}).\n\n![Refer to caption]\n\n|  | train | valid | test |\n| Noisy Rectifier | 6.7e-4 | 1.52 | 1.87 |\n| Straight-through | 3.3e-3 | 1.42 | 1.39 |\n| Smooth Times Stoch. | 4.4e-3 | 1.86 | 1.96 |\n| Stoch. Binary Neuron | 9.9e-3 | 1.78 | 1.89 |\n| Baseline Rectifier | 6.3e-5 | 1.66 | 1.60 |\n| Baseline Sigmoid+Noise | 1.8e-3 | 1.88 | 1.87 |\n| Baseline Sigmoid | 3.2e-3 | 1.97 | 1.92 |\n\nFigure 2: Left: Learning curves for the various stochastic approaches applied to the MNIST dataset. The y-axis indicates classification error on the validation set. Right: Comparison of training criteria and valid/test classification error (%) for stochastic (top) and baseline (bottom) gaters.\n\nExperiments are performed with a gater of 400 hidden units and 2000 output\nunits. The main path also has 2000 hidden units. A sparsity constraint is imposed\non the 2000 gater output units such that each is non-zero 10% of the time,\non average. The validation and test errors of the stochastic models\nare obtained after optimizing a threshold in their deterministic counterpart used during testing.\nSee the appendix in supplementary material for more details on\nthe experiments.\n\nFor this experiment, we compare with 3 baselines. The Baseline Rectifier is\njust like the noisy rectifier, but with 0 noise, and is also\nconstrained to have a sparsity of 10%. The Baseline Sigmoid is like the\nSTS and ST models in that the gater uses sigmoids for its output and tanh\nfor its hidden units, but has no sparsity constraint and only 200 output\nunits for the gater and main part. The baseline has the same hypothetical\nresource constraint in terms of computational efficiency (run-time), but\nhas less total parameters (memory). The Baseline Sigmoid with Noise is the\nsame, but with Gaussian noise added during training.\n\n## 6 Conclusion\nIn this paper, we have motivated estimators of the gradient through highly\nnon-linear non-differentiable functions (such as those corresponding to an\nindicator function), especially in networks involving noise sources, such\nas neural networks with stochastic neurons. They can be useful as\nbiologically motivated models and they might be useful for engineering\n(computational efficiency) reasons when trying to reduce computation via\nconditional computation or to reduce interactions between parameters via\nsparse updates (Bengio, [2013])). We have proven interesting properties\nof three classes of stochastic neurons, the noisy rectifier, the STS unit,\nand the binary stochastic neuron, in particular showing the existence\nof an unbiased estimator of the gradient for the latter.\nUnlike the SPSA (Spall, [1992])) estimator, our estimator is unbiased\neven though the perturbations are not small (0 or 1), and it multiplies\nby the perturbation rather than dividing by it.\n\nExperiments show that all the tested methods actually allow training to\nproceed. It is interesting to note that the gater with noisy rectifiers\nyielded better results than the one with the non-noisy baseline\nrectifiers. Similarly, the sigmoid baseline with noise performed better\nthan without, even on the training objective: all these results\nsuggest that injecting noise can be useful not just as a regularizer but\nalso to help explore good parameters and fit the training objective. These\nresults are particularly surprising for the stochastic binary neuron, which\ndoes not use any backprop for getting a signal into the gater, and opens\nthe door to applications where no backprop signal is available. In terms of\nconditional computation, we see that the expected saving is achieved,\nwithout an important loss in performance. Another surprise is the good\nperformance of the straight-through units, which provided the best\nvalidation and test error, and are very simple to implement.\n\n#### Acknowledgments\n\nThe authors would like to acknowledge the useful comments from Guillaume\nAlain and funding from NSERC, Ubisoft, CIFAR (YB is a CIFAR Fellow),\nand the Canada Research Chairs.\n</Content></Candidate>\n\n<Candidate>Candidate [2]:\nProgressive growing of gans for improved quality, stability, and variation.\n\n<Abstract>Abstract: We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset.</Abstract><Content>\n## 1 Introduction\nGenerative methods that produce novel samples from high-dimensional data distributions, such as images, are finding widespread use, for example in speech synthesis (van den Oord et al., [2016a])), image-to-image translation (Zhu et al., [2017]); Liu et al., [2017]); Wang et al., [2017])), and image inpainting (Iizuka et al., [2017])). Currently the most prominent approaches are autoregressive models (van den Oord et al., [2016b]); [c])), variational autoencoders (VAE) (Kingma & Welling, [2014])), and generative adversarial networks (GAN) (Goodfellow et al., [2014])). Currently they all have significant strengths and weaknesses. Autoregressive models \u2013 such as PixelCNN \u2013 produce sharp images but are slow to evaluate and do not have a latent representation as they directly model the conditional distribution over pixels, potentially limiting their applicability. VAEs are easy to train but tend to produce blurry results due to restrictions in the model, although recent work is improving this (Kingma et al., [2016])). GANs produce sharp images, albeit only in fairly small resolutions and with somewhat limited variation, and the training continues to be unstable despite recent progress (Salimans et al., [2016]); Gulrajani et al., [2017]); Berthelot et al., [2017]); Kodali et al., [2017])). Hybrid methods combine various strengths of the three, but so far lag behind GANs in image quality (Makhzani & Frey, [2017]); Ulyanov et al., [2017]); Dumoulin et al., [2016])).\n\nTypically, a GAN consists of two networks: generator and discriminator (aka critic). The generator produces a sample, e.g., an image, from a latent code, and the distribution of these images should ideally be indistinguishable from the training distribution. Since it is generally infeasible to engineer a function that tells whether that is the case, a discriminator network is trained to do the assessment, and since networks are differentiable, we also get a gradient we can use to steer both networks to the right direction. Typically, the generator is of main interest \u2013 the discriminator is an adaptive loss function that gets discarded once the generator has been trained.\n\nThere are multiple potential problems with this formulation. When we measure the distance between the training distribution and the generated distribution, the gradients can point to more or less random directions if the distributions do not have substantial overlap, i.e., are too easy to tell apart (Arjovsky & Bottou, [2017])). Originally, Jensen-Shannon divergence was used as a distance metric (Goodfellow et al., [2014])), and recently that formulation has been improved (Hjelm et al., [2017])) and a number of more stable alternatives have been proposed, including least squares (Mao et al., [2016b])), absolute deviation with margin (Zhao et al., [2017])), and Wasserstein distance (Arjovsky et al., [2017]); Gulrajani et al., [2017])). Our contributions are largely orthogonal to this ongoing discussion, and we primarily use the improved Wasserstein loss, but also experiment with least-squares loss.\n\nThe generation of high-resolution images is difficult because higher resolution makes it easier to tell the generated images apart from training images (Odena et al., [2017])), thus drastically amplifying the gradient problem. Large resolutions also necessitate using smaller minibatches due to memory constraints, further compromising training stability. Our key insight is that we can grow both the generator and discriminator progressively, starting from easier low-resolution images, and add new layers that introduce higher-resolution details as the training progresses. This greatly speeds up training and improves stability in high resolutions, as we will discuss in Section [2]Progressive growing of GANs \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\").\n\nThe GAN formulation does not explicitly require the entire training data distribution to be represented by the resulting generative model. The conventional wisdom has been that there is a tradeoff between image quality and variation, but that view has been recently challenged (Odena et al., [2017])). The degree of preserved variation is currently receiving attention and various methods have been suggested for measuring it, including inception score (Salimans et al., [2016])), multi-scale structural similarity (MS-SSIM) (Odena et al., [2017]); Wang et al., [2003])), birthday paradox (Arora & Zhang, [2017])), and explicit tests for the number of discrete modes discovered (Metz et al., [2016])). We will describe our method for encouraging variation in Section [3]Increasing variation using minibatch standard deviation \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\"), and propose a new metric for evaluating the quality and variation in Section [5]Multi-scale statistical similarity for assessing GAN results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\").\n\nSection [4.1]Equalized learning rate \u2023 4 Normalization in generator and discriminator \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") discusses a subtle modification to the initialization of networks, leading to a more balanced learning speed for different layers. Furthermore, we observe that mode collapses traditionally plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy competition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section [4.2]Pixelwise feature vector normalization in generator \u2023 4 Normalization in generator and discriminator \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")).\n\nWe evaluate our contributions using the CelebA, LSUN, CIFAR10 datasets. We improve the best published inception score for CIFAR10. Since the datasets commonly used in benchmarking generative methods are limited to a fairly low resolution, we have also created a higher quality version of the CelebA dataset that allows experimentation with output resolutions up to 1024\u00d71024102410241024\\\\times 1024 pixels. This dataset and our full implementation are available at `https://github.com/tkarras/progressive_growing_of_gans`, trained networks can be found at https://drive.google.com/open?id=0B4qLcYyJmiz0NHFULTdYc05lX0U along with result images, and a supplementary video illustrating the datasets, additional results, and latent space interpolations is at https://youtu.be/G06dEcZ-QTg.\n\n## 2 Progressive growing of GANs\nOur primary contribution is a training methodology for GANs where we start with low-resolution images, and then progressively increase the resolution by adding layers to the networks as visualized in Figure [1]Figure 1 \u2023 2 Progressive growing of GANs \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\"). This incremental nature allows the training to first discover large-scale structure of the image distribution and then shift attention to increasingly finer scale detail, instead of having to learn all scales simultaneously.\n\nWe use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly, as illustrated in Figure [2]Figure 2 \u2023 2 Progressive growing of GANs \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\"). This avoids sudden shocks to the already well-trained, smaller-resolution layers. Appendix [A]Appendix A Network structure and training configuration \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") describes structure of the generator and discriminator in detail, along with other training parameters.\n\nWe observe that the progressive training has several benefits. Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes (Odena et al., [2017])). By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors to e.g. 10242superscript102421024^{2} images. This approach has conceptual similarity to recent work by Chen & Koltun ( [2017])). In practice it stabilizes the training sufficiently for us to reliably synthesize megapixel-scale images using WGAN-GP loss (Gulrajani et al., [2017])) and even LSGAN loss (Mao et al., [2016b])).\n\nAnother benefit is the reduced training time. With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2\u20136 times faster, depending on the final output resolution.\n\n![Refer to caption]\n\n![Refer to caption]\n\nFigure 1: Our training starts with both the generator (G) and discriminator (D) having a low spatial resolution of 4\u00d7\\\\times4 pixels. As the training advances, we incrementally add layers to G and D, thus increasing the spatial resolution of the generated images. All existing layers remain trainable throughout the process. Here N\u00d7N\ud835\udc41\ud835\udc41N\\\\times N refers to convolutional layers operating on N\u00d7N\ud835\udc41\ud835\udc41N\\\\times N spatial resolution. This allows stable synthesis in high resolutions and also speeds up training considerably. One the right we show six example images generated using progressive growing at 1024\u00d71024102410241024\\\\times 1024.\n\nThe idea of growing GANs progressively is related to the work of Wang et al. ( [2017])), who use multiple discriminators that operate on different spatial resolutions.\nThat work in turn is motivated by Durugkar et al. ( [2016])) who use one generator and multiple discriminators concurrently, and Ghosh et al. ( [2017])) who do the opposite with multiple generators and one discriminator.\n\nHierarchical GANs (Denton et al., [2015]); Huang et al., [2016]); Zhang et al., [2017])) define a generator and discriminator for each level of an image pyramid. These methods build on the same observation as our work \u2013 that the complex mapping from latents to high-resolution images is easier to learn in steps \u2013 but the crucial difference is that we have only a single GAN instead of a hierarchy of them.\nIn contrast to early work on adaptively growing networks, e.g., growing neural gas (Fritzke, [1995])) and neuro evolution of augmenting topologies (Stanley & Miikkulainen, [2002])) that grow networks greedily, we simply defer the introduction of pre-configured layers. In that sense our approach resembles layer-wise training of autoencoders (Bengio et al., [2007])).\n\n![Refer to caption]Figure 2: When doubling the resolution of the generator (G) and discriminator (D) we fade in the new layers smoothly. This example illustrates the transition from 16\u00d716161616\\\\times 16 images (a) to 32\u00d732323232\\\\times 32 images (c). During the transition (b) we treat the layers that operate on the higher resolution like a residual block, whose weight \u03b1\ud835\udefc\\\\alpha increases linearly from 00 to 111. Here 2\u00d72\\\\times and 0.5\u00d70.5\\\\times refer to doubling and halving the image resolution using nearest neighbor filtering and average pooling, respectively. The toRGB represents a layer that projects feature vectors to RGB colors and fromRGB does the reverse; both use 1\u00d71111\\\\times 1 convolutions.\nWhen training the discriminator, we feed in real images that are downscaled to match the current resolution of the network.\nDuring a resolution transition, we interpolate between two resolutions of the real images, similarly to how the generator output combines two resolutions.\n\n## 3 Increasing variation using minibatch standard deviation\nGANs have a tendency to capture only a subset of the variation found in training data, and Salimans et al. ( [2016])) suggest \u201cminibatch discrimination\u201d as a solution. They compute feature statistics not only from individual images but also across the minibatch, thus encouraging the minibatches of generated and training images to show similar statistics.\nThis is implemented by adding a minibatch layer towards the end of the discriminator, where the layer learns a large tensor that projects the input activation to an array of statistics. A separate set of statistics is produced for each example in a minibatch and it is concatenated to the layer\u2019s output, so that the discriminator can use the statistics internally. We simplify this approach drastically while also improving the variation.\n\nOur simplified solution has neither learnable parameters nor new hyperparameters. We first compute the standard deviation for each feature in each spatial location over the minibatch. We then average these estimates over all features and spatial locations to arrive at a single value. We replicate the value and concatenate it to all spatial locations and over the minibatch, yielding one additional (constant) feature map. This layer could be inserted anywhere in the discriminator, but we have found it best to insert it towards the end (see Appendix [A.1]A.1 1024\u00d71024 networks used for CelebA-HQ \u2023 Appendix A Network structure and training configuration \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") for details). We experimented with a richer set of statistics, but were not able to improve the variation further.\nIn parallel work, Lin et al. ( [2017])) provide theoretical insights about the benefits of showing multiple images to the discriminator.\n\nAlternative solutions to the variation problem include unrolling the discriminator (Metz et al., [2016])) to regularize its updates, and a \u201crepelling regularizer\u201d\n(Zhao et al., [2017])) that adds a new loss term to the generator, trying to encourage it to orthogonalize the feature vectors in a minibatch. The multiple generators of Ghosh et al. ( [2017])) also serve a similar goal. We acknowledge that these solutions may increase the variation even more than our solution \u2013 or possibly be orthogonal to it \u2013 but leave a detailed comparison to a later time.\n\n## 4 Normalization in generator and discriminator\nGANs are prone to the escalation of signal magnitudes as a result of unhealthy competition between the two networks. Most if not all earlier solutions discourage this by using a variant of batch normalization (Ioffe & Szegedy, [2015]); Salimans & Kingma, [2016]); Ba et al., [2016])) in the generator, and often also in the discriminator. These normalization methods were originally introduced to eliminate covariate shift. However, we have not observed that to be an issue in GANs, and thus believe that the actual need in GANs is constraining signal magnitudes and competition. We use a different approach that consists of two ingredients, neither of which include learnable parameters.\n\n### 4.1 Equalized learning rate\n\nWe deviate from the current trend of careful weight initialization, and instead use a trivial \ud835\udca9\u200b(0,1)\ud835\udca901\\\\mathcal{N}(0,1) initialization and then explicitly scale the weights at runtime. To be precise, we set w^i=wi/csubscript^\ud835\udc64\ud835\udc56subscript\ud835\udc64\ud835\udc56\ud835\udc50\\\\hat{w}\\_{i}=w\\_{i}/c, where wisubscript\ud835\udc64\ud835\udc56w\\_{i} are the weights and c\ud835\udc50c is the per-layer normalization constant from He\u2019s initializer (He et al., [2015])). The benefit of doing this dynamically instead of during initialization is somewhat subtle, and relates to the scale-invariance in commonly used adaptive stochastic gradient descent methods such as RMSProp (Tieleman & Hinton, [2012])) and Adam (Kingma & Ba, [2015])). These methods normalize a gradient update by its estimated standard deviation, thus making the update independent of the scale of the parameter. As a result, if some parameters have a larger dynamic range than others, they will take longer to adjust. This is a scenario modern initializers cause, and thus it is possible that a learning rate is both too large and too small at the same time. Our approach ensures that the dynamic range, and thus the learning speed, is the same for all weights. A similar reasoning was independently used by van Laarhoven ( [2017])).\n\n### 4.2 Pixelwise feature vector normalization in generator\n\nTo disallow the scenario where the magnitudes in the generator and discriminator spiral out of control as a result of competition, we normalize the feature vector in each pixel to unit length in the generator after each convolutional layer. We do this using a variant of \u201clocal response normalization\u201d (Krizhevsky et al., [2012])), configured as\nbx,y=ax,y/1N\u200b\u2211j=0N\u22121(ax,yj)2+\u03f5subscript\ud835\udc4f\ud835\udc65\ud835\udc66subscript\ud835\udc4e\ud835\udc65\ud835\udc661\ud835\udc41superscriptsubscript\ud835\udc570\ud835\udc411superscriptsubscriptsuperscript\ud835\udc4e\ud835\udc57\ud835\udc65\ud835\udc662italic-\u03f5b\\_{x,y}=a\\_{x,y}/\\\\sqrt{\\\\frac{1}{N}\\\\sum\\_{j=0}^{N-1}(a^{j}\\_{x,y})^{2}+\\\\epsilon},\nwhere \u03f5=10\u22128italic-\u03f5superscript108\\\\epsilon=10^{-8}, N\ud835\udc41N is the number of feature maps, and ax,ysubscript\ud835\udc4e\ud835\udc65\ud835\udc66a\\_{x,y} and bx,ysubscript\ud835\udc4f\ud835\udc65\ud835\udc66b\\_{x,y} are the original and normalized feature vector in pixel (x,y)\ud835\udc65\ud835\udc66(x,y), respectively. We find it surprising that this heavy-handed constraint does not seem to harm the generator in any way, and indeed with most datasets it does not change the results much, but it prevents the escalation of signal magnitudes very effectively when needed.\n\n## 5 Multi-scale statistical similarity for assessing GAN results\nIn order to compare the results of one GAN to another, one needs to investigate a large number of images, which can be tedious, difficult, and subjective. Thus it is desirable to rely on automated methods that compute some indicative metric from large image collections. We noticed that existing methods such as MS-SSIM (Odena et al., [2017])) find large-scale mode collapses reliably but fail to react to smaller effects such as loss of variation in colors or textures, and they also do not directly assess image quality in terms of similarity to the training set.\n\nWe build on the intuition that a successful generator will produce samples whose local image structure is similar to the training set over all scales. We propose to study this by considering the multi-scale statistical similarity between distributions of local image patches drawn from Laplacian pyramid (Burt & Adelson, [1987])) representations of generated and target images, starting at a low-pass resolution of 16\u00d716161616\\\\times 16 pixels. As per standard practice, the pyramid progressively doubles until the full resolution is reached, each successive level encoding the difference to an up-sampled version of the previous level.\n\nA single Laplacian pyramid level corresponds to a specific spatial frequency band.\nWe randomly sample 16384 images and extract 128 descriptors from each level in the Laplacian pyramid, giving us 221superscript2212^{21} (2.1M) descriptors per level. Each descriptor is a 7\u00d77777\\\\times 7 pixel neighborhood with 3 color channels, denoted by \ud835\udc99\u2208\u211d7\u00d77\u00d73=\u211d147\ud835\udc99superscript\u211d773superscript\u211d147\\\\boldsymbol{x}\\\\in\\\\mathbb{R}^{7\\\\times 7\\\\times 3}=\\\\mathbb{R}^{147}. We denote the patches from level l\ud835\udc59l of the training set and generated set as {\ud835\udc99il}i=1221superscriptsubscriptsubscriptsuperscript\ud835\udc99\ud835\udc59\ud835\udc56\ud835\udc561superscript221\\\\{\\\\boldsymbol{x}^{l}\\_{i}\\\\}\\_{i=1}^{2^{21}} and {\ud835\udc9ail}i=1221superscriptsubscriptsubscriptsuperscript\ud835\udc9a\ud835\udc59\ud835\udc56\ud835\udc561superscript221\\\\{\\\\boldsymbol{y}^{l}\\_{i}\\\\}\\_{i=1}^{2^{21}}, respectively. We first normalize {\ud835\udc99il}subscriptsuperscript\ud835\udc99\ud835\udc59\ud835\udc56\\\\{\\\\boldsymbol{x}^{l}\\_{i}\\\\} and {\ud835\udc9ail}subscriptsuperscript\ud835\udc9a\ud835\udc59\ud835\udc56\\\\{\\\\boldsymbol{y}^{l}\\_{i}\\\\} w.r.t. the mean and standard deviation of each color channel,\nand then estimate the statistical similarity by computing their sliced Wasserstein distance SWD\u200b({\ud835\udc99il},{\ud835\udc9ail})SWDsubscriptsuperscript\ud835\udc99\ud835\udc59\ud835\udc56subscriptsuperscript\ud835\udc9a\ud835\udc59\ud835\udc56\\\\mathrm{SWD}(\\\\{\\\\boldsymbol{x}^{l}\\_{i}\\\\},\\\\{\\\\boldsymbol{y}^{l}\\_{i}\\\\}), an efficiently computable randomized approximation to earthmover s distance, using 512 projections (Rabin et al., [2011])).\n\nIntuitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowest-resolution 16\u00d716161616\\\\times 16 images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise.\n\n## 6 Experiments\nIn this section we discuss a set of experiments that we conducted to evaluate the quality of our results.\nPlease refer to Appendix [A]Appendix A Network structure and training configuration \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") for detailed description of our network structures and training configurations.\nWe also invite the reader to consult the accompanying video  for additional result images and latent space interpolations.\nIn this section we will distinguish between the _network structure_ (e.g., convolutional layers, resizing), _training configuration_ (various normalization layers, minibatch-related operations), and _training loss_ (WGAN-GP, LSGAN).\n\n### 6.1 Importance of individual contributions in terms of statistical similarity\n\nWe will first use the sliced Wasserstein distance (SWD) and multi-scale structural similarity (MS-SSIM) (Odena et al., [2017])) to evaluate the importance our individual contributions, and also perceptually validate the metrics themselves. We will do this by building on top of a previous state-of-the-art loss function (WGAN-GP) and training configuration (Gulrajani et al., [2017])) in an unsupervised setting using CelebA(Liu et al., [2015])) and LSUN bedroom(Yu et al., [2015])) datasets in 1282superscript1282128^{2} resolution. CelebA is particularly well suited for such comparison because the training images contain noticeable artifacts (aliasing, compression, blur) that are difficult for the generator to reproduce faithfully.\nIn this test we amplify the differences between training configurations by choosing a relatively low-capacity network structure (Appendix [A.2]A.2 Other networks \u2023 Appendix A Network structure and training configuration \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")) and terminating the training once the discriminator has been shown a total of 10M real images. As such the results are not fully converged.\n\n|     |     |     |     |     |     |     |     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  | CelebA | LSUN bedroom |\n| Training configuration | Sliced Wasserstein distance \u00d7103absentsuperscript103\\\\times 10^{3} | MS-SSIM | Sliced Wasserstein distance \u00d7103absentsuperscript103\\\\times 10^{3} | MS-SSIM |\n|  |  | 128 | 64 | 32 | 16 | Avg |  | 128 | 64 | 32 | 16 | Avg |  |\n| (a) | Gulrajani et al. ( [2017])) | 12.99 | 7.79 | 7.62 | 8.73 | 9.28 | 0.2854 | 11.97 | 10.51 | 8.03 | 14.48 | 11.25 | 0.0587 |\n| (b) | \\+ Progressive growing | 4.62 | 2.64 | 3.78 | 6.06 | 4.28 | 0.2838 | 7.09 | 6.27 | 7.40 | 9.64 | 7.60 | 0.0615 |\n| (c) | \\+ Small minibatch | 75.42 | 41.33 | 41.62 | 26.57 | 46.23 | 0.4065 | 72.73 | 40.16 | 42.75 | 42.46 | 49.52 | 0.1061 |\n| (d) | \\+ Revised training parameters | 9.20 | 6.53 | 4.71 | 11.84 | 8.07 | 0.3027 | 7.39 | 5.51 | 3.65 | 9.63 | 6.54 | 0.0662 |\n| (e\u2217) | \\+ Minibatch discrimination | 10.76 | 6.28 | 6.04 | 16.29 | 9.84 | 0.3057 | 10.29 | 6.22 | 5.32 | 11.88 | 8.43 | 0.0648 |\n| (e) | Minibatch stddev | 13.94 | 5.67 | 2.82 | 5.71 | 7.04 | 0.2950 | 7.77 | 5.23 | 3.27 | 9.64 | 6.48 | 0.0671 |\n| (f) | \\+ Equalized learning rate | 4.42 | 3.28 | 2.32 | 7.52 | 4.39 | 0.2902 | 3.61 | 3.32 | 2.71 | 6.44 | 4.02 | 0.0668 |\n| (g) | \\+ Pixelwise normalization | 4.06 | 3.04 | 2.02 | 5.13 | 3.56 | 0.2845 | 3.89 | 3.05 | 3.24 | 5.87 | 4.01 | 0.0640 |\n| (h) | Converged | 2.42 | 2.17 | 2.24 | 4.99 | 2.96 | 0.2828 | 3.47 | 2.60 | 2.30 | 4.87 | 3.31 | 0.0636 |\n\nTable 1: Sliced Wasserstein distance (SWD) between the generated and training images (Section [5]Multi-scale statistical similarity for assessing GAN results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")) and multi-scale structural similarity (MS-SSIM) among the generated images for several\ntraining setups at 128\u00d7128128128128\\\\times 128. For SWD, each column represents one level of the Laplacian pyramid, and the last one gives an average of the four distances.\n\n(a)(b)(c)(d)(e\u2217\u2217\\\\ast)(e)(f)(g)(h) Converged\n\nFigure 3: (a) \u2013 (g) CelebA examples corresponding to rows in Table [1]Table 1 \u2023 6.1 Importance of individual contributions in terms of statistical similarity \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\"). These are intentionally non-converged. (h) Our converged result.\nNotice that some images show aliasing and some are not sharp \u2013 this is a flaw of the dataset, which the model learns to replicate faithfully.\n\nTable [1]Table 1 \u2023 6.1 Importance of individual contributions in terms of statistical similarity \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") lists the numerical values for SWD and MS-SSIM in several training configurations, where our individual contributions are cumulatively enabled one by one on top of the baseline (Gulrajani et al., [2017])). The MS-SSIM numbers were averaged from 10000 pairs of generated images, and SWD was calculated as described in Section [5]Multi-scale statistical similarity for assessing GAN results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\"). Generated CelebA images from these configurations are shown in Figure [3]Figure 3 \u2023 6.1 Importance of individual contributions in terms of statistical similarity \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\").\nDue to space constraints, the figure shows only a small number of examples for each row of the table, but a significantly broader set is available in Appendix [H]Appendix H Additional images for Table 1 \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\").\nIntuitively, a good evaluation metric should reward plausible images that exhibit plenty of variation in colors, textures, and viewpoints.\nHowever, this is not captured by MS-SSIM: we can immediately see that configuration (h) generates significantly better images than configuration (a), but MS-SSIM remains approximately unchanged because it measures only the variation between outputs, not similarity to the training set. SWD, on the other hand, does indicate a clear improvement.\n\nThe first training configuration (a) corresponds to Gulrajani et al. ( [2017])), featuring batch normalization in the generator, layer normalization in the discriminator, and minibatch size of 64. (b) enables progressive growing of the networks, which results in sharper and more believable output images. SWD correctly finds the distribution of generated images to be more similar to the training set.\n\nOur primary goal is to enable high output resolutions, and this requires reducing the size of minibatches in order to stay within the available memory budget. We illustrate the ensuing challenges in (c) where we decrease the minibatch size from 64 to 16. The generated images are unnatural, which is clearly visible in both metrics. In (d), we stabilize the training process by adjusting the hyperparameters as well as by removing batch normalization and layer normalization (Appendix [A.2]A.2 Other networks \u2023 Appendix A Network structure and training configuration \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")).\nAs an intermediate test (e\u2217\u2217\\\\ast), we enable minibatch discrimination (Salimans et al., [2016])), which somewhat surprisingly fails to improve any of the metrics, including MS-SSIM that measures output variation.\nIn contrast, our minibatch standard deviation (e) improves the average SWD scores and images.\nWe then enable our remaining contributions in (f) and (g), leading to an overall improvement in SWD and subjective visual quality. Finally, in (h) we use a non-crippled network and longer training \u2013 we feel the quality of the generated images is at least comparable to the best published results so far.\n\n### 6.2 Convergence and training speed\n\n![Refer to caption]![Refer to caption]![Refer to caption]\n\n     (a)     (b)    (c)\n\nFigure 4: Effect of progressive growing on training speed and convergence. The timings were measured on a single-GPU setup using NVIDIA Tesla P100.\n(a) Statistical similarity with respect to wall clock time for Gulrajani et al. ( [2017])) using CelebA at 128\u00d7128128128128\\\\times 128 resolution. Each graph represents sliced Wasserstein distance on one level of the Laplacian pyramid, and the vertical line indicates the point where we stop the training in Table [1]Table 1 \u2023 6.1 Importance of individual contributions in terms of statistical similarity \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\").\n(b) Same graph with progressive growing enabled. The dashed vertical lines indicate points where we double the resolution of G and D.\n(c) Effect of progressive growing on the raw training speed in 1024\u00d71024102410241024\\\\times 1024 resolution.\n\nFigure [4]Figure 4 \u2023 6.2 Convergence and training speed \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") illustrates the effect of progressive growing in terms of the SWD metric and raw image throughput.\nThe first two plots correspond to the training configuration of Gulrajani et al. ( [2017])) without and with progressive growing.\nWe observe that the progressive variant offers two main benefits: it converges to a considerably better optimum and also reduces the total training time by about a factor of two.\nThe improved convergence is explained by an implicit form of curriculum learning that is imposed by the gradually increasing network capacity.\nWithout progressive growing, all layers of the generator and discriminator are tasked with simultaneously finding succinct intermediate representations for both the large-scale variation and the small-scale detail.\nWith progressive growing, however, the existing low-resolution layers are likely to have already converged early on, so the networks are only tasked with refining the representations by increasingly smaller-scale effects as new layers are introduced.\nIndeed, we see in Figure [4]Figure 4 \u2023 6.2 Convergence and training speed \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")(b) that the largest-scale statistical similarity curve (16) reaches its optimal value very quickly and remains consistent throughout the rest of the training.\nThe smaller-scale curves (32, 64, 128) level off one by one as the resolution is increased, but the convergence of each curve is equally consistent.\nWith non-progressive training in Figure [4]Figure 4 \u2023 6.2 Convergence and training speed \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")(a), each scale of the SWD metric converges roughly in unison, as could be expected.\n\nThe speedup from progressive growing increases as the output resolution grows.\nFigure [4]Figure 4 \u2023 6.2 Convergence and training speed \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\")(c) shows training progress, measured in number of real images shown to the discriminator, as a function of training time when the training progresses all the way to 10242superscript102421024^{2} resolution.\nWe see that progressive growing gains a significant head start because the networks are shallow and quick to evaluate at the beginning.\nOnce the full resolution is reached, the image throughput is equal between the two methods.\nThe plot shows that the progressive variant reaches approximately 6.4 million images in 96 hours, whereas it can be extrapolated that the non-progressive variant would take about 520 hours to reach the same point.\nIn this case, the progressive growing offers roughly a 5.4\u00d75.4\\\\times speedup.\n\n### 6.3 High-resolution image generation using CelebA-HQ dataset\n\n![Refer to caption]Figure 5: 1024\u00d71024102410241024\\\\times 1024 images generated using the CelebA-HQ dataset. See Appendix [F]Appendix F Additional CelebA-HQ results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") for a larger set of results, and the accompanying video for latent space interpolations.\n\nTo meaningfully demonstrate our results at high output resolutions, we need a sufficiently varied high-quality dataset.\nHowever, virtually all publicly available datasets previously used in GAN literature are limited to relatively low resolutions ranging from 322superscript32232^{2} to 4802superscript4802480^{2}.\nTo this end, we created a high-quality version of the CelebA dataset consisting of 30000 of the images at 1024\u00d71024102410241024\\\\times 1024 resolution. We refer to Appendix [C]Appendix C CelebA-HQ dataset \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") for further details about the generation of this dataset.\n\nOur contributions allow us to deal with high output resolutions in a robust and efficient fashion.\nFigure [5]Figure 5 \u2023 6.3 High-resolution image generation using CelebA-HQ dataset \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") shows selected 1024\u00d71024102410241024\\\\times 1024 images produced by our network.\nWhile megapixel GAN results have been shown before in another dataset (Marchesi, [2017])), our results are vastly more varied and of higher perceptual quality.\nPlease refer to Appendix [F]Appendix F Additional CelebA-HQ results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") for a larger set of result images as well as the nearest neighbors found from the training data. The accompanying video shows latent space interpolations and visualizes the progressive training.\nThe interpolation works so that we first randomize a latent code for each frame (512 components sampled individually from \ud835\udca9\u200b(0,1)\ud835\udca901\\\\mathcal{N}(0,1)), then blur the latents across time with a Gaussian (\u03c3=45\u200b frames @ 60Hz\ud835\udf0e45 frames @ 60Hz\\\\sigma=45\\\\text{ frames @ 60Hz}), and finally normalize each vector to lie on a hypersphere.\n\nWe trained the network on 8 Tesla V100 GPUs for 4 days, after which we no longer observed qualitative differences between the results of consecutive training iterations.\nOur implementation used an adaptive minibatch size depending on the current output resolution so that the available memory budget was optimally utilized.\n\nIn order to demonstrate that our contributions are largely orthogonal to the choice of a loss function, we also trained the same network using LSGAN loss instead of WGAN-GP loss. Figure [1]Figure 1 \u2023 2 Progressive growing of GANs \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") shows six examples of 10242superscript102421024^{2} images produced using our method using LSGAN. Further details of this setup are given in Appendix [B]Appendix B Least-squares GAN (LSGAN) at 1024\u00d71024 \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\").\n\n### 6.4 LSUN results\n\nMao et al. ( [2016b])) (128\u00d7128128128128\\\\times 128)Gulrajani et al. ( [2017])) (128\u00d7128128128128\\\\times 128)Our (256\u00d7256256256256\\\\times 256)\n\nFigure 6: Visual quality comparison in LSUN bedroom; pictures copied from the cited articles.\n\n![Refer to caption]\n\npottedplanthorsesofabuschurchoutdoorbicycletvmonitor\n\nFigure 7: Selection of 256\u00d7256256256256\\\\times 256 images generated from different LSUN categories.\n\nFigure [6]Figure 6 \u2023 6.4 LSUN results \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") shows a purely visual comparison between our solution and earlier results in LSUN bedroom.\nFigure [7]Figure 7 \u2023 6.4 LSUN results \u2023 6 Experiments \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") gives selected examples from seven very different LSUN categories at 2562superscript2562256^{2}. A larger, non-curated set of results from all 30 LSUN categories is available in Appendix [G]Appendix G LSUN results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\"), and the video demonstrates interpolations. We are not aware of earlier results in most of these categories, and while some categories work better than others, we feel that the overall quality is high.\n\n### 6.5 CIFAR10 inception scores\n\nThe best inception scores for CIFAR10 (10 categories of 32\u00d732323232\\\\times 32 RGB images) we are aware of are 7.90 for unsupervised and 8.87 for label conditioned setups (Grinblat et al., [2017])). The large difference between the two numbers is primarily caused by \u201cghosts\u201d that necessarily appear between classes in the unsupervised setting, while label conditioning can remove many such transitions.\n\nWhen all of our contributions are enabled, we get 8.808.808.80 in the _unsupervised_ setting. Appendix [D]Appendix D CIFAR10 results \u2023 Progressive Growing of GANs for Improved Quality, Stability, and Variation\") shows a representative set of generated images along with a more comprehensive list of results from earlier methods. The network and training setup were the same as for CelebA, progression limited to 32\u00d732323232\\\\times 32 of course. The only customization was to the WGAN-GP\u2019s regularization term \ud835\udd3c\ud835\udc31^\u223c\u2119\ud835\udc31^\\[(\\|\\|\u2207\ud835\udc31^D(\ud835\udc31^)\\|\\|2\u2212\u03b3)2/\u03b32\\\\mathop{\\\\mathbb{E}}\\_{\\\\hat{\\\\mathbf{x}}\\\\sim\\\\mathbb{P}\\_{\\\\hat{\\\\mathbf{x}}}}\\[(\\|\\|\\\\nabla\\_{\\\\hat{\\\\mathbf{x}}}D(\\\\hat{\\\\mathbf{x}})\\|\\|\\_{2}-\\\\gamma)^{2}/\\\\gamma^{2}\\]. Gulrajani et al. ( [2017])) used \u03b3=1.0\ud835\udefe1.0\\\\gamma=1.0, which corresponds to 1-Lipschitz, but we noticed that it is in fact significantly better to prefer fast transitions (\u03b3=750\ud835\udefe750\\\\gamma=750) to minimize the ghosts. We have not tried this trick with other datasets.\\\n\\\n\n## 7 Discussion\\\n\\\nWhile the quality of our results is generally high compared to earlier work on GANs, and the training is stable in large resolutions, there is a long way to true photorealism. Semantic sensibility and understanding dataset-dependent constraints, such as certain objects being straight rather than curved, leaves a lot to be desired. There is also room for improvement in the micro-structure of the images.\\\nThat said, we feel that convincing realism may now be within reach, especially in CelebA-HQ.\\\n\\\n\n## 8 Acknowledgements\\\n\\\nWe would like to thank Mikael Honkavaara, Tero Kuosmanen, and Timi Hietanen for the compute infrastructure. Dmitry Korobchenko and Richard Calderwood for efforts related to the CelebA-HQ dataset. Oskar Elek, Jacob Munkberg, and Jon Hasselgren for useful comments.\\\n\\\n</Content></Candidate>\n\n<Candidate>Candidate [3]:\nBalance of number of embedding and their dimensions in vector quantization.\n\n<Abstract>Abstract: The dimensionality of the embedding and the number of available embeddings (\nalso called codebook size) are critical factors influencing the performance of\nVector Quantization(VQ), a discretization process used in many models such as\nthe Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study\nexamines the balance between the codebook sizes and dimensions of embeddings in\nVQ, while maintaining their product constant. Traditionally, these hyper\nparameters are static during training; however, our findings indicate that\naugmenting the codebook size while simultaneously reducing the embedding\ndimension can significantly boost the effectiveness of the VQ-VAE. As a result,\nthe strategic selection of codebook size and embedding dimensions, while\npreserving the capacity of the discrete codebook space, is critically\nimportant. To address this, we propose a novel adaptive dynamic quantization\napproach, underpinned by the Gumbel-Softmax mechanism, which allows the model\nto autonomously determine the optimal codebook configuration for each data\ninstance. This dynamic discretizer gives the VQ-VAE remarkable flexibility.\nThorough empirical evaluations across multiple benchmark datasets validate the\nnotable performance enhancements achieved by our approach, highlighting the\nsignificant potential of adaptive dynamic quantization to improve model\nperformance.</Abstract><Content>\n## 1 Introduction\nThe Vector Quantized-Variational Autoencoder (VQ-VAE)(Van Den Oord et al., [2017])), which blends the methodologies of Variational Autoencoders (VAEs) (Kingma and Welling, [2013])) and Vector Quantization (VQ) (Gray, [1984])), effectively tackles the issue of noisy reconstructions commonly associated with VAEs by learning data representations that are both compact and efficient. In the VQ-VAE architecture, a vector quantization(VQ) layer converts continuous embeddings zesubscript\ud835\udc67\ud835\udc52z\\_{e} into their discrete equivalents zqsubscript\ud835\udc67\ud835\udc5ez\\_{q}. Due to the non-differentiable nature of the discretization process, the straight-through estimator (STE) (Bengio et al., [2013])) is used to perform backpropagation. The codebook, which stores these discrete values, has been a focal point of significant research aimed at addressing codebook collapse, a challenge introduced by the use of STE. Innovative techniques such as the use of Exponential Moving Average (EMA)(Van Den Oord et al., [2017])) and the introduction of multi-scale hierarchical structures in the VQ-VAE-2 (Razavi et al., [2019])) have proven effective in improving codebook efficiency.\n\nA variety of techniques have been utilized to enhance the diversity of codebook selections and alleviate issues of underutilization. These methods include stochastic sampling (Roy et al., [2018]); Kaiser et al., [2018]); Takida et al., [2022])), repeated K-means (MacQueen et al., [1967]); \u0141a\u0144cucki et al., [2020])), and various replacement policies (Zeghidour et al., [2021]); Dhariwal et al., [2020])). Furthermore, studies have investigated the impact of codebook size on performance. For example, Finite Scalar Quantization (FSQ) (Mentzer et al., [2023])) achieves comparable performance to VQ-VAE with a reduced quantization space, thereby preventing codebook collapse. Similarly, Language-Free Quantization (LFQ) (Yu et al., [2023])) minimizes the embedding dimension of the VQ-VAE codebook to zero, improving reconstruction capabilities. The VQ optimization technique (Li et al., [2023])) allows for adjustments in codebook size without necessitating retraining, addressing both computational limits and demands for reconstruction quality. Despite these significant developments, an automated approach to determine the ideal codebook size and dimension of each codeword for specific datasets and model parameters remains unexplored.\n\nThis research tackles the fore-mentioned gap by exploring the ideal codebook size for VQ-VAE. We present three major contributions:\n\n- \u2022\n\n\nOptimal Codebook Size and Embedding Dimension Determination: We systematically explored different combinations of codebook size and embedding dimension to understand optimal balances for different tasks.\n\n- \u2022\n\n\nAdaptive Dynamic Quantization: We introduce novel quantization mechanism that enables data points to autonomously select the optimal codebook size and embedding dimension.\n\n- \u2022\n\n\nEmpirical Validation and Performance Enhancement:\nWe conducted thorough empirical evaluations across multiple benchmark datasets to understand behaviors of the dynamic quantizer.\n\n## 2 Preliminaries\nIn this study, x\ud835\udc65x is the input data, x^^\ud835\udc65\\\\hat{x} is the output data, E\u200b(\u22c5)\ud835\udc38\u22c5E(\\\\cdot) is the encoding function, G\u200b(\u22c5)\ud835\udc3a\u22c5G(\\\\cdot) is the decoding function, and \u2112\u200b(\u22c5)\u2112\u22c5\\\\mathcal{L}(\\\\cdot) is the loss function.\n\n### 2.1 Vector-quantized networks\n\nA VQ-VAE network comprises three main components: encoder, decoder and a quantization layer h\u200b(\u22c5)\u210e\u22c5h(\\\\cdot).\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | x^=G\u200b(h\u200b(E\u200b(x)))=G\u200b(h\u200b(ze))=G\u200b(zq)^\ud835\udc65\ud835\udc3a\u210e\ud835\udc38\ud835\udc65\ud835\udc3a\u210esubscript\ud835\udc67\ud835\udc52\ud835\udc3asubscript\ud835\udc67\ud835\udc5e\\\\hat{x}=G(h(E(x)))=G(h(z\\_{e}))=G(z\\_{q}) |  | (1) |\n\nThe VQ layer h\u200b(\u22c5)\u210e\u22c5h\\\\left(\\\\cdot\\\\right) quantizes the vector ze=E\u200b(x)subscript\ud835\udc67\ud835\udc52\ud835\udc38\ud835\udc65z\\_{e}=E(x), by selecting one vector from N\ud835\udc41N vectors so that zq=h\u200b(ze)subscript\ud835\udc67\ud835\udc5e\u210esubscript\ud835\udc67\ud835\udc52z\\_{q}=h(z\\_{e}). These N\ud835\udc41N candidate vectors are called code embeddings or codewords, and they are combined to form a codebook C={c1,c2,c3\u200b\u2026\u200bcN}\ud835\udc36subscript\ud835\udc501subscript\ud835\udc502subscript\ud835\udc503\u2026subscript\ud835\udc50\ud835\udc41C=\\\\left\\\\{c\\_{1},c\\_{2},c\\_{3}...c\\_{N}\\\\right\\\\}.\n\nIn codebook C\ud835\udc36C, each code vector has a fixed index. Therefore, for the quantization layer, the discrete embedding zqsubscript\ud835\udc67\ud835\udc5ez\\_{q} that zesubscript\ud835\udc67\ud835\udc52z\\_{e} is quantized into is selected from the N\ud835\udc41N code vectors in the codebook, zq\u2208RT\u00d7Dsubscript\ud835\udc67\ud835\udc5esuperscript\ud835\udc45\ud835\udc47\ud835\udc37z\\_{q}\\\\in R^{T\\\\times D}, D\ud835\udc37D is the embedding dimension and T\ud835\udc47T refers to the number of data points. This selection process is implemented using the nearest neighbor method:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | zq=ck,w\u200bh\u200be\u200br\u200be\u200bk=a\u200br\u200bg\u200bm\u200bi\u200bn\ud835\udc57\u200b\u2016ze\u2212cj\u20162formulae-sequencesubscript\ud835\udc67\ud835\udc5esubscript\ud835\udc50\ud835\udc58\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc58\ud835\udc57\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc56\ud835\udc5bsubscriptnormsubscript\ud835\udc67\ud835\udc52subscript\ud835\udc50\ud835\udc572z\\_{q}=c\\_{k},\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,\\\\,where\\\\,\\\\,k=\\\\underset{j}{argmin}\\\\left\\\\\\|z\\_{e}-c\\_{j}\\\\right\\\\\\|\\_{2} |  | (2) |\n\nIn this process, PZsubscript\ud835\udc43\ud835\udc4dP\\_{Z} is represented as the data distribution after being encoded by the encoder, and QZsubscript\ud835\udc44\ud835\udc4dQ\\_{Z} represents the data distribution in the codebook space. After quantization, zqsubscript\ud835\udc67\ud835\udc5ez\\_{q} is used to predict the output x^=G\u200b(zq)^\ud835\udc65\ud835\udc3asubscript\ud835\udc67\ud835\udc5e\\\\hat{x}=G(z\\_{q}), and the loss is computed with the target x:L\u200b(x^,x):\ud835\udc65\ud835\udc3f^\ud835\udc65\ud835\udc65x:L(\\\\hat{x},x). The above introduces the data flow process of a single vector passing through the VQ-VAE model. For images, VQ is performed at each spatial location on the existing tensor, where the channel dimensions are used to represent vectors. For example, the graphical data of ze\u2208RB\u00d7C\u00d7H\u00d7Wsubscript\ud835\udc67\ud835\udc52superscript\ud835\udc45\ud835\udc35\ud835\udc36\ud835\udc3b\ud835\udc4az\\_{e}\\\\in R^{B\\\\times C\\\\times H\\\\times W} is flattened into ze\u2208R(B\u00d7H\u00d7W)\u00d7Csubscript\ud835\udc67\ud835\udc52superscript\ud835\udc45\ud835\udc35\ud835\udc3b\ud835\udc4a\ud835\udc36z\\_{e}\\\\in R^{(B\\\\times H\\\\times W)\\\\times C}, and then it goes through the process ze\u2208R(B\u00d7H\u00d7W)\u00d7C\u2192p\u200br\u200bo\u200bj\u200be\u200bc\u200bt\u200bi\u200bo\u200bnh\u200b(\u22c5)zq\u2208RT\u00d7D,T=B\u00d7H\u00d7Wformulae-sequencesubscript\ud835\udc67\ud835\udc52superscript\ud835\udc45\ud835\udc35\ud835\udc3b\ud835\udc4a\ud835\udc36\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\u210e\u22c5\u2192subscript\ud835\udc67\ud835\udc5esuperscript\ud835\udc45\ud835\udc47\ud835\udc37\ud835\udc47\ud835\udc35\ud835\udc3b\ud835\udc4az\\_{e}\\\\in R^{(B\\\\times H\\\\times W)\\\\times C}\\\\xrightarrow\\[projection\\]{h(\\\\cdot)}z\\_{q}\\\\in R^{T\\\\times D},T=B\\\\times H\\\\times W. B\ud835\udc35B, C\ud835\udc36C, H\ud835\udc3bH and W\ud835\udc4aW are the batch size, number of channels, image height, and width of image data, respectively. The standard training objective of VQ-VAE is to minimize reconstruction loss, its total loss is:\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2112=\u2112t\u200ba\u200bs\u200bk+\u2112c\u200bm\u200bt=\u2225x\u2212x^\u222522+\u03b2{\\|\\|sg\\[ze\\]\u2212zq\\|\\|22+\u03b1\\|\\|ze\u2212sg\\[zq\\]\\|\\|22\\]}\\\\mathcal{L}=\\\\mathcal{L}\\_{task}+\\\\mathcal{L}\\_{cmt}=\\\\left\\\\\\|x-\\\\hat{x}\\\\right\\\\\\|\\_{2}^{2}+\\\\beta\\\\left\\\\{\\|\\|sg\\[z\\_{e}\\]-z\\_{q}\\|\\|\\_{2}^{2}+\\\\alpha\\|\\|z\\_{e}-sg\\[z\\_{q}\\]\\|\\|\\_{2}^{2}\\]\\\\right\\\\} |  | (3) |\n\nIncluding the terms with s\u200bg\u200b\\[\u22c5\\]\ud835\udc60\ud835\udc54delimited-\\[\\]\u22c5sg\\\\left\\[\\\\cdot\\\\right\\] which is stop-gradient operation, in the two items are the commitment loss. The gradient through non-differentiable step is calculated using straight-through estimation \u2202\u2112\u2202E=\u2202\u2112\u2202zq\u200b\u2202\ud835\udc33e\u2202E\u2248\u2202\u2112\u2202zq\u200b\u2202zq\u2202ze\u200b\u2202\ud835\udc33e\u2202E\u2112\ud835\udc38\u2112subscript\ud835\udc67\ud835\udc5esubscript\ud835\udc33\ud835\udc52\ud835\udc38\u2112subscript\ud835\udc67\ud835\udc5esubscript\ud835\udc67\ud835\udc5esubscript\ud835\udc67\ud835\udc52subscript\ud835\udc33\ud835\udc52\ud835\udc38\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial E}=\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial{z\\_{q}}}\\\\frac{\\\\partial\\\\mathbf{z}\\_{e}}{\\\\partial E}\\\\approx\\\\frac{\\\\partial\\\\mathcal{L}}{\\\\partial{z\\_{q}}}\\\\frac{\\\\partial{z\\_{q}}}{\\\\partial{z\\_{e}}}\\\\frac{\\\\partial\\\\mathbf{z}\\_{e}}{\\\\partial E} is accurate.\n\u03b1\ud835\udefc\\\\alpha is a hyperparameter and set to 0.250.250.25. \u03b2\ud835\udefd\\\\beta is a parameter that controls the proportion of quantization loss. Exponential Moving Average (EMA) is a popular approach to update the codebook according to values of the encoder outputs: \ud835\udc33q(t+1)\u2190(1\u2212\u03b3)\u22c5\ud835\udc33q(t)+\u03b3\u22c5\ud835\udc33e(t)\u2190superscriptsubscript\ud835\udc33\ud835\udc5e\ud835\udc611\u22c51\ud835\udefesuperscriptsubscript\ud835\udc33\ud835\udc5e\ud835\udc61\u22c5\ud835\udefesuperscriptsubscript\ud835\udc33\ud835\udc52\ud835\udc61\\\\mathbf{z}\\_{q}^{(t+1)}\\\\leftarrow(1-\\\\gamma)\\\\cdot\\\\mathbf{z}\\_{q}^{(t)}+\\\\gamma\\\\cdot\\\\mathbf{z}\\_{e}^{(t)}, where \u03b3\ud835\udefe\\\\gamma is the decay coefficient.\n\n### 2.2 Related work\n\nVector Quantization (VQ) is a prominent technique in deep learning for deriving informative discrete latent representations. As an alternative to VQ, Soft Convex Quantization(Gautam et al., [2023])) solves for the optimal convex combination of codebook vectors during the forward pass process. Various methods, such as HQ-VAE(Takida et al., [2023])), CVQ-VAE(Zheng and Vedaldi, [2023])), HyperVQ(Goswami et al., [2024])), and EdVAE(Baykal et al., [2023])), have been developed to enhance the efficiency of codeword usage, achieve effective latent representations, and improve model performance. The Product Quantizer(Baldassarre et al., [2023])) addresses the issue of local significance by generating a set of discrete codes for each image block instead of a single index. One-Hot Max (OHM)(L\u00f6hdefink et al., [2022])) reorganizes the feature space, creating a one-hot vector quantization through activation comparisons. The Residual-Quantized VAE(Lee et al., [2022])) can represent a 256\u00d7256256256256\\\\times 256 image as an 8\u00d78888\\\\times 8 resolution feature map with a fixed codebook size. DVNC(Liu et al., [2021])) improves the utilization rate of codebooks through multi head discretization. The following DVQ(Liu et al., [2022])) dynamically selects discrete compactness based on input data. QINCo(Huijben et al., [2024])), a neural network variant of residual quantization, uses a neural network to predict a specialized codebook for each vector based on the vector approximation from the previous step. Similarly, this research utilizes an attention mechanism(Vaswani et al., [2017])) combined with Gumbel-Softmax to ensure the selection of the most suitable codebook for each data point.\n\n## 3 Method\n![Refer to caption]Figure 1: Schematic diagram of VQ-VAE network with adaptive dynamic quantization structure: (1)1(1)Utilizes the multi-head \"attention mechanism\" and Gumbel-Softmax operation to achieve dynamic selection of the quantization codebook. (2)2(2)Both the encoder and decoder employ CNN+ResNet architectures. (3)3(3)The experimental model includes functions for dimension transformation before and after quantization within the quantization layer to facilitate data flow.\n\n### 3.1 Codebook structure search\n\nTo investigate the impact of codebook size and embedding dimension on VQ-VAE\u2019s performance while maintaining a fixed capacity of the discrete information space, we conducted a series of experiments with various combinations of codebook sizes and embedding dimensions.. We denote the number of codewords by N\ud835\udc41N and the embedding dimension by D\ud835\udc37D. For a given discrete information space capacity W\ud835\udc4aW, the possible codebook structures are represented as H={\\[N1,D1\\],\\[N2,D2\\],\u2026,\\[Nm,Dm\\]}\ud835\udc3bsubscript\ud835\udc411subscript\ud835\udc371subscript\ud835\udc412subscript\ud835\udc372\u2026subscript\ud835\udc41\ud835\udc5asubscript\ud835\udc37\ud835\udc5aH=\\\\{\\[N\\_{1},D\\_{1}\\],\\[N\\_{2},D\\_{2}\\],\\\\ldots,\\[N\\_{m},D\\_{m}\\]\\\\}, where Ni\u00d7Di=Wsubscript\ud835\udc41\ud835\udc56subscript\ud835\udc37\ud835\udc56\ud835\udc4aN\\_{i}\\\\times D\\_{i}=W and Ni,Di\u2208\u2115+subscript\ud835\udc41\ud835\udc56subscript\ud835\udc37\ud835\udc56superscript\u2115N\\_{i},D\\_{i}\\\\in\\\\mathbb{N}^{+}. In this study, we fix W\ud835\udc4aW and explore different combination of N\ud835\udc41N and D\ud835\udc37D.\n\nFor these codebook combinations, we ensure that the number of codewords N\ud835\udc41N is always greater than the embedding dimension D\ud835\udc37D for two reasons: (1) a small number of codewords can significantly degrade the model\u2019s performance. (2) maintaining consistency in subsequent experiments. These systematic experiments aim to validate our hypothesis that the codebook size influences model performance and its pattern of impact. We hypothesize that increasing the number of codewords can enhance the reconstruction performance of VQ-VAE. Based on the results obtained from exploring different codebook combinations, further refined experiments are detailed in Section [3.2]Adaptive dynamic quantization mechanism \u2023 3 Method \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\").\n\n### 3.2 Adaptive dynamic quantization mechanism\n\nUnlike methods that use a single fixed codebook, our approach utilizes multiple codebooks while ensuring that the product of the codebook size and the embedding dimension remains constant. The diagram of our experimental model is presented in Figure [1]Figure 1 \u2023 3 Method \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"). The detailed algorithm of VQ-VAE with Adaptive Dynamic Quantization is clearly depicted in Algorithm [1]Algorithm 1 \u2023 3.2 Adaptive dynamic quantization mechanism \u2023 3 Method \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\").\n\nAlgorithm 1 VQ-VAE with Adaptive Dynamic Quantization.\n\n1:Input: Original images x\ud835\udc65x, Number of embeddings n\u200bu\u200bm\u2212\u200be\u200bm\u200bb\u200be\u200bd\u200bd\u200bi\u200bn\u200bg\u200bs\ud835\udc5b\ud835\udc62subscript\ud835\udc5a\ud835\udc52\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc60num\\_{-}embeddings, Embedding dimension e\u200bm\u200bb\u200be\u200bd\u200bd\u200bi\u200bn\u200bg\u2212\u200bd\u200bi\u200bm\u200bs\ud835\udc52\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc51\ud835\udc51\ud835\udc56\ud835\udc5bsubscript\ud835\udc54\ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60embedding\\_{-}dims, Hidden layer dimension n\u200bu\u200bm\u2212\u200bh\u200bi\u200bd\u200bd\u200be\u200bn\u200bs\ud835\udc5b\ud835\udc62subscript\ud835\udc5a\u210e\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc60num\\_{-}hiddens\n\n2:Output: Reconstructed images x^^\ud835\udc65\\\\hat{x}, Reconstruction loss L\ud835\udc3fL\n\n3:  Randomly initialize K\ud835\udc3eK with sizes \\[m,1,n\u200bu\u200bm\u2212\u200bh\u200bi\u200bd\u200bd\u200be\u200bn\u200bs\\]\ud835\udc5a1\ud835\udc5b\ud835\udc62subscript\ud835\udc5a\u210e\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc60\\[m,1,num\\_{-}hiddens\\].\n\n4:  Encoder E\u200b(\u22c5)\ud835\udc38\u22c5E\\\\left(\\\\cdot\\\\right), Decoder G\u200b(\u22c5)\ud835\udc3a\u22c5G\\\\left(\\\\cdot\\\\right), Quantization layers hi\u200b(\u22c5),i\u2208{1,2,3,\u2026,m}subscript\u210e\ud835\udc56\u22c5\ud835\udc56123\u2026\ud835\udc5ah\\_{i}\\\\left(\\\\cdot\\\\right),i\\\\in\\\\left\\\\{1,2,3,...,m\\\\right\\\\}, Gumbel-Softmax function G\u200bS\u200b(\u22c5)\ud835\udc3a\ud835\udc46\u22c5GS\\\\left(\\\\cdot\\\\right)\n\n5:ze=E\u200b(x)subscript\ud835\udc67\ud835\udc52\ud835\udc38\ud835\udc65z\\_{e}=E(x)\n\n6:Q=ze.f\u200bl\u200ba\u200bt\u200bt\u200be\u200bn\u200b(\u22121,n\u200bu\u200bm\u2212\u200bh\u200bi\u200bd\u200bd\u200be\u200bn\u200bs)formulae-sequence\ud835\udc44subscript\ud835\udc67\ud835\udc52\ud835\udc53\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b1\ud835\udc5b\ud835\udc62subscript\ud835\udc5a\u210e\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc60Q=z\\_{e}.flatten(-1,num\\_{-}hiddens)\n\n7:  One-hot selection score:s\u200bc\u200bo\u200br\u200be\u200bs=G\u200bS\u200b(M\u200bu\u200bl\u200bt\u200bi\u200bH\u200be\u200ba\u200bd\u200b(Q,K))\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc3a\ud835\udc46\ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc56\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc44\ud835\udc3escores=GS(MultiHead(Q,K))\n\n8:zqisuperscriptsubscript\ud835\udc67\ud835\udc5e\ud835\udc56z\\_{q}^{i}, v\u200bq\u2212\u200bl\u200bo\u200bs\u200bsi=hi\u200b(ze)\ud835\udc63subscript\ud835\udc5e\ud835\udc59\ud835\udc5c\ud835\udc60subscript\ud835\udc60\ud835\udc56subscript\u210e\ud835\udc56subscript\ud835\udc67\ud835\udc52vq\\_{-}loss\\_{i}=h\\_{i}(z\\_{e})\n\n9:zs=c\u200bo\u200bn\u200bc\u200ba\u200bt\u200b\\[zq1,zq2,\u2026\u200bzqm\\]subscript\ud835\udc67\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61superscriptsubscript\ud835\udc67\ud835\udc5e1superscriptsubscript\ud835\udc67\ud835\udc5e2\u2026superscriptsubscript\ud835\udc67\ud835\udc5e\ud835\udc5az\\_{s}=concat\\\\left\\[z\\_{q}^{1},z\\_{q}^{2},...z\\_{q}^{m}\\\\right\\], e\u200bx\u200bt\u200br\u200ba\u200bl\u200bo\u200bs\u200bs=1m\u200b\u2211i=1mv\u200bq\u2212\u200bl\u200bo\u200bs\u200bsi\ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc601\ud835\udc5asuperscriptsubscript\ud835\udc561\ud835\udc5a\ud835\udc63subscript\ud835\udc5e\ud835\udc59\ud835\udc5c\ud835\udc60subscript\ud835\udc60\ud835\udc56extraloss=\\\\frac{1}{m}\\\\sum\\_{i=1}^{m}vq\\_{-}loss\\_{i}\n\n10:zq=B\u200bM\u200bM\u200b(s\u200bc\u200bo\u200br\u200be\u200bs,zs)subscript\ud835\udc67\ud835\udc5e\ud835\udc35\ud835\udc40\ud835\udc40\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc60subscript\ud835\udc67\ud835\udc60z\\_{q}=BMM(scores,z\\_{s})\n\n11:x^=G\u200b(zq)^\ud835\udc65\ud835\udc3asubscript\ud835\udc67\ud835\udc5e\\\\hat{x}=G\\\\left(z\\_{q}\\\\right)\n\n12:r\u200be\u200bc\u200bo\u200bn\u2212\u200bl\u200bo\u200bs\u200bs=M\u200bS\u200bE\u200b(x,x^)\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc5csubscript\ud835\udc5b\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc40\ud835\udc46\ud835\udc38\ud835\udc65^\ud835\udc65recon\\_{-}loss=MSE(x,\\\\hat{x}), L=r\u200be\u200bc\u200bo\u200bn\u2212\u200bl\u200bo\u200bs\u200bs+e\u200bx\u200bt\u200br\u200ba\u200bl\u200bo\u200bs\u200bs\ud835\udc3f\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc5csubscript\ud835\udc5b\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60L=recon\\_{-}loss+extraloss.\n\nA pool of m\ud835\udc5am discretization codebooks H={\\[Ni,Di\\]}i\u2208\\[m\\]\ud835\udc3bsubscriptsubscript\ud835\udc41\ud835\udc56subscript\ud835\udc37\ud835\udc56\ud835\udc56delimited-\\[\\]\ud835\udc5aH=\\\\left\\\\{\\[N\\_{i},D\\_{i}\\]\\\\right\\\\}\\_{i\\\\in\\[m\\]} is provided for all representations undergoing discretization. Each codebook \\[Ni,Di\\]subscript\ud835\udc41\ud835\udc56subscript\ud835\udc37\ud835\udc56\\[N\\_{i},D\\_{i}\\] is associated with a unique quantization layer hi\u200b(\u22c5)subscript\u210e\ud835\udc56\u22c5h\\_{i}(\\\\cdot). These discretization codebooks and their corresponding quantization layers are independent and do not share parameters. The discrete vector obtained by quantizing zesubscript\ud835\udc67\ud835\udc52z\\_{e} through the quantization layer hi\u200b(\u22c5)subscript\u210e\ud835\udc56\u22c5h\\_{i}(\\\\cdot) is denoted as zqisuperscriptsubscript\ud835\udc67\ud835\udc5e\ud835\udc56z\\_{q}^{i}. Each quantizer hi\u200b(\u22c5)subscript\u210e\ud835\udc56\u22c5h\\_{i}(\\\\cdot) possesses a unique key vector kisubscript\ud835\udc58\ud835\udc56k\\_{i} and value vector visubscript\ud835\udc63\ud835\udc56v\\_{i}, which are initially randomly initialized. To perform parameter feature learning for the quantization layers, we utilize the Multi-Head Attention mechanism, defined as scoresattention=MultiHead\u200b(Q,K,V)subscriptscoresattentionMultiHead\ud835\udc44\ud835\udc3e\ud835\udc49\\\\text{scores}\\_{\\\\text{attention}}=\\\\text{MultiHead}(Q,K,V), where Q\ud835\udc44Q is the flattened vector of zesubscript\ud835\udc67\ud835\udc52z\\_{e} and K=concat\u200b(k1,k2,\u2026,km)\ud835\udc3econcatsubscript\ud835\udc581subscript\ud835\udc582\u2026subscript\ud835\udc58\ud835\udc5aK=\\\\text{concat}(k\\_{1},k\\_{2},\\\\ldots,k\\_{m}), V=concat\u200b(v1,v2,\u2026,vm)\ud835\udc49concatsubscript\ud835\udc631subscript\ud835\udc632\u2026subscript\ud835\udc63\ud835\udc5aV=\\\\text{concat}(v\\_{1},v\\_{2},\\\\ldots,v\\_{m}). The Gumbel-Softmax operation is applied to the attention scores for one-hot selection to determine which codebook to use, with the categorical distribution given by \u03c0ze\u200b(i)=exp\u2061(ki\u22a4\u200bze)\u2211j\u2208\\[N\\]exp\u2061(kj\u22a4\u200bze)superscript\ud835\udf0bsubscript\ud835\udc67\ud835\udc52\ud835\udc56superscriptsubscript\ud835\udc58\ud835\udc56topsubscript\ud835\udc67\ud835\udc52subscript\ud835\udc57delimited-\\[\\]\ud835\udc41superscriptsubscript\ud835\udc58\ud835\udc57topsubscript\ud835\udc67\ud835\udc52\\\\pi^{z\\_{e}}(i)=\\\\frac{\\\\exp(k\\_{i}^{\\\\top}z\\_{e})}{\\\\sum\\_{j\\\\in\\[N\\]}\\\\exp(k\\_{j}^{\\\\top}z\\_{e})}.\n\nFinally, the attention scores obtained after the Gumbel-Softmax operation are combined with multiple quantized vectors to produce the final quantized vector zqsubscript\ud835\udc67\ud835\udc5ez\\_{q}. This is achieved using batch matrix multiplication, denoted as zq=BMM\u200b(scoresattention,zs)subscript\ud835\udc67\ud835\udc5eBMMsubscriptscoresattentionsubscript\ud835\udc67\ud835\udc60z\\_{q}=\\\\text{BMM}(\\\\text{scores}\\_{\\\\text{attention}},z\\_{s}), where zs=concat\u200b(zq1,zq2,\u2026,zqm)subscript\ud835\udc67\ud835\udc60concatsuperscriptsubscript\ud835\udc67\ud835\udc5e1superscriptsubscript\ud835\udc67\ud835\udc5e2\u2026superscriptsubscript\ud835\udc67\ud835\udc5e\ud835\udc5az\\_{s}=\\\\text{concat}(z\\_{q}^{1},z\\_{q}^{2},\\\\ldots,z\\_{q}^{m}). The loss function for this dynamic quantization-based VQ-VAE model is given in Equation [4]In 3.2 Adaptive dynamic quantization mechanism \u2023 3 Method \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\").\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u2112=\u2225x\u2212x^\u222522+1m\u2211i=1m\u03b2{\\|\\|sg\\[ze\\]\u2212zqi\\|\\|22+\u03b1\\|\\|ze\u2212sg\\[zqi\\]\\|\\|22\\]}\\\\mathcal{L}=\\\\left\\\\\\|x-\\\\hat{x}\\\\right\\\\\\|\\_{2}^{2}+\\\\frac{1}{m}\\\\sum\\_{i=1}^{m}\\\\beta\\\\left\\\\{\\|\\|sg\\[z\\_{e}\\]-z\\_{q}^{i}\\|\\|\\_{2}^{2}+\\\\alpha\\|\\|z\\_{e}-sg\\[z\\_{q}^{i}\\]\\|\\|\\_{2}^{2}\\]\\\\right\\\\} |  | (4) |\n\n## 4 Experimental results and analysis\nAll our experiments were conducted with a constant information capacity for the codebooks. This means that while the sizes of the codebooks and the embedding dimensions varied, their product remained constant, ensuring the discrete space capacity stayed unchanged. We experimented with six datasets: MNIST, FashionMNIST, CIFAR-10, Tiny-Imagenet, CelebA, and a real-world dataset, the Diabetic Retinopathy dataset. In the experiments described in Sections [4.1]The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\") and [4.2]Adaptive dynamic quantization mechanism \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), the discrete information space size of the experimental model was set to 655366553665536. The multi-head attention mechanism used had two heads. The codebook in the VQ operation was updated using the EMA method. Both the encoder and decoder architectures were designed with Convolutional Neural Networks(CNN)(LeCun et al., [1998])) and Residual Network(ResNet)(He et al., [2016])). The hyperparameters \u03b1\ud835\udefc\\\\alpha, \u03b2\ud835\udefd\\\\beta, and \u03b3\ud835\udefe\\\\gamma were set to 0.250.250.25, 111, and 0.990.990.99, respectively. Part of the experimental results are presented in Section [4]Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), please refer to the [Appendix]In Balance of Number of Embedding and their Dimensions in Vector Quantization\") for other results.\n\n### 4.1 The influence of codebook size and embedding dimension\n\nBefore enhancing the quantization method of the VQ-VAE, we conducted an in-depth analysis on how various combinations of codebook size N\ud835\udc41N and embedding dimensions D\ud835\udc37D affect the model\u2019s performance. This study was carried out with the constraint that the capacity of the discrete space in the VQ-VAE model remained constant, meaning that the product of N\ud835\udc41N and D\ud835\udc37D was kept unchanged. In the original VQ-VAE model configuration, we specified a codebook size of 512512512 and embedding dimensions of 128128128. From this starting point, we transitioned to experimenting with various other combinations of N\ud835\udc41N (codebook size) and D\ud835\udc37D (embedding dimensions), ranging from \\[1024,64\\]102464\\[1024,64\\] to \\[65536,1\\]655361\\[65536,1\\], to observe their individual effects on the model\u2019s performance. Our findings indicate that increasing N\ud835\udc41N, the codebook size, generally enhances the model\u2019s performance. However, maintaining a sufficiently high D\ud835\udc37D, the embedding dimension, is also essential to sustain this improvement. The results from these investigations are displayed in Table [1]Table 1 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), which outlines the reconstruction loss on the validation set for various N,D\ud835\udc41\ud835\udc37N,D configurations, each maintained under identical training conditions across six datasets.\n\nTable 1: Reconstruction loss under different fixed codebook models\n\n| Datasets |\n| --- |\n| \\[N,D\\] | MNIST | FshionMNIST | CIFAR10 | Tiny-ImageNet | Diabetic-retinopathy | CelebA |\n| --- | --- | --- | --- | --- | --- | --- |\n| \\[512,128\\] | 0.285 | 0.577 | 0.606 | 2.679 | 0.361 | 0.177 |\n| \\[1024,64\\] | 0.294 | 0.590 | 0.663 | 2.654 | 0.400 | 0.156 |\n| \\[2048,32\\] | 0.291 | 0.602 | 0.618 | 2.640 | 0.410 | 0.155 |\n| \\[4096,16\\] | 0.265 | 0.589 | 0.618 | 2.650 | 0.362 | 0.136 |\n| \\[8192,8\\] | 0.240 | 0.503 | 0.553 | 2.605 | 0.337 | 0.122 |\n| \\[16384,4\\] | 0.210 | 0.489 | 0.503 | 2.528 | 0.347 | 0.120 |\n| \\[32768,2\\] | 0.199 | 0.578 | 0.808 | 3.364 | 0.407 | 0.171 |\n| \\[65536,1\\] | 0.377 | 0.877 | 1.353 | 4.631 | 0.493 | 0.334 |\n\nTable [1]Table 1 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\") illustrates that smaller embedding dimensions in the codebook do not always lead to better results. In the original VQ-VAE model, while the codebook size remains constant, modifying it and retraining can yield varied outcomes. Furthermore, within a fixed discrete information space, there is an optimal codebook size that best fits the specific dataset. Following these findings, we computed the terminal gradient gap(Huh et al., [2023])) for models that use a consistent codebook size throughout their training. A successful quantization function h\u200b(\u22c5)\u210e\u22c5h\\\\left(\\\\cdot\\\\right) retains essential information from a finite vector set zesubscript\ud835\udc67\ud835\udc52z\\_{e}. The quantized vector is expressed as zq=ze+\u03b5subscript\ud835\udc67\ud835\udc5esubscript\ud835\udc67\ud835\udc52\ud835\udf00z\\_{q}=z\\_{e}+\\\\varepsilon, where \u03b5\ud835\udf00\\\\varepsilon is the residual vector derived from the quantization process. If \u03b5=0\ud835\udf000\\\\varepsilon=0, it implies no direct estimation error, suggesting that the model effectively lacks a quantization function. Equation [5]In 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\") describes how to measure the gradient gap concerning this ideal, lossless quantization function.\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|  | \u0394\u200bG\u200ba\u200bp=\u2016\u2202\u2112task\u200b(G\u200b(\ud835\udc33e))\u2202E\u200b(x)\u2212\u2202\u2112task\u200b(G\u200b(\ud835\udc33q))\u2202E\u200b(x)\u2016\u0394\ud835\udc3a\ud835\udc4e\ud835\udc5dnormsubscript\u2112task\ud835\udc3asubscript\ud835\udc33\ud835\udc52\ud835\udc38\ud835\udc65subscript\u2112task\ud835\udc3asubscript\ud835\udc33\ud835\udc5e\ud835\udc38\ud835\udc65\\\\Delta{Gap}=\\\\left\\\\\\|\\\\frac{\\\\partial\\\\mathcal{L}\\_{\\\\text{task}}(G(\\\\mathbf{z}\\_{e}))}{\\\\partial E(x)}-\\\\frac{\\\\partial\\\\mathcal{L}\\_{\\\\text{task}}(G(\\\\mathbf{z}\\_{q}))}{\\\\partial E(x)}\\\\right\\\\\\| |  | (5) |\n\nDuring the training phase, we monitored the variations in the gradient gap. Notably, the pattern of gradient gap changes associated with the codebook remained consistent across three-channel datasets. For instance, we demonstrated the evolving pattern of the gradient gap for the CIFAR10 dataset in Figure [2]Figure 2 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\")(left). Although the gradient gap changes are quite similar between two single-channel datasets, differences emerge when comparing single-channel and three-channel datasets. More comprehensive details on these differences are discussed in the [Appendix]In Balance of Number of Embedding and their Dimensions in Vector Quantization\").\n\n![Refer to caption]\n\n![Refer to caption]\n\nFigure 2: Gradient gap and quantization loss under fixed codebook models.\n\nThe reconstruction loss for the other three datasets shows a similar trend. As the codebook size increases up to the point of achieving the lowest validation loss, the gradient gap remains constant. However, once the size surpasses this optimal codebook combination, any further decrease in embedding dimension results in a progressive increase in the gradient gap for subsequent models. From the findings in Table [1]Table 1 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), we conclude that within a fixed discrete information space, enlarging the codebook size to a certain extent can enhance the model\u2019s reconstruction capabilities. Nonetheless, if the embedding dimension becomes too small, it will lead to an increased quantization gradient gap, which could degrade the model\u2019s performance rather than enhance it.\n\nGiven that each batch contains an equal number of data points, the quantization loss is impacted by the embedding dimension. A smaller embedding dimension tends to yield a reduced quantization loss, as depicted in Figure [2]Figure 2 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\")(right).\n\nAnalysis: Increasing N\ud835\udc41N generally allows the model to capture more variability in the data due to a larger number of vectors available for encoding the data, potentially reducing quantization error. However, as N\ud835\udc41N increases, the dimension D\ud835\udc37D typically decreases to manage computational complexity and memory usage, which can affect the quality of each individual representation. Lower D\ud835\udc37D might lead to loss of information, since each vector has less capacity to encapsulate details of the data. However, too high a dimension could also mean that the vectors are too specific, reducing their generalizability. As N\ud835\udc41N increases, each vector in the codebook needs to handle less variance, theoretically reducing the reconstruction loss. However, each vector also has fewer D\ud835\udc37D to capture that variance as N\ud835\udc41N decreases. The balance between these factors is delicate and dataset-specific. The characteristics of each dataset heavily influence the optimal N\ud835\udc41N and D\ud835\udc37D . Simpler datasets with less intrinsic variability (like MNIST) can be effectively modeled even with lower D\ud835\udc37D , while more complex datasets require either higher D\ud835\udc37D or more carefully balanced D\ud835\udc37D.\n\n### 4.2 Adaptive dynamic quantization mechanism\n\nThe adaptive dynamic quantization mechanism allows each data point in a batch to independently select the most appropriate codebook for quantization and training. Mirroring the format seen in Table [1]Table 1 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), we have showcased in Table [2]Table 2 \u2023 4.2 Adaptive dynamic quantization mechanism \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\") the comparison of reconstruction loss between the best fixed codebook model an adaptive dynamic quantization model across various validation datasets.\n\nTable 2: Comparison of reconstruction performance between adaptive dynamic quantization model and optimal fixed codebook model.\n\n| Datasets |\n| --- |\n| Models | MNIST | FshionMNIST | CIFAR10 | Tiny-ImageNet | Diabetic-retinopathy | CelebA |\n| --- | --- | --- | --- | --- | --- | --- |\n| Fixed Codebook | 0.199 | 0.489 | 0.503 | 2.528 | 0.337 | 0.120 |\n| Adaptive Codebook | 0.166 | 0.410 | 0.394 | 2.022 | 0.294 | 0.099 |\n\nEven with an optimized fixed quantization codebook for reconstruction performance, it falls short compared to our proposed adaptive dynamic quantization model. This is because our model allows each input data point to choose from eight codebooks within a discrete information space of 65536 possibilities, whereas the fixed codebook model offers no such flexibility.\n\nThe main advantage of our method lies in its ability to learn the distribution of different features in the dataset and adapt to the codebook during training. To illustrate the usage of different codebooks, we plotted the frequency of codebook usage in the adaptive dynamic quantization model throughout the training process. The results align closely with the data in Section [4.1]The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"). As shown in Figure [3]Figure 3 \u2023 4.2 Adaptive dynamic quantization mechanism \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), the adaptive dynamic quantization model, implemented with Gumbel-softmax and an attention mechanism, gradually learns the various feature distributions of the dataset and selects different codebooks. At the beginning, the model tends to choose the largest codebook for fast learning, while in the later stage of learning, it tends to choose the most suitable codebook. By the final stage of training, the codebook size with the highest usage frequency matches the codebook size that achieved the lowest reconstruction loss in the fixed codebook experiment detailed in Section [4.1]The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"). For the Diabetic Retinopathy dataset, our analysis is that the dataset itself has not been uniformly preprocessed and the distribution of different features in the dataset is not obvious(As shown in Figure [5]Figure 5 \u2023 A.1 Experimental details \u2023 Appendix A Appendix \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\")), resulting in a lack of inclination to choose a certain codebook at the beginning of training. However, in the final stage, the model will gradually converge and tend to choose the most suitable codebook.\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\nFigure 3: The frequency of adaptive selection of different-sized codebooks throughout the training process. The training can be divided into two stages: Initially, the model predominantly learns from the codebook with the most codewords; Subsequently, the model optimizes and increasingly selects the type of codebook with the minimum reconstruction loss in the fixed codebook model. The experimental results on the Diabetic Retinopathy dataset are slightly less conspicuous, but in the final stage of training, there is still a tendency to choose the most suitable codebook\n\nWe plotteed a comparison graph of the gradient gap produced by our method versus that produced by fixed codebooks. The results, displayed in Figure [4]Figure 4 \u2023 4.2 Adaptive dynamic quantization mechanism \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\")(left), show that the adaptive dynamic quantizer method achieves optimal reconstruction performance while maintaining low gradient gap. The size of the gradient difference depends on the magnitude of the quantization error and the smoothness of the decoding function G\u200b(\u22c5)\ud835\udc3a\u22c5G\\\\left(\\\\cdot\\\\right). The minimal change in quantization loss indicates that the decoder of the adaptive dynamic quantization model exhibits better smoothness compared to the fixed codebook model.\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\n![Refer to caption]\n\nFigure 4:\nComparison of quantization loss and gradient gap between adaptive dynamic quantization model and fixed codebook model. The blue color represents the model with the smallest gradient gap and the largest quantization loss in the fixed codebook model, the yellow color represents the fixed codebook model with the smallest reconstruction loss, and the red color represents the adaptive dynamic quantization model. The gradient gap and quantization loss of the adaptive quantizer are kept at low values.\n\nThe pattern in Figure [2]Figure 2 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\")(right) is clear: larger codebook sizes correspond to smaller embedding dimensions and lower quantization losses. We compared the quantization loss of the dynamic quantizer with various fixed codebooks and plotted the range of the dynamic quantizer\u2019s quantization loss. The quantization loss resulting from our dynamic quantization method is close to that of the optimal fixed codebook, demonstrating the effectiveness of our approach. As shown in Figure [4]Figure 4 \u2023 4.2 Adaptive dynamic quantization mechanism \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\")(right), the y-axis is represented in logarithmic form for clarity.\n\nAnalysis: At the beginning of training, the model is in an exploratory phase. It is trying to learn and understand the structure of the data. Larger codebook size provide a finer granularity of representation and offer more codewords to choose from, which allows the model to capture more detailed and diverse patterns in the data. Using larger codebook size can help the model avoid early commitment to suboptimal quantization, giving it the flexibility to explore different parts of the data space. As training progresses, the model starts to specialise and optimise its parameters based on the data it has seen. The need for the fine granularity provided by the larger codebook size diminishes because the model becomes more confident in its learned representations. They help prevent overfitting by providing a more generalised representation of the data. With fewer codewords, the model is forced to generalise better, which can lead to improved performance on unseen data. Gradually transitioning to the most suitable codebook size helps strike a balance between accurate data representation and model generalisation. For detailed mathematical proofs, please refer to the [Appendix]In Balance of Number of Embedding and their Dimensions in Vector Quantization\").\n\n### 4.3 Albation study\n\nTable 3: Comparison of reconstruction performance among variants of the model that adaptively select codebooks.\n\n| Datasets |\n| --- |\n| Models | MNIST | FshionMNIST | CIFAR10 | Tiny-ImageNet | Diabetic-retinopathy | CelebA |\n| --- | --- | --- | --- | --- | --- | --- |\n| Base | 0.165 | 0.410 | 0.394 | 2.022 | 0.294 | 0.099 |\n| W=32786\ud835\udc4a32786W=32786 | 0.176 | 0.436 | 0.412 | 2.156 | 0.338 | 0.106 |\n| W=16384\ud835\udc4a16384W=16384 | 0.181 | 0.442 | 0.427 | 2.207 | 0.337 | 0.108 |\n| W=8192\ud835\udc4a8192W=8192 | 0.191 | 0.447 | 0.427 | 2.329 | 0.305 | 0.104 |\n| W=4096\ud835\udc4a4096W=4096 | 0.193 | 0.447 | 0.458 | 2.336 | 0.358 | 0.104 |\n| Without EMA | 0.317 | 0.595 | 0.490 | 2.559 | 0.361 | 0.173 |\n| \u03b1=0.5\ud835\udefc0.5\\\\alpha=0.5 | 0.171 | 0.424 | 0.428 | 2.204 | 0.355 | 0.102 |\n| \u03b1=0.75\ud835\udefc0.75\\\\alpha=0.75 | 0.175 | 0.442 | 0.440 | 2.482 | 0.325 | 0.104 |\n| \u03b1=1.0\ud835\udefc1.0\\\\alpha=1.0 | 0.188 | 0.492 | 0.466 | 2.387 | 0.380 | 0.118 |\n| \u03b1=5\ud835\udefc5\\\\alpha=5 | 0.257 | 0.682 | 0.630 | 3.027 | 0.464 | 0.138 |\n| \u03b1=10\ud835\udefc10\\\\alpha=10 | 0.271 | 0.823 | 0.700 | 3.278 | 0.485 | 0.182 |\n| \u03b2=0.01\ud835\udefd0.01\\\\beta=0.01 | 0.169 | 0.427 | 0.381 | 2.179 | 0.299 | 0.091 |\n| \u03b2=0.2\ud835\udefd0.2\\\\beta=0.2 | 0.167 | 0.397 | 0.349 | 1.999 | 0.279 | 0.088 |\n| \u03b2=5\ud835\udefd5\\\\beta=5 | 0.191 | 0.487 | 0.491 | 2.462 | 0.360 | 0.110 |\n| CNN | 0.154 | 0.365 | 0.330 | 1.828 | 0.293 | 0.091 |\n| CNN+Inception | 0.190 | 0.539 | 0.459 | 2.467 | 0.360 | 0.104 |\n\nThe \u2019Base\u2019 model in Table [3]Table 3 \u2023 4.3 Albation study \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\") refers to the model discussed at the beginning of Section [4]Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"). Building on this model configuration, we conducted ablation experiments addressing various aspects such as the model\u2019s encoder-decoder structure, hyperparameters, discrete information space, and the use of EMA. Increasing the capacity of the discrete space and employing EMA to update codebooks both enhance the model\u2019s performance, with the base experiment\u2019s value of \u03b1=0.25\ud835\udefc0.25\\\\alpha=0.25 being appropriate. Across these datasets, reducing the proportion of quantization loss (\u03b2=0.2\ud835\udefd0.2\\\\beta=0.2) is shown to be beneficial. Additionally, it is noteworthy that these datasets do not require overly complex encoders and decoders. Adding ResNet and Inception layers (Szegedy et al., [2015])) after the convolutional layers actually decreases model performance. The loss values in Tables [1]Table 1 \u2023 4.1 The influence of codebook size and embedding dimension \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), [2]Table 2 \u2023 4.2 Adaptive dynamic quantization mechanism \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\"), and [3]Table 3 \u2023 4.3 Albation study \u2023 4 Experimental results and analysis \u2023 Balance of Number of Embedding and their Dimensions in Vector Quantization\") represent the sum of reconstruction losses incurred by the model when evaluated on the respective standard validation sets of the corresponding datasets after one complete pass.\n\n## 5 Conclusion\nIn this study, we focused on the impact of the number of codebooks and the embedding dimension on VQ-VAE\u2019s performance, with particular attention to the situation where the product of these two parameters remains constant. Our research introduced an adaptive dynamic quantizer based on Gumbel-softmax, allowing the model to adaptively choose the most ideal codebook size and embedding dimension at any given data point. It was found that the model, adopting the new method, is divided into two stages throughout the learning process. In the first stage, it tends to learn using codebooks of the largest size, while in the second stage, it gradually favors the use of the most suitable codebook. Our method demonstrated significant performance improvements in empirical validation across multiple benchmark datasets, proving the importance of adaptive dynamic discretizers for optimizing VQ-VAE models.\n</Content></Candidate>\n\n</References>\nRemember to output the index of the selected reference enclosed within <answer> and </answer>.",
  "reference_id": "bib.bib13",
  "label_id": 0,
  "mask_all": false,
  "citation": {
    "section_name": "5 Experiments",
    "citation_id": "bib.bib13",
    "citation_text": "Dosovitskiy, [2020]",
    "citation_type": "method",
    "start_pos": 12776,
    "end_pos": 12795,
    "nearest_citations": [
      "bib.bib26",
      "bib.bib6",
      "bib.bib11"
    ]
  },
  "candidates": [
    {
      "id": "bib.bib13",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
      "token_count": 10434
    },
    {
      "id": "bib.bib2",
      "title": "Estimating or propagating gradients through stochastic neurons for conditional computation.",
      "token_count": 18527
    },
    {
      "id": "bib.bib26",
      "title": "Progressive growing of gans for improved quality, stability, and variation.",
      "token_count": 9543
    },
    {
      "id": "bib.bib6",
      "title": "Balance of number of embedding and their dimensions in vector quantization.",
      "token_count": 12122
    }
  ],
  "additional_fields": {
    "paper_field": "full",
    "mask_all": false,
    "candidate_fields": [
      "title",
      "abstract",
      "sections"
    ],
    "sample": "random",
    "max_selected_references_per_paper": 5
  },
  "id": 95
}